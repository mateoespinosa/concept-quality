{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purity Correlation Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import concepts_xai\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import rc\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "import scipy\n",
    "import utils\n",
    "import model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "## Set seeds up for reproducibility\n",
    "################################################################################\n",
    "\n",
    "utils.reseed(87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "## Global Variables Defining Experiment Flow\n",
    "################################################################################\n",
    "\n",
    "LATEX_SYMBOL = \"\"\n",
    "NUM_TRIALS = 5\n",
    "LOAD_FROM_CACHE = True\n",
    "RESULTS_DIR = \"results/toy_tabular\"\n",
    "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "rc('text', usetex=(LATEX_SYMBOL == \"$\"))\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "\n",
    "def bold_text(x):\n",
    "    if LATEX_SYMBOL == \"$\":\n",
    "        return r\"$\\textbf{\" + x + \"}$\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## Generate Data\n",
    "############################################################################\n",
    "def produce_data(samples, cov=0.0, num_concepts=3):\n",
    "    x = np.zeros((samples, 7), dtype=np.float32)\n",
    "    y = np.zeros((samples,), dtype=np.float32)\n",
    "    \n",
    "    # Sample the x, y, and z variables\n",
    "    vars = np.random.multivariate_normal(\n",
    "        mean=[0, 0, 0],\n",
    "        cov=[\n",
    "            [1, cov, cov],\n",
    "            [cov, 1, cov],\n",
    "            [cov, cov, 1],\n",
    "        ],\n",
    "        size=(samples,),\n",
    "    )\n",
    "    x_vars = vars[:, :1]\n",
    "    y_vars = vars[:, 1:2]\n",
    "    z_vars = vars[:, 2:]\n",
    "    \n",
    "    # The features are just non-linear functions applied to each\n",
    "    # variable\n",
    "    features = [\n",
    "        np.sin(x_vars) + x_vars,\n",
    "        np.cos(x_vars) + x_vars,\n",
    "        np.sin(y_vars) + y_vars,\n",
    "        np.cos(y_vars) + y_vars,\n",
    "        np.sin(z_vars) + z_vars,\n",
    "        np.cos(z_vars) + z_vars,\n",
    "        x_vars**2 + y_vars**2 + z_vars**2,\n",
    "    ]\n",
    "    features = np.stack(features, axis=1)\n",
    "\n",
    "    # The concepts just check if the variables are positive\n",
    "    x_pos = (x_vars > 0).astype(np.int32)\n",
    "    y_pos = (y_vars > 0).astype(np.int32)\n",
    "    z_pos = (z_vars > 0).astype(np.int32)\n",
    "    concepts = np.squeeze(\n",
    "        np.stack([x_pos, y_pos, z_pos][:num_concepts], axis=1)\n",
    "    ).astype(np.float32)\n",
    "    \n",
    "    # The labels are generated by checking if at least two of the\n",
    "    # latent concepts are greater than zero\n",
    "    labels = x_pos + y_pos + z_pos\n",
    "    labels = (labels > 1).astype(np.int32)\n",
    "    \n",
    "    # And that's it buds\n",
    "    return features, labels, concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the encoder model\n",
    "def construct_encoder(\n",
    "    input_shape,\n",
    "    units,\n",
    "    num_concepts,\n",
    "    end_activation=\"sigmoid\",\n",
    "    latent_dims=0,\n",
    "    output_logits=False,\n",
    "):\n",
    "    encoder_inputs = tf.keras.Input(shape=input_shape)\n",
    "    encoder_compute_graph = encoder_inputs\n",
    "    for i, units in enumerate(units):\n",
    "        encoder_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(encoder_compute_graph)\n",
    "\n",
    "    # And finally map this to the number of concepts we have in our set\n",
    "    encoder_compute_graph = tf.keras.layers.Flatten()(encoder_compute_graph)\n",
    "    if latent_dims:\n",
    "        bypass = tf.keras.layers.Dense(\n",
    "            latent_dims,\n",
    "            activation=end_activation,\n",
    "            name=\"encoder_bypass_channel\",\n",
    "        )(encoder_compute_graph)\n",
    "    else:\n",
    "        bypass = None\n",
    "    encoder_compute_graph = tf.keras.layers.Dense(\n",
    "        num_concepts,\n",
    "        activation=None if output_logits else \"sigmoid\",\n",
    "        name=\"encoder_concept_outputs\",\n",
    "    )(encoder_compute_graph)\n",
    "\n",
    "    # Now time to collapse all the concepts again back into a single vector\n",
    "    encoder_model = tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        encoder_compute_graph if bypass is None else [encoder_compute_graph, bypass],\n",
    "        name=\"encoder\",\n",
    "    )\n",
    "    return encoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## Build concepts-to-labels model\n",
    "############################################################################\n",
    "\n",
    "def construct_decoder(units, num_outputs=1,):\n",
    "    decoder_layers = [tf.keras.layers.Flatten()] + [\n",
    "        tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation=tf.nn.relu,\n",
    "            name=f\"decoder_dense_{i+1}\",\n",
    "        ) for i, units in enumerate(units)\n",
    "    ]\n",
    "    decoder_model = tf.keras.Sequential(decoder_layers + [\n",
    "        tf.keras.layers.Dense(\n",
    "            num_outputs,\n",
    "            activation=None,\n",
    "            name=\"decoder_model_output\",\n",
    "        )\n",
    "    ])\n",
    "    return decoder_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBM Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.CBM.CBModel as CBM\n",
    "reload(CBM)\n",
    "\n",
    "############################################################################\n",
    "## Build CBM\n",
    "############################################################################\n",
    "\n",
    "def construct_cbm(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    latent_dims=0,\n",
    "    alpha=0.1,\n",
    "    learning_rate=1e-3,\n",
    "    encoder_output_logits=False,\n",
    "):\n",
    "    model_factory = CBM.BypassJointCBM if latent_dims else CBM.JointConceptBottleneckModel\n",
    "    cbm_model = model_factory(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        task_loss=(\n",
    "            tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        ),\n",
    "        name=\"joint_cbm\",\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()],\n",
    "        alpha=alpha,\n",
    "        pass_concept_logits=encoder_output_logits,\n",
    "    )\n",
    "\n",
    "    ############################################################################\n",
    "    ## Compile CBM Model\n",
    "    ############################################################################\n",
    "\n",
    "    cbm_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    )\n",
    "    return cbm_model\n",
    "\n",
    "# Construct the complete model\n",
    "def construct_end_to_end_model(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    input_shape,\n",
    "    learning_rate=1e-3,\n",
    "):\n",
    "    model_inputs = tf.keras.Input(shape=input_shape)\n",
    "    encoder_out = encoder(model_inputs)\n",
    "    if isinstance(encoder_out, list):\n",
    "        encoder_out = tf.concat(encoder_out, axis=-1)\n",
    "    model_compute_graph = decoder(encoder_out)\n",
    "    # Now time to collapse all the concepts again back into a single vector\n",
    "    model = tf.keras.Model(\n",
    "        model_inputs,\n",
    "        model_compute_graph,\n",
    "        name=\"complete_model\",\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[\"binary_accuracy\"],\n",
    "    )\n",
    "    return model, encoder, decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purity experiment with concept logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.evaluation.metrics.oracle as oracle\n",
    "\n",
    "############################################################################\n",
    "## Experiment loop\n",
    "############################################################################\n",
    "\n",
    "def construct_trivial_auc_mat(num_concepts):\n",
    "    result = np.ones((num_concepts, num_concepts), dtype=np.float32) * 0.5\n",
    "    return result + np.eye(num_concepts, dtype=np.float32) * 0.5\n",
    "    \n",
    "def cbm_experiment_loop(experiment_config, load_from_cache=False):\n",
    "    experiment_variables = dict(\n",
    "        task_accuracies=[],\n",
    "        task_aucs=[],\n",
    "        concept_accuracies=[],\n",
    "        purity_scores=[],\n",
    "        non_oracle_purity_scores=[],\n",
    "        purity_matrices=[],\n",
    "        oracle_matrices=[],\n",
    "        correlation_matrices=[],\n",
    "    )\n",
    "    utils.reseed(87)\n",
    "    experiment_config[\"data_concepts\"] = experiment_config.get(\n",
    "        \"data_concepts\",\n",
    "        experiment_config[\"num_concepts\"],\n",
    "    )\n",
    "    \n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(experiment_config[\"covariances\"]):\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)}.'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    utils.serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "    \n",
    "    # Let's save our config here either way\n",
    "    utils.serialize_experiment_config(\n",
    "        experiment_config,\n",
    "        experiment_config[\"results_dir\"],\n",
    "    )\n",
    "    \n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    for cov in experiment_config[\"covariances\"][start_ind:]:\n",
    "        print(\"Training with covariance:\", cov)\n",
    "        task_accs = []\n",
    "        concept_accs = []\n",
    "        aucs = []\n",
    "        purity_mats = []\n",
    "        oracle_mats = []\n",
    "        purities = []\n",
    "        non_oracle_purities = []\n",
    "        corr_mats = []\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} with covariance {cov}\")\n",
    "            # First construct the dataset\n",
    "            (x_train, y_train, y_train_concepts) = produce_data(\n",
    "                experiment_config[\"train_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            (x_test, y_test, y_test_concepts) = produce_data(\n",
    "                experiment_config[\"test_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            \n",
    "            # Then proceed to do and end-to-end model in case we want to\n",
    "            # do some task-specific pretraining\n",
    "            end_to_end_model, encoder, decoder = construct_end_to_end_model(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                encoder=construct_encoder(\n",
    "                    input_shape=experiment_config[\"input_shape\"],\n",
    "                    units=experiment_config[\"encoder_units\"],\n",
    "                    num_concepts=experiment_config[\"num_concepts\"],\n",
    "                    end_activation=\"sigmoid\",\n",
    "                    latent_dims=experiment_config[\"latent_dims\"],\n",
    "                    output_logits=experiment_config.get(\"encoder_output_logits\", False),\n",
    "                ),\n",
    "                decoder=construct_decoder(\n",
    "                    units=experiment_config[\"decoder_units\"],\n",
    "                    num_outputs=experiment_config[\"num_outputs\"],\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            if experiment_config.get(\"pre_train_epochs\"):\n",
    "                print(\"\\tModel pre-training...\")\n",
    "                end_to_end_model.fit(\n",
    "                    x=x_train,\n",
    "                    y=y_train,\n",
    "                    epochs=experiment_config[\"pre_train_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "                print(\"\\t\\tModel pre-training completed\")\n",
    "            \n",
    "            # Now time to actually construct and train the CBM\n",
    "            cbm_model = construct_cbm(\n",
    "                encoder=encoder,\n",
    "                decoder=decoder,\n",
    "                alpha=experiment_config[\"alpha\"],\n",
    "                learning_rate=experiment_config[\"learning_rate\"],\n",
    "                latent_dims=experiment_config.get(\"latent_dims\", 0),\n",
    "                encoder_output_logits=experiment_config.get(\"encoder_output_logits\", False),\n",
    "            )\n",
    "\n",
    "            early_stopping_monitor = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_concept_accuracy\",\n",
    "                min_delta=experiment_config[\"min_delta\"],\n",
    "                patience=experiment_config[\"patience\"],\n",
    "                restore_best_weights=True,\n",
    "                verbose=2,\n",
    "                mode='max',\n",
    "            )\n",
    "            if experiment_config[\"warmup_epochs\"]:\n",
    "                print(\"\\tWarmup training...\")\n",
    "                cbm_model.fit(\n",
    "                    x=x_train,\n",
    "                    y=(\n",
    "                        y_train,\n",
    "                        y_train_concepts[:, :experiment_config[\"num_concepts\"]],\n",
    "                    ),\n",
    "                    epochs=experiment_config[\"warmup_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "                print(\"\\t\\tWarmup training completed\")\n",
    "\n",
    "\n",
    "            print(\"\\tCBM training...\")\n",
    "            cbm_model.fit(\n",
    "                x=x_train,\n",
    "                y=(\n",
    "                    y_train,\n",
    "                    y_train_concepts[:, :experiment_config[\"num_concepts\"]],\n",
    "                ),\n",
    "                epochs=experiment_config[\"max_epochs\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                callbacks=[\n",
    "                    early_stopping_monitor,\n",
    "                ],\n",
    "                validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                verbose=verbosity,\n",
    "            )\n",
    "            print(\"\\t\\tCBM training completed\")\n",
    "            print(\"\\tSerializing model\")\n",
    "            encoder.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/encoder_cov_{cov:.1f}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            decoder.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/decoder_cov_{cov:.1f}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            print(\"\\tEvaluating model\")\n",
    "            \n",
    "            test_result = cbm_model.evaluate(\n",
    "                x_test,\n",
    "                (\n",
    "                    y_test,\n",
    "                    y_test_concepts[:, :experiment_config[\"num_concepts\"]],\n",
    "                ),\n",
    "                verbose=0,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            task_accs.append(test_result['binary_accuracy'])\n",
    "            concept_accs.append(test_result['concept_accuracy'])\n",
    "            aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                y_test,\n",
    "                cbm_model.predict(x_test)[0],\n",
    "            ))\n",
    "            \n",
    "            print(\n",
    "                f\"\\t\\tTest auc = {aucs[-1]:.4f}, \"\n",
    "                f\"test concept accuracy = {concept_accs[-1]:.4f}, \"\n",
    "                f\"task accuracy = {task_accs[-1]:.4f}\"\n",
    "            )\n",
    "            \n",
    "            print(\"\\tComputing linear correlations...\")\n",
    "            soft_acts = (\n",
    "                np.concatenate(cbm_model.encoder(x_test), axis=-1)\n",
    "                if experiment_config[\"latent_dims\"] else encoder(x_test).numpy()\n",
    "            )\n",
    "            corr_mat = np.ones((soft_acts.shape[-1], y_test_concepts.shape[-1]))\n",
    "            for c in range(corr_mat.shape[0]):\n",
    "                for l in range(corr_mat.shape[1]):\n",
    "                    corr_mat[c][l] = np.corrcoef(\n",
    "                        soft_acts[:, c],\n",
    "                        y_test_concepts[:, l],\n",
    "                    )[0, 1]\n",
    "\n",
    "            corr_mats.append(corr_mat)\n",
    "\n",
    "            print(f\"\\t\\tComputing OIS...\")\n",
    "            purity_score, purity_mat, oracle_mat = oracle.oracle_impurity_score(\n",
    "                c_soft=soft_acts,\n",
    "                c_true=y_test_concepts,\n",
    "                output_matrices=True,\n",
    "            )\n",
    "            purity_mats.append(purity_mat)\n",
    "            oracle_mats.append(oracle_mat)\n",
    "            purities.append(purity_score)\n",
    "            print(f\"\\t\\t\\tDone {purity_score:.4f}\")\n",
    "\n",
    "            print(\"\\t\\tComputing non-oracle purity score...\")\n",
    "        \n",
    "            non_oracle_purities.append(oracle.oracle_impurity_score(\n",
    "                c_soft=soft_acts,\n",
    "                c_true=y_test_concepts,\n",
    "                oracle_matrix=construct_trivial_auc_mat(\n",
    "                    experiment_config[\"data_concepts\"]\n",
    "                ),\n",
    "                purity_matrix=purity_mat,\n",
    "            ))\n",
    "            print(f\"\\t\\t\\tDone {non_oracle_purities[-1]:.4f}\")\n",
    "            print(\"\\t\\tDone with trial\", trial + 1)\n",
    "\n",
    "        task_acc_mean, task_acc_std = np.mean(task_accs), np.std(task_accs)\n",
    "        experiment_variables[\"task_accuracies\"].append((task_acc_mean, task_acc_std))\n",
    "        print(f\"\\tTest task accuracy: {task_acc_mean:.4f} ± {task_acc_std:.4f}\")\n",
    "\n",
    "        concept_acc_mean, concept_acc_std = np.mean(concept_accs), np.std(concept_accs)\n",
    "        experiment_variables[\"concept_accuracies\"].append((concept_acc_mean, concept_acc_std))\n",
    "        print(f\"\\tTest concept accuracy: {concept_acc_mean:.4f} ± {concept_acc_std:.4f}\")\n",
    "\n",
    "\n",
    "        task_auc_mean, task_auc_std = np.mean(aucs), np.std(aucs)\n",
    "        experiment_variables[\"task_aucs\"].append((task_auc_mean, task_auc_std))\n",
    "        print(f\"\\tTest task AUC: {task_auc_mean:.4f} ± {task_auc_std:.4f}\")\n",
    "        \n",
    "        \n",
    "        corr_mats = np.stack(corr_mats, axis=0)\n",
    "        corr_mat_mean = np.mean(corr_mats, axis=0)\n",
    "        corr_mat_std = np.std(corr_mats, axis=0)\n",
    "        print(\"\\tCorrelation matrix:\")\n",
    "        for i in range(corr_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(corr_mat_mean.shape[1]):\n",
    "                line += f'{corr_mat_mean[i, j]:.4f} ± {corr_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"correlation_matrices\"].append((corr_mat_mean, corr_mat_std))\n",
    "\n",
    "        purity_mats = np.stack(purity_mats, axis=0)\n",
    "        purity_mat_mean = np.mean(purity_mats, axis=0)\n",
    "        purity_mat_std = np.std(purity_mats, axis=0)\n",
    "        print(\"\\tPurity matrix:\")\n",
    "        for i in range(purity_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(purity_mat_mean.shape[1]):\n",
    "                line += f'{purity_mat_mean[i, j]:.4f} ± {purity_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"purity_matrices\"].append((purity_mat_mean, purity_mat_std))\n",
    "\n",
    "\n",
    "        oracle_mats = np.stack(oracle_mats, axis=0)\n",
    "        oracle_mat_mean = np.mean(oracle_mats, axis=0)\n",
    "        oracle_mat_std = np.std(oracle_mats, axis=0)\n",
    "        print(\"\\tOracle matrix:\")\n",
    "        for i in range(oracle_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(oracle_mat_mean.shape[1]):\n",
    "                line += f'{oracle_mat_mean[i, j]:.4f} ± {oracle_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"oracle_matrices\"].append((oracle_mat_mean, oracle_mat_std))\n",
    "\n",
    "        purity_mean, purity_std = np.mean(purities), np.std(purities)\n",
    "        experiment_variables[\"purity_scores\"].append((purity_mean, purity_std))\n",
    "        print(f\"\\tOIS: {purity_mean:.4f} ± {purity_std:.4f}\")\n",
    "\n",
    "        non_oracle_purity_mean, non_oracle_purity_std = np.mean(non_oracle_purities), np.std(non_oracle_purities)\n",
    "        experiment_variables[\"non_oracle_purity_scores\"].append((non_oracle_purity_mean, non_oracle_purity_std))\n",
    "        print(f\"\\tNon-oracle purity score: {non_oracle_purity_mean:.4f} ± {non_oracle_purity_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables\n",
    "\n",
    "def cbm_bottleneck_predict_experiment_loop(\n",
    "    experiment_config,\n",
    "    load_from_cache=False,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    experiment_variables = dict(\n",
    "        latent_predictive_accuracies=[],\n",
    "        latent_predictive_aucs=[],\n",
    "    )\n",
    "\n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                print(\"Could not find\", f'\"{file_name}_means.npz\" or \"{file_name}_stds.npz\" in cache.')\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(experiment_config[\"covariances\"]):\n",
    "                print(\"Found\", len(loaded_means), \"means for variable\", var_name, \"vs\", len(experiment_config[\"covariances\"]), \"datasets\")\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)} ({file_name}).'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            print(experiment_variables)\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    utils.serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    for cov in experiment_config[\"covariances\"][start_ind:]:\n",
    "        print(\"Training with covariance:\", cov)\n",
    "        task_accs = []\n",
    "        aucs = []\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} with covariance {cov}\")\n",
    "            # First construct the dataset\n",
    "            (x_train, y_train, y_train_concepts) = produce_data(\n",
    "                experiment_config[\"train_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            (x_test, y_test, y_test_concepts) = produce_data(\n",
    "                experiment_config[\"test_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "\n",
    "            encoder = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/encoder_cov_{cov:.1f}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            predictive_decoder = construct_decoder(\n",
    "                units=experiment_config[\"latent_decoder_units\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "            )\n",
    "            predictive_decoder.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(experiment_config[\"learning_rate\"]),\n",
    "                loss=(\n",
    "                    tf.keras.losses.BinaryCrossentropy(from_logits=True) if (experiment_config[\"num_outputs\"] <= 2)\n",
    "                    else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "                ),\n",
    "                metrics=[\n",
    "                    \"binary_accuracy\" if (experiment_config[\"num_outputs\"] <= 2)\n",
    "                    else tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            print(\"\\tTraining model\")\n",
    "            train_codes = encoder(x_train)\n",
    "            if isinstance(train_codes, list):\n",
    "                train_codes = np.concatenate(list(map(lambda x: x.numpy(), train_codes)), axis=-1)\n",
    "            else:\n",
    "                train_codes = train_codes.numpy()\n",
    "            test_codes = encoder(x_test)\n",
    "            if isinstance(test_codes, list):\n",
    "                test_codes = np.concatenate(list(map(lambda x: x.numpy(), test_codes)), axis=-1)\n",
    "            else:\n",
    "                test_codes = test_codes.numpy()\n",
    "            predictive_decoder.fit(\n",
    "                x=train_codes,\n",
    "                y=y_train,\n",
    "                epochs=experiment_config[\"predictor_max_epochs\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                verbose=verbosity,\n",
    "            )\n",
    "            print(\"\\tEvaluating model\")\n",
    "            test_result = predictive_decoder.evaluate(\n",
    "                test_codes,\n",
    "                y_test,\n",
    "                verbose=0,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            task_accs.append(\n",
    "                test_result['sparse_top_k_categorical_accuracy']\n",
    "                if experiment_config['num_outputs'] > 1 else\n",
    "                test_result['binary_accuracy']\n",
    "            )\n",
    "\n",
    "            if experiment_config['num_outputs'] > 1:\n",
    "                # Then lets apply a softmax activation over all the probability\n",
    "                # classes\n",
    "                preds = scipy.special.softmax(\n",
    "                    predictive_decoder.predict(test_codes),\n",
    "                    axis=-1,\n",
    "                )\n",
    "\n",
    "                # And select just the labels that are in fact being used\n",
    "                print(np.sum(preds[:100, :], axis=-1))\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    y_test,\n",
    "                    preds,\n",
    "                    multi_class='ovo',\n",
    "                ))\n",
    "            else:\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    y_test,\n",
    "                    predictive_decoder.predict(test_codes),\n",
    "                ))\n",
    "            \n",
    "            print(\n",
    "                f\"\\t\\tTest auc = {aucs[-1]:.4f}, \"\n",
    "                f\"task accuracy = {task_accs[-1]:.4f}\"\n",
    "            )\n",
    "            print(\"\\t\\tDone with trial\", trial + 1)\n",
    "\n",
    "        task_acc_mean, task_acc_std = np.mean(task_accs), np.std(task_accs)\n",
    "        experiment_variables[\"latent_predictive_accuracies\"].append((task_acc_mean, task_acc_std))\n",
    "        print(f\"\\tTest task accuracy: {task_acc_mean:.4f} ± {task_acc_std:.4f}\")\n",
    "\n",
    "        task_auc_mean, task_auc_std = np.mean(aucs), np.std(aucs)\n",
    "        experiment_variables[\"latent_predictive_aucs\"].append((task_auc_mean, task_auc_std))\n",
    "        print(f\"\\tTest task AUC: {task_auc_mean:.4f} ± {task_auc_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables\n",
    "\n",
    "def cbm_bottleneck_concept_predict_experiment_loop(\n",
    "    experiment_config,\n",
    "    load_from_cache=False,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    experiment_variables = dict(\n",
    "        latent_avg_concept_predictive_accuracies=[],\n",
    "        latent_avg_concept_predictive_aucs=[],\n",
    "    )\n",
    "\n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                print(\"Could not find\", f'\"{file_name}_means.npz\" or \"{file_name}_stds.npz\" in cache.')\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(experiment_config[\"covariances\"]):\n",
    "                print(\"Found\", len(loaded_means), \"means for variable\", var_name, \"vs\", len(experiment_config[\"covariances\"]), \"datasets\")\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)} ({file_name}).'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            print(experiment_variables)\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    utils.serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    for cov in experiment_config[\"covariances\"][start_ind:]:\n",
    "        print(\"Training with covariance:\", cov)\n",
    "        avg_concept_accs = []\n",
    "        avg_concept_aucs = []\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} with covariance {cov}\")\n",
    "            # First construct the dataset\n",
    "            (x_train, y_train, c_train) = produce_data(\n",
    "                experiment_config[\"train_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            (x_test, y_test, c_test) = produce_data(\n",
    "                experiment_config[\"test_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']}\")\n",
    "            encoder = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/encoder_cov_{cov:.1f}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "\n",
    "            train_codes = encoder(x_train)\n",
    "            if isinstance(train_codes, list):\n",
    "                train_codes = np.concatenate(list(map(lambda x: x.numpy(), train_codes)), axis=-1)\n",
    "            else:\n",
    "                train_codes = train_codes.numpy()\n",
    "            test_codes = encoder(x_test)\n",
    "            if isinstance(test_codes, list):\n",
    "                test_codes = np.concatenate(list(map(lambda x: x.numpy(), test_codes)), axis=-1)\n",
    "            else:\n",
    "                test_codes = test_codes.numpy()\n",
    "            \n",
    "            current_accuracies = []\n",
    "            current_aucs = []\n",
    "            for concept_idx in range(experiment_config[\"num_concepts\"]):\n",
    "                print(\"\\tTraining model for concept\", concept_idx)\n",
    "                predictive_decoder = construct_decoder(\n",
    "                    units=experiment_config[\"latent_decoder_units\"],\n",
    "                    num_outputs=1,\n",
    "                )\n",
    "                predictive_decoder.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(experiment_config[\"learning_rate\"]),\n",
    "                    loss=(\n",
    "                        tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "                    ),\n",
    "                    metrics=[\n",
    "                        \"binary_accuracy\"\n",
    "                    ],\n",
    "                )\n",
    "                predictive_decoder.fit(\n",
    "                    x=train_codes,\n",
    "                    y=c_train[:, concept_idx],\n",
    "                    epochs=experiment_config[\"concept_predictor_max_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "                print(\"\\t\\tEvaluating model\")\n",
    "                test_result = predictive_decoder.evaluate(\n",
    "                    test_codes,\n",
    "                    c_test[:, concept_idx],\n",
    "                    verbose=0,\n",
    "                    return_dict=True,\n",
    "                )\n",
    "                current_accuracies.append(test_result['binary_accuracy'])\n",
    "                \n",
    "                current_aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    c_test[:, concept_idx],\n",
    "                    predictive_decoder.predict(test_codes),\n",
    "                ))\n",
    "                print(\n",
    "                f\"\\t\\t\\tAverage test concept accuracy = {current_accuracies[-1]:.4f}, \"\n",
    "                f\"average test concept AUC = {current_aucs[-1]:.4f}\"\n",
    "            )\n",
    "            \n",
    "            avg_concept_accs.append(np.mean(current_accuracies))\n",
    "            avg_concept_aucs.append(np.mean(current_aucs))\n",
    "            print(\n",
    "                f\"\\t\\tAverage test concept accuracy = {avg_concept_accs[-1]:.4f}, \"\n",
    "                f\"average test concept AUC = {avg_concept_aucs[-1]:.4f}\"\n",
    "            )\n",
    "            print(\"\\t\\tDone with trial\", trial + 1)\n",
    "\n",
    "        avg_concept_acc_mean, avg_concept_acc_std = np.mean(avg_concept_accs), np.std(avg_concept_accs)\n",
    "        experiment_variables[\"latent_avg_concept_predictive_accuracies\"].append((avg_concept_acc_mean, avg_concept_acc_std))\n",
    "        print(f\"\\tTest task accuracy: {avg_concept_acc_mean:.4f} ± {avg_concept_acc_std:.4f}\")\n",
    "\n",
    "        avg_concept_auc_mean, avg_concept_auc_std = np.mean(avg_concept_aucs), np.std(avg_concept_aucs)\n",
    "        experiment_variables[\"latent_avg_concept_predictive_aucs\"].append((avg_concept_auc_mean, avg_concept_auc_std))\n",
    "        print(f\"\\tTest task AUC: {avg_concept_auc_mean:.4f} ± {avg_concept_auc_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(CBM)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "from_logits_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=300,\n",
    "    warmup_epochs=0,\n",
    "    pre_train_epochs=0,\n",
    "    trials=NUM_TRIALS,\n",
    "    alpha=0.1,\n",
    "    learning_rate=1e-3,\n",
    "    encoder_units=[128, 64],\n",
    "    decoder_units=[128, 64],\n",
    "    num_outputs=1,\n",
    "    \n",
    "    latent_decoder_units=[128, 64],\n",
    "    predictor_max_epochs=300,\n",
    "    concept_predictor_max_epochs=300,\n",
    "    \n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        \"cbm/from_logits\"\n",
    "    ),\n",
    "    input_shape=[7],\n",
    "    num_concepts=3,\n",
    "    latent_dims=0,\n",
    "    holdout_fraction=0.1,\n",
    "    train_samples=2000,\n",
    "    test_samples=1000,\n",
    "    covariances=np.arange(0, 1, 0.1),\n",
    "    verbosity=0,\n",
    "    encoder_output_logits=True,\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(from_logits_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "from_logits_figure_dir = os.path.join(from_logits_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(from_logits_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "from_logits_results = cbm_experiment_loop(\n",
    "    from_logits_experiment_config,\n",
    "    load_from_cache=LOAD_FROM_CACHE,\n",
    ")\n",
    "print(\"task_accuracies:\", from_logits_results[\"task_accuracies\"])\n",
    "print(\"concept_accuracies:\", from_logits_results[\"concept_accuracies\"])\n",
    "print(\"task_aucs:\", from_logits_results[\"task_aucs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_logits_results.update(cbm_bottleneck_predict_experiment_loop(\n",
    "    from_logits_experiment_config,\n",
    "    load_from_cache=LOAD_FROM_CACHE,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_logits_results.update(cbm_bottleneck_concept_predict_experiment_loop(\n",
    "    from_logits_experiment_config,\n",
    "    load_from_cache=LOAD_FROM_CACHE,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(CBM)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "base_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=300,\n",
    "    warmup_epochs=0,\n",
    "    pre_train_epochs=0,\n",
    "    trials=NUM_TRIALS,\n",
    "    alpha=0.1,\n",
    "    learning_rate=1e-3,\n",
    "    encoder_units=[128, 64],\n",
    "    decoder_units=[128, 64],\n",
    "    num_outputs=1,\n",
    "    \n",
    "    latent_decoder_units=[128, 64],\n",
    "    predictor_max_epochs=300,\n",
    "    concept_predictor_max_epochs=300,\n",
    "    \n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        \"cbm/base\"\n",
    "    ),\n",
    "    input_shape=[7],\n",
    "    num_concepts=3,\n",
    "    latent_dims=0,\n",
    "    holdout_fraction=0.1,\n",
    "    train_samples=2000,\n",
    "    test_samples=1000,\n",
    "    covariances=np.arange(0, 1, 0.1),\n",
    "    verbosity=0,\n",
    "    encoder_output_logits=False,\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(base_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "base_figure_dir = os.path.join(base_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(base_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "base_results = cbm_experiment_loop(\n",
    "    base_experiment_config,\n",
    "    load_from_cache=LOAD_FROM_CACHE,\n",
    ")\n",
    "print(\"task_accuracies:\", base_results[\"task_accuracies\"])\n",
    "print(\"concept_accuracies:\", base_results[\"concept_accuracies\"])\n",
    "print(\"task_aucs:\", base_results[\"task_aucs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_results.update(cbm_bottleneck_predict_experiment_loop(\n",
    "    base_experiment_config,\n",
    "    load_from_cache=LOAD_FROM_CACHE,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_results.update(cbm_bottleneck_concept_predict_experiment_loop(\n",
    "    base_experiment_config,\n",
    "    load_from_cache=LOAD_FROM_CACHE,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capacity Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbm_capacity_experiment_loop(experiment_config, load_from_cache=False):\n",
    "    experiment_variables = dict(\n",
    "        task_accuracies=[],\n",
    "        task_aucs=[],\n",
    "        concept_accuracies=[],\n",
    "        purity_scores=[],\n",
    "        non_oracle_purity_scores=[],\n",
    "        purity_matrices=[],\n",
    "        oracle_matrices=[],\n",
    "    )\n",
    "    utils.reseed(87)\n",
    "    experiment_config[\"data_concepts\"] = experiment_config.get(\n",
    "        \"data_concepts\",\n",
    "        experiment_config[\"num_concepts\"],\n",
    "    )\n",
    "    \n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(experiment_config[\"model_units\"]):\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)}.'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    utils.serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "    \n",
    "    # Let's save our config here either way\n",
    "    utils.serialize_experiment_config(\n",
    "        experiment_config,\n",
    "        experiment_config[\"results_dir\"],\n",
    "    )\n",
    "    \n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    for units in experiment_config[\"model_units\"][start_ind:]:\n",
    "        print(\"Training with units:\", [units, units//2])\n",
    "        task_accs = []\n",
    "        concept_accs = []\n",
    "        aucs = []\n",
    "        purity_mats = []\n",
    "        oracle_mats = []\n",
    "        purities = []\n",
    "        non_oracle_purities = []\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']}\")\n",
    "            # First construct the dataset\n",
    "            (x_train, y_train, y_train_concepts) = produce_data(\n",
    "                experiment_config[\"train_samples\"],\n",
    "                cov=0,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            (x_test, y_test, y_test_concepts) = produce_data(\n",
    "                experiment_config[\"test_samples\"],\n",
    "                cov=0,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            \n",
    "            # Then proceed to do and end-to-end model in case we want to\n",
    "            # do some task-specific pretraining\n",
    "            end_to_end_model, encoder, decoder = construct_end_to_end_model(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                encoder=construct_encoder(\n",
    "                    input_shape=experiment_config[\"input_shape\"],\n",
    "                    units=[units, units//2],\n",
    "                    num_concepts=experiment_config[\"num_concepts\"],\n",
    "                    end_activation=\"sigmoid\",\n",
    "                    latent_dims=experiment_config[\"latent_dims\"],\n",
    "                ),\n",
    "                decoder=construct_decoder(\n",
    "                    units=[units, units//2],\n",
    "                    num_outputs=experiment_config[\"num_outputs\"],\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            if experiment_config.get(\"pre_train_epochs\"):\n",
    "                print(\"\\tModel pre-training...\")\n",
    "                end_to_end_model.fit(\n",
    "                    x=x_train,\n",
    "                    y=y_train,\n",
    "                    epochs=experiment_config[\"pre_train_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "                print(\"\\t\\tModel pre-training completed\")\n",
    "            \n",
    "            # Now time to actually construct and train the CBM\n",
    "            cbm_model = construct_cbm(\n",
    "                encoder=encoder,\n",
    "                decoder=decoder,\n",
    "                alpha=experiment_config[\"alpha\"],\n",
    "                learning_rate=experiment_config[\"learning_rate\"],\n",
    "                latent_dims=experiment_config.get(\"latent_dims\", 0),\n",
    "                encoder_output_logits=experiment_config.get(\"encoder_output_logits\", False),\n",
    "            )\n",
    "\n",
    "            early_stopping_monitor = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_concept_accuracy\",\n",
    "                min_delta=experiment_config[\"min_delta\"],\n",
    "                patience=experiment_config[\"patience\"],\n",
    "                restore_best_weights=True,\n",
    "                verbose=2,\n",
    "                mode='max',\n",
    "            )\n",
    "            if experiment_config[\"warmup_epochs\"]:\n",
    "                print(\"\\tWarmup training...\")\n",
    "                cbm_model.fit(\n",
    "                    x=x_train,\n",
    "                    y=(\n",
    "                        y_train,\n",
    "                        y_train_concepts[:, :experiment_config[\"num_concepts\"]],\n",
    "                    ),\n",
    "                    epochs=experiment_config[\"warmup_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "                print(\"\\t\\tWarmup training completed\")\n",
    "\n",
    "\n",
    "            print(\"\\tCBM training...\")\n",
    "            cbm_model.fit(\n",
    "                x=x_train,\n",
    "                y=(\n",
    "                    y_train,\n",
    "                    y_train_concepts[:, :experiment_config[\"num_concepts\"]],\n",
    "                ),\n",
    "                epochs=experiment_config[\"max_epochs\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                callbacks=[\n",
    "                    early_stopping_monitor,\n",
    "                ],\n",
    "                validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                verbose=verbosity,\n",
    "            )\n",
    "            print(\"\\t\\tCBM training completed\")\n",
    "            print(\"\\tSerializing model\")\n",
    "            encoder.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/encoder_capacity_{units}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            decoder.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/decoder_capacity_{units}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            print(\"\\tEvaluating model\")\n",
    "            test_result = cbm_model.evaluate(\n",
    "                x_test,\n",
    "                (\n",
    "                    y_test,\n",
    "                    y_test_concepts[:, :experiment_config[\"num_concepts\"]],\n",
    "                ),\n",
    "                verbose=0,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            task_accs.append(test_result['binary_accuracy'])\n",
    "            concept_accs.append(test_result['concept_accuracy'])\n",
    "            aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                y_test,\n",
    "                cbm_model.predict(x_test)[0],\n",
    "            ))\n",
    "            \n",
    "            print(\n",
    "                f\"\\t\\tTest auc = {aucs[-1]:.4f}, \"\n",
    "                f\"test concept accuracy = {concept_accs[-1]:.4f}, \"\n",
    "                f\"task accuracy = {task_accs[-1]:.4f}\"\n",
    "            )\n",
    "\n",
    "            print(f\"\\t\\tComputing purity score...\")\n",
    "            soft_acts = (\n",
    "                np.concatenate(cbm_model.encoder(x_test), axis=-1)\n",
    "                if experiment_config[\"latent_dims\"] else encoder(x_test).numpy()\n",
    "            )\n",
    "            purity_score, purity_mat, oracle_mat = oracle.oracle_impurity_score(\n",
    "                c_soft=soft_acts,\n",
    "                c_true=y_test_concepts,\n",
    "                output_matrices=True,\n",
    "            )\n",
    "            purity_mats.append(purity_mat)\n",
    "            oracle_mats.append(oracle_mat)\n",
    "            purities.append(purity_score)\n",
    "            print(f\"\\t\\t\\tDone {purity_score:.4f}\")\n",
    "\n",
    "            print(\"\\t\\tComputing non-oracle purity score...\")\n",
    "        \n",
    "            non_oracle_purities.append(oracle.oracle_impurity_score(\n",
    "                c_soft=soft_acts,\n",
    "                c_true=y_test_concepts,\n",
    "                oracle_matrix=construct_trivial_auc_mat(\n",
    "                    experiment_config[\"data_concepts\"]\n",
    "                ),\n",
    "                purity_matrix=purity_mat,\n",
    "            ))\n",
    "            print(f\"\\t\\t\\tDone {non_oracle_purities[-1]:.4f}\")\n",
    "            print(\"\\t\\tDone with trial\", trial + 1)\n",
    "\n",
    "        task_acc_mean, task_acc_std = np.mean(task_accs), np.std(task_accs)\n",
    "        experiment_variables[\"task_accuracies\"].append((task_acc_mean, task_acc_std))\n",
    "        print(f\"\\tTest task accuracy: {task_acc_mean:.4f} ± {task_acc_std:.4f}\")\n",
    "\n",
    "        concept_acc_mean, concept_acc_std = np.mean(concept_accs), np.std(concept_accs)\n",
    "        experiment_variables[\"concept_accuracies\"].append((concept_acc_mean, concept_acc_std))\n",
    "        print(f\"\\tTest concept accuracy: {concept_acc_mean:.4f} ± {concept_acc_std:.4f}\")\n",
    "\n",
    "\n",
    "        task_auc_mean, task_auc_std = np.mean(aucs), np.std(aucs)\n",
    "        experiment_variables[\"task_aucs\"].append((task_auc_mean, task_auc_std))\n",
    "        print(f\"\\tTest task AUC: {task_auc_mean:.4f} ± {task_auc_std:.4f}\")\n",
    "\n",
    "        purity_mats = np.stack(purity_mats, axis=0)\n",
    "        purity_mat_mean = np.mean(purity_mats, axis=0)\n",
    "        purity_mat_std = np.std(purity_mats, axis=0)\n",
    "        print(\"\\tPurity matrix:\")\n",
    "        for i in range(purity_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(purity_mat_mean.shape[1]):\n",
    "                line += f'{purity_mat_mean[i, j]:.4f} ± {purity_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"purity_matrices\"].append((purity_mat_mean, purity_mat_std))\n",
    "\n",
    "\n",
    "        oracle_mats = np.stack(oracle_mats, axis=0)\n",
    "        oracle_mat_mean = np.mean(oracle_mats, axis=0)\n",
    "        oracle_mat_std = np.std(oracle_mats, axis=0)\n",
    "        print(\"\\tOracle matrix:\")\n",
    "        for i in range(oracle_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(oracle_mat_mean.shape[1]):\n",
    "                line += f'{oracle_mat_mean[i, j]:.4f} ± {oracle_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"oracle_matrices\"].append((oracle_mat_mean, oracle_mat_std))\n",
    "\n",
    "        purity_mean, purity_std = np.mean(purities), np.std(purities)\n",
    "        experiment_variables[\"purity_scores\"].append((purity_mean, purity_std))\n",
    "        print(f\"\\tPurity score: {purity_mean:.4f} ± {purity_std:.4f}\")\n",
    "\n",
    "        non_oracle_purity_mean, non_oracle_purity_std = np.mean(non_oracle_purities), np.std(non_oracle_purities)\n",
    "        experiment_variables[\"non_oracle_purity_scores\"].append((non_oracle_purity_mean, non_oracle_purity_std))\n",
    "        print(f\"\\tNon-oracle purity score: {non_oracle_purity_mean:.4f} ± {non_oracle_purity_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "capacity_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=300,\n",
    "    warmup_epochs=0,\n",
    "    pre_train_epochs=0,\n",
    "    trials=NUM_TRIALS,\n",
    "    alpha=0.1,\n",
    "    learning_rate=1e-3,\n",
    "    model_units=[256, 128, 64, 32, 16, 8, 4],\n",
    "    num_outputs=1,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        \"cbm/capacity_purity\",\n",
    "    ),\n",
    "    input_shape=[7],\n",
    "    num_concepts=3,\n",
    "    latent_dims=0,\n",
    "    holdout_fraction=0.1,\n",
    "    train_samples=2000,\n",
    "    test_samples=1000,\n",
    "    verbosity=0,\n",
    "    data_concepts=3,  # But we still use three concepts in the data\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(capacity_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "capacity_figure_dir = os.path.join(capacity_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(capacity_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "capacity_results = cbm_capacity_experiment_loop(\n",
    "    capacity_experiment_config,\n",
    "    load_from_cache=LOAD_FROM_CACHE,\n",
    ")\n",
    "print(\"task_accuracies:\", capacity_results[\"task_accuracies\"])\n",
    "print(\"concept_accuracies:\", capacity_results[\"concept_accuracies\"])\n",
    "print(\"task_aucs:\", capacity_results[\"task_aucs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Capacity Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbm_mixed_capacity_experiment_loop(experiment_config, load_from_cache=False):\n",
    "    experiment_variables = dict(\n",
    "        task_accuracies=[],\n",
    "        task_aucs=[],\n",
    "        concept_accuracies=[],\n",
    "        purity_scores=[],\n",
    "        non_oracle_purity_scores=[],\n",
    "        purity_matrices=[],\n",
    "        oracle_matrices=[],\n",
    "    )\n",
    "    utils.reseed(87)\n",
    "    experiment_config[\"data_concepts\"] = experiment_config.get(\n",
    "        \"data_concepts\",\n",
    "        experiment_config[\"num_concepts\"],\n",
    "    )\n",
    "    \n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(experiment_config[\"model_units\"]):\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)}.'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    utils.serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "    \n",
    "    # Let's save our config here either way\n",
    "    utils.serialize_experiment_config(\n",
    "        experiment_config,\n",
    "        experiment_config[\"results_dir\"],\n",
    "    )\n",
    "    \n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    for units in experiment_config[\"model_units\"][start_ind:]:\n",
    "        print(\"Training with units:\", [units, units//2])\n",
    "        task_accs = []\n",
    "        concept_accs = []\n",
    "        aucs = []\n",
    "        purity_mats = []\n",
    "        oracle_mats = []\n",
    "        purities = []\n",
    "        non_oracle_purities = []\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']}\")\n",
    "            # First construct the dataset\n",
    "            (x_train, y_train, y_train_concepts) = produce_data(\n",
    "                experiment_config[\"train_samples\"],\n",
    "                cov=0,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            (x_test, y_test, y_test_concepts) = produce_data(\n",
    "                experiment_config[\"test_samples\"],\n",
    "                cov=0,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            \n",
    "            # Then proceed to do and end-to-end model in case we want to\n",
    "            # do some task-specific pretraining\n",
    "            encoder_units = (\n",
    "                [units, units//2] if experiment_config[\"encoder_experiment\"]\n",
    "                else experiment_config[\"encoder_units\"]\n",
    "            )\n",
    "            decoder_units = (\n",
    "                [units, units//2] if (not experiment_config[\"encoder_experiment\"])\n",
    "                else experiment_config[\"decoder_units\"]\n",
    "            )\n",
    "            end_to_end_model, encoder, decoder = construct_end_to_end_model(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                encoder=construct_encoder(\n",
    "                    input_shape=experiment_config[\"input_shape\"],\n",
    "                    units=encoder_units,\n",
    "                    num_concepts=experiment_config[\"num_concepts\"],\n",
    "                    end_activation=\"sigmoid\",\n",
    "                    latent_dims=experiment_config[\"latent_dims\"],\n",
    "                    output_logits=experiment_config.get(\"encoder_output_logits\", False),\n",
    "                ),\n",
    "                decoder=construct_decoder(\n",
    "                    units=decoder_units,\n",
    "                    num_outputs=experiment_config[\"num_outputs\"],\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            if experiment_config.get(\"pre_train_epochs\"):\n",
    "                print(\"\\tModel pre-training...\")\n",
    "                end_to_end_model.fit(\n",
    "                    x=x_train,\n",
    "                    y=y_train,\n",
    "                    epochs=experiment_config[\"pre_train_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "                print(\"\\t\\tModel pre-training completed\")\n",
    "            \n",
    "            # Now time to actually construct and train the CBM\n",
    "            cbm_model = construct_cbm(\n",
    "                encoder=encoder,\n",
    "                decoder=decoder,\n",
    "                alpha=experiment_config[\"alpha\"],\n",
    "                learning_rate=experiment_config[\"learning_rate\"],\n",
    "                latent_dims=experiment_config.get(\"latent_dims\", 0),\n",
    "                encoder_output_logits=experiment_config.get(\"encoder_output_logits\", False),\n",
    "            )\n",
    "\n",
    "            early_stopping_monitor = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_concept_accuracy\",\n",
    "                min_delta=experiment_config[\"min_delta\"],\n",
    "                patience=experiment_config[\"patience\"],\n",
    "                restore_best_weights=True,\n",
    "                verbose=2,\n",
    "                mode='max',\n",
    "            )\n",
    "            if experiment_config[\"warmup_epochs\"]:\n",
    "                print(\"\\tWarmup training...\")\n",
    "                cbm_model.fit(\n",
    "                    x=x_train,\n",
    "                    y=(\n",
    "                        y_train,\n",
    "                        y_train_concepts[:, :experiment_config[\"num_concepts\"]],\n",
    "                    ),\n",
    "                    epochs=experiment_config[\"warmup_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "                print(\"\\t\\tWarmup training completed\")\n",
    "\n",
    "\n",
    "            print(\"\\tCBM training...\")\n",
    "            cbm_model.fit(\n",
    "                x=x_train,\n",
    "                y=(\n",
    "                    y_train,\n",
    "                    y_train_concepts[:, :experiment_config[\"num_concepts\"]],\n",
    "                ),\n",
    "                epochs=experiment_config[\"max_epochs\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                callbacks=[\n",
    "                    early_stopping_monitor,\n",
    "                ],\n",
    "                validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                verbose=verbosity,\n",
    "            )\n",
    "            print(\"\\t\\tCBM training completed\")\n",
    "            print(\"\\tSerializing model\")\n",
    "            encoder.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/encoder_capacity_{encoder_units[0]}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            decoder.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/decoder_capacity_{decoder_units[0]}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            print(\"\\tEvaluating model\")\n",
    "            test_result = cbm_model.evaluate(\n",
    "                x_test,\n",
    "                (\n",
    "                    y_test,\n",
    "                    y_test_concepts[:, :experiment_config[\"num_concepts\"]],\n",
    "                ),\n",
    "                verbose=0,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            task_accs.append(test_result['binary_accuracy'])\n",
    "            concept_accs.append(test_result['concept_accuracy'])\n",
    "            aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                y_test,\n",
    "                cbm_model.predict(x_test)[0],\n",
    "            ))\n",
    "            \n",
    "            print(\n",
    "                f\"\\t\\tTest auc = {aucs[-1]:.4f}, \"\n",
    "                f\"test concept accuracy = {concept_accs[-1]:.4f}, \"\n",
    "                f\"task accuracy = {task_accs[-1]:.4f}\"\n",
    "            )\n",
    "\n",
    "            print(f\"\\t\\tComputing purity score...\")\n",
    "            soft_acts = (\n",
    "                np.concatenate(cbm_model.encoder(x_test), axis=-1)\n",
    "                if experiment_config[\"latent_dims\"] else encoder(x_test).numpy()\n",
    "            )\n",
    "            purity_score, purity_mat, oracle_mat = oracle.oracle_impurity_score(\n",
    "                c_soft=soft_acts,\n",
    "                c_true=y_test_concepts,\n",
    "                output_matrices=True,\n",
    "            )\n",
    "            purity_mats.append(purity_mat)\n",
    "            oracle_mats.append(oracle_mat)\n",
    "            purities.append(purity_score)\n",
    "            print(f\"\\t\\t\\tDone {purity_score:.4f}\")\n",
    "\n",
    "            print(\"\\t\\tComputing non-oracle purity score...\")\n",
    "        \n",
    "            non_oracle_purities.append(oracle.oracle_impurity_score(\n",
    "                c_soft=soft_acts,\n",
    "                c_true=y_test_concepts,\n",
    "                oracle_matrix=construct_trivial_auc_mat(\n",
    "                    experiment_config[\"data_concepts\"]\n",
    "                ),\n",
    "                purity_matrix=purity_mat,\n",
    "            ))\n",
    "            print(f\"\\t\\t\\tDone {non_oracle_purities[-1]:.4f}\")\n",
    "            print(\"\\t\\tDone with trial\", trial + 1)\n",
    "\n",
    "        task_acc_mean, task_acc_std = np.mean(task_accs), np.std(task_accs)\n",
    "        experiment_variables[\"task_accuracies\"].append((task_acc_mean, task_acc_std))\n",
    "        print(f\"\\tTest task accuracy: {task_acc_mean:.4f} ± {task_acc_std:.4f}\")\n",
    "\n",
    "        concept_acc_mean, concept_acc_std = np.mean(concept_accs), np.std(concept_accs)\n",
    "        experiment_variables[\"concept_accuracies\"].append((concept_acc_mean, concept_acc_std))\n",
    "        print(f\"\\tTest concept accuracy: {concept_acc_mean:.4f} ± {concept_acc_std:.4f}\")\n",
    "\n",
    "\n",
    "        task_auc_mean, task_auc_std = np.mean(aucs), np.std(aucs)\n",
    "        experiment_variables[\"task_aucs\"].append((task_auc_mean, task_auc_std))\n",
    "        print(f\"\\tTest task AUC: {task_auc_mean:.4f} ± {task_auc_std:.4f}\")\n",
    "\n",
    "        purity_mats = np.stack(purity_mats, axis=0)\n",
    "        purity_mat_mean = np.mean(purity_mats, axis=0)\n",
    "        purity_mat_std = np.std(purity_mats, axis=0)\n",
    "        print(\"\\tPurity matrix:\")\n",
    "        for i in range(purity_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(purity_mat_mean.shape[1]):\n",
    "                line += f'{purity_mat_mean[i, j]:.4f} ± {purity_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"purity_matrices\"].append((purity_mat_mean, purity_mat_std))\n",
    "\n",
    "\n",
    "        oracle_mats = np.stack(oracle_mats, axis=0)\n",
    "        oracle_mat_mean = np.mean(oracle_mats, axis=0)\n",
    "        oracle_mat_std = np.std(oracle_mats, axis=0)\n",
    "        print(\"\\tOracle matrix:\")\n",
    "        for i in range(oracle_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(oracle_mat_mean.shape[1]):\n",
    "                line += f'{oracle_mat_mean[i, j]:.4f} ± {oracle_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"oracle_matrices\"].append((oracle_mat_mean, oracle_mat_std))\n",
    "\n",
    "        purity_mean, purity_std = np.mean(purities), np.std(purities)\n",
    "        experiment_variables[\"purity_scores\"].append((purity_mean, purity_std))\n",
    "        print(f\"\\tPurity score: {purity_mean:.4f} ± {purity_std:.4f}\")\n",
    "\n",
    "        non_oracle_purity_mean, non_oracle_purity_std = np.mean(non_oracle_purities), np.std(non_oracle_purities)\n",
    "        experiment_variables[\"non_oracle_purity_scores\"].append((non_oracle_purity_mean, non_oracle_purity_std))\n",
    "        print(f\"\\tNon-oracle purity score: {non_oracle_purity_mean:.4f} ± {non_oracle_purity_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "encoder_capacity_logits_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=300,\n",
    "    warmup_epochs=0,\n",
    "    pre_train_epochs=0,\n",
    "    trials=NUM_TRIALS,\n",
    "    alpha=0.1,\n",
    "    learning_rate=1e-3,\n",
    "    model_units=[256, 128, 64, 32, 16, 8, 4],\n",
    "    decoder_units=[128, 64],\n",
    "    encoder_experiment=True,\n",
    "    \n",
    "    num_outputs=1,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        \"cbm/encoder_capacity_logits_purity\",\n",
    "    ),\n",
    "    input_shape=[7],\n",
    "    num_concepts=3,\n",
    "    latent_dims=0,\n",
    "    holdout_fraction=0.1,\n",
    "    train_samples=2000,\n",
    "    test_samples=1000,\n",
    "    verbosity=0,\n",
    "    data_concepts=3,  # But we still use three concepts in the data\n",
    "    encoder_output_logits=True,\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(encoder_capacity_logits_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "encoder_capacity_logits_figure_dir = os.path.join(encoder_capacity_logits_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(encoder_capacity_logits_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "encoder_capacity_logits_results = cbm_mixed_capacity_experiment_loop(\n",
    "    encoder_capacity_logits_experiment_config,\n",
    "    load_from_cache=LOAD_FROM_CACHE,\n",
    ")\n",
    "print(\"task_accuracies:\", encoder_capacity_logits_results[\"task_accuracies\"])\n",
    "print(\"concept_accuracies:\", encoder_capacity_logits_results[\"concept_accuracies\"])\n",
    "print(\"task_aucs:\", encoder_capacity_logits_results[\"task_aucs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "decoder_capacity_logits_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=300,\n",
    "    warmup_epochs=0,\n",
    "    pre_train_epochs=0,\n",
    "    trials=NUM_TRIALS,\n",
    "    alpha=0.1,\n",
    "    learning_rate=1e-3,\n",
    "    model_units=[256, 128, 64, 32, 16, 8, 4],\n",
    "    encoder_units=[128, 64],\n",
    "    encoder_experiment=False,\n",
    "    \n",
    "    num_outputs=1,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        \"cbm/decoder_capacity_logits_purity\",\n",
    "    ),\n",
    "    input_shape=[7],\n",
    "    num_concepts=3,\n",
    "    latent_dims=0,\n",
    "    holdout_fraction=0.1,\n",
    "    train_samples=2000,\n",
    "    test_samples=1000,\n",
    "    verbosity=0,\n",
    "    data_concepts=3,  # But we still use three concepts in the data\n",
    "    encoder_output_logits=True,\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(decoder_capacity_logits_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "decoder_capacity_logits_figure_dir = os.path.join(decoder_capacity_logits_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(decoder_capacity_logits_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "decoder_capacity_logits_results = cbm_mixed_capacity_experiment_loop(\n",
    "    decoder_capacity_logits_experiment_config,\n",
    "    load_from_cache=LOAD_FROM_CACHE,\n",
    ")\n",
    "print(\"task_accuracies:\", decoder_capacity_logits_results[\"task_accuracies\"])\n",
    "print(\"concept_accuracies:\", decoder_capacity_logits_results[\"concept_accuracies\"])\n",
    "print(\"task_aucs:\", decoder_capacity_logits_results[\"task_aucs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept Whitening Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.CW.CWLayer as CW\n",
    "\n",
    "def construct_cw_model(\n",
    "    input_shape,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    learning_rate=1e-3,\n",
    "    activation=tf.keras.activations.relu,\n",
    "    activation_mode='max_pool_mean',\n",
    "):\n",
    "    model_inputs = tf.keras.Input(shape=input_shape)\n",
    "    cw_layer = CW.ConceptWhiteningLayer(\n",
    "        activation_mode=activation_mode,\n",
    "    )\n",
    "    cw_model = tf.keras.Model(\n",
    "        model_inputs,\n",
    "        cw_layer(encoder(model_inputs)),\n",
    "        name=\"cw_model\",\n",
    "    )\n",
    "    \n",
    "    # Now time to collapse all the concepts again back into a single vector\n",
    "    model = tf.keras.Model(\n",
    "        model_inputs,\n",
    "        decoder(activation(cw_layer(encoder(model_inputs)))),\n",
    "        name=\"complete_model\",\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[\"binary_accuracy\"],\n",
    "    )\n",
    "    return model, cw_model\n",
    "\n",
    "def channels_corr_mat(outputs):\n",
    "    if len(outputs.shape) == 2:\n",
    "        outputs = np.expand_dims(\n",
    "            np.expand_dims(outputs, axis=1),\n",
    "            axis=1,\n",
    "        )\n",
    "    # Change (N, H, W, C) to (C, N, H, W)\n",
    "    outputs = np.transpose(outputs, [3, 0, 1, 2])\n",
    "    # Change (C, N, H, W) to (C, NxHxW)\n",
    "    cnhw_shape = outputs.shape\n",
    "    outputs = np.transpose(np.reshape(outputs, [cnhw_shape[0], -1]))\n",
    "    outputs -= np.mean(outputs, axis=0, keepdims=True)\n",
    "    outputs = outputs / np.std(outputs, axis=0, keepdims=True)\n",
    "    return np.dot(outputs.transpose(), outputs) / outputs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.evaluation.metrics.leakage as leakage\n",
    "\n",
    "def cw_experiment_loop(experiment_config, load_from_cache=False):\n",
    "    experiment_variables = dict(\n",
    "        task_accuracies=[],\n",
    "        task_aucs=[],\n",
    "        concept_aucs=[],\n",
    "        purity_scores=[],\n",
    "        non_oracle_purity_scores=[],\n",
    "        purity_matrices=[],\n",
    "        oracle_matrices=[],\n",
    "        similarity_ratio_matrices=[],\n",
    "        correlation_matrices=[],\n",
    "    )\n",
    "    utils.reseed(87)\n",
    "    experiment_config[\"data_concepts\"] = experiment_config.get(\n",
    "        \"data_concepts\",\n",
    "        experiment_config[\"num_concepts\"],\n",
    "    )\n",
    "    \n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(experiment_config[\"covariances\"]):\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)}.'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    utils.serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "    \n",
    "    # Let's save our config here either way\n",
    "    utils.serialize_experiment_config(\n",
    "        experiment_config,\n",
    "        experiment_config[\"results_dir\"],\n",
    "    )\n",
    "    \n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    for cov in experiment_config[\"covariances\"][start_ind:]:\n",
    "        print(\"Training with covariance:\", cov)\n",
    "        task_accs = []\n",
    "        c_aucs = []\n",
    "        aucs = []\n",
    "        purity_mats = []\n",
    "        oracle_mats = []\n",
    "        purities = []\n",
    "        non_oracle_purities = []\n",
    "        similarities = []\n",
    "        correlations = []\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']}\")\n",
    "\n",
    "            # First construct the dataset\n",
    "            (x_train, y_train, y_train_concepts) = produce_data(\n",
    "                experiment_config[\"train_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            (x_test, y_test, y_test_concepts) = produce_data(\n",
    "                experiment_config[\"test_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            x_true_inds = (y_train_concepts[:, 0] == 1)\n",
    "            y_true_inds = (y_train_concepts[:, 1] == 1)\n",
    "            z_true_inds = (y_train_concepts[:, 2] == 1)\n",
    "            x_group_inds = np.logical_and(\n",
    "                x_true_inds,\n",
    "                np.logical_and(\n",
    "                    np.logical_not(y_true_inds),\n",
    "                    np.logical_not(z_true_inds),\n",
    "                )\n",
    "            )\n",
    "            y_group_inds = np.logical_and(\n",
    "                y_true_inds,\n",
    "                np.logical_and(\n",
    "                    np.logical_not(x_true_inds),\n",
    "                    np.logical_not(z_true_inds),\n",
    "                )\n",
    "            )\n",
    "            z_group_inds = np.logical_and(\n",
    "                z_true_inds,\n",
    "                np.logical_and(\n",
    "                    np.logical_not(x_true_inds),\n",
    "                    np.logical_not(y_true_inds),\n",
    "                )\n",
    "            )\n",
    "            exclusive_concept_groups = [\n",
    "                x_train[x_group_inds, :],\n",
    "                x_train[y_group_inds, :],\n",
    "                x_train[z_group_inds, :],\n",
    "            ][:experiment_config[\"data_concepts\"]]\n",
    "            \n",
    "            if not experiment_config.get(\"exclusive_concepts\", False):\n",
    "                x_group_inds = x_true_inds\n",
    "                y_group_inds = y_true_inds\n",
    "                z_group_inds = z_true_inds\n",
    "            concept_groups = [\n",
    "                x_train[x_group_inds, :],\n",
    "                x_train[y_group_inds, :],\n",
    "                x_train[z_group_inds, :],\n",
    "            ][:experiment_config[\"data_concepts\"]]\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Construct our CW model\n",
    "            encoder = construct_encoder(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                units=experiment_config[\"encoder_units\"],\n",
    "                num_concepts=experiment_config[\"num_concepts\"],\n",
    "                end_activation=None,\n",
    "                latent_dims=experiment_config[\"latent_dims\"],\n",
    "            )\n",
    "            decoder = construct_decoder(\n",
    "                units=experiment_config[\"decoder_units\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "            )\n",
    "            \n",
    "            model, cw_model = construct_cw_model(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                encoder=encoder,\n",
    "                decoder=decoder,\n",
    "                learning_rate=experiment_config[\"learning_rate\"],\n",
    "                activation=tf.keras.activations.relu,\n",
    "                activation_mode=experiment_config['activation_mode'],\n",
    "            )\n",
    "            \n",
    "            # First do some pretraining for warming up the estimates if needed\n",
    "            if experiment_config.get(\"pre_train_epochs\"):\n",
    "                print(\"\\tModel pre-training...\")\n",
    "                model.fit(\n",
    "                    x=x_train,\n",
    "                    y=y_train,\n",
    "                    epochs=experiment_config[\"pre_train_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "                fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "                similarity_ratio = oracle.concept_similarity_matrix(\n",
    "                    concept_representations=list(map(\n",
    "                        lambda x: cw_model(x).numpy(),\n",
    "                        concept_groups\n",
    "                    )),\n",
    "                    compute_ratios=True,\n",
    "                )\n",
    "                im, cbar = utils.heatmap(\n",
    "                    similarity_ratio,\n",
    "                    [f\"$c_{i}$\" for i in range(len(concept_groups))],\n",
    "                    [f\"$c_{i}$\" for i in range(len(concept_groups))],\n",
    "                    ax=ax,\n",
    "                    cmap=\"magma\",\n",
    "                    cbarlabel=f\"Similarity Ratio\",\n",
    "                    vmin=0,\n",
    "                    vmax=1,\n",
    "                )\n",
    "                texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "                fig.tight_layout()\n",
    "\n",
    "                fig.suptitle(f\"Baseline Concept Axis Separability\", fontsize=25)\n",
    "                fig.subplots_adjust(top=0.85)\n",
    "                plt.show()\n",
    "                print(\"\\t\\tModel pre-training completed\")\n",
    "            \n",
    "            # Set up the dataset in a nice usable form for unrolling the training\n",
    "            # loop\n",
    "            main_dataset_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            main_dataset_loader = main_dataset_loader.shuffle(buffer_size=1000).batch(\n",
    "                experiment_config[\"batch_size\"]\n",
    "            )\n",
    "            \n",
    "            min_size = min(list(map(lambda x: x.shape[0], concept_groups)))\n",
    "            print(\"Minimum size is\", min_size, \"given concept datasets\", list(map(lambda x: x.shape[0], concept_groups)))\n",
    "            concept_groups = list(map(lambda x: x[:min_size, :], concept_groups))\n",
    "            concept_group_loader = tf.data.Dataset.from_tensor_slices(tuple(concept_groups))\n",
    "            concept_group_loader = concept_group_loader.shuffle(buffer_size=1000).batch(\n",
    "                experiment_config[\"batch_size\"]\n",
    "            )\n",
    "\n",
    "            @tf.function\n",
    "            def _train_step(model, x_batch_train, y_batch_train):\n",
    "                # Update the other model parameters\n",
    "                with tf.GradientTape() as tape:\n",
    "                    logits = model(x_batch_train, training=True)\n",
    "                    loss_value = model.loss(y_batch_train, logits)\n",
    "\n",
    "                grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "                model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "                return loss_value\n",
    "            \n",
    "            total_steps = 0\n",
    "            for epoch in range(experiment_config[\"max_epochs\"]):\n",
    "                for current_step, (x_batch_train, y_batch_train) in enumerate(main_dataset_loader):\n",
    "                    print(\n",
    "                        f'Epoch {epoch + 1} and step {current_step}/{int(np.ceil(x_train.shape[0] / experiment_config[\"batch_size\"]))}         ',\n",
    "                        end=\"\\r\",\n",
    "                    )\n",
    "                    # Need to update the rotation matrix\n",
    "                    if (total_steps + 1) % experiment_config[\"cw_train_freq\"] == 0:\n",
    "                        for _ in range(experiment_config.get(\"cw_train_iterations\", 1)):\n",
    "                            cw_batch_steps = 0\n",
    "                            for concept_groups_batch in concept_group_loader:\n",
    "                                if cw_batch_steps > experiment_config.get(\"cw_train_batch_steps\", float(\"inf\")):\n",
    "                                    break\n",
    "                                model.layers[experiment_config[\"cw_layer\"]].update_rotation_matrix(\n",
    "                                    concept_groups=list(map(lambda x: encoder(x), concept_groups_batch)),\n",
    "                                )\n",
    "                                cw_batch_steps += 1\n",
    "                    if experiment_config.get(\"concept_auc_freq\"):\n",
    "                        if (total_steps % experiment_config[\"concept_auc_freq\"]) == 0:\n",
    "                            concept_aucs = leakage.compute_concept_aucs(\n",
    "                                cw_model=model,\n",
    "                                encoder=encoder,\n",
    "                                cw_layer=experiment_config[\"cw_layer\"],\n",
    "                                x_test=x_test,\n",
    "                                c_test=y_test_concepts,\n",
    "                                num_concepts=experiment_config[\"num_concepts\"],\n",
    "                            )\n",
    "                            print(\n",
    "                                f'Concept AUC at step {total_steps}:',\n",
    "                                concept_aucs\n",
    "                            )\n",
    "                    _train_step(model, x_batch_train, y_batch_train)\n",
    "                    total_steps += 1\n",
    "            \n",
    "            if experiment_config.get(\"post_cw_train_epochs\"):\n",
    "                for post_epoch in range(experiment_config.get(\"post_cw_train_epochs\", 0)):\n",
    "                    cw_batch_steps = 0\n",
    "                    steps_in_batch = len(concept_group_loader)\n",
    "                    for concept_groups_batch in concept_group_loader:\n",
    "                        print(\n",
    "                            f'Post epoch {post_epoch + 1} and step {cw_batch_steps}/{steps_in_batch}         ',\n",
    "                            end=\"\\r\",\n",
    "                        )\n",
    "                        model.layers[experiment_config[\"cw_layer\"]].update_rotation_matrix(\n",
    "                            concept_groups=list(map(lambda x: encoder(x), concept_groups_batch)),\n",
    "                        )\n",
    "                        cw_batch_steps += 1\n",
    "            \n",
    "            print(\"\\t\\tCW training completed\")\n",
    "            print(\"\\tSerializing model\")\n",
    "            model.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/end_to_end_model_cov_{cov:.1f}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            print(\"\\tEvaluating model\")\n",
    "            test_result = model.evaluate(\n",
    "                x_test,\n",
    "                y_test,\n",
    "                verbose=0,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            task_accs.append(test_result['binary_accuracy'])\n",
    "            c_aucs.append(leakage.compute_concept_aucs(\n",
    "                cw_model=model,\n",
    "                encoder=encoder,\n",
    "                cw_layer=experiment_config[\"cw_layer\"],\n",
    "                x_test=x_test,\n",
    "                c_test=y_test_concepts,\n",
    "                num_concepts=experiment_config[\"num_concepts\"],\n",
    "            ))\n",
    "            \n",
    "            aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                y_test,\n",
    "                model.predict(x_test),\n",
    "            ))\n",
    "            \n",
    "            print(\n",
    "                f\"\\t\\tTest auc = {aucs[-1]:.4f}, \"\n",
    "                f\"test concept AUCs = {c_aucs[-1]}, \"\n",
    "                f\"task accuracy = {task_accs[-1]:.4f}\"\n",
    "            )\n",
    "            \n",
    "            \n",
    "            print(\"\\t\\tComputing purity score...\")\n",
    "            concept_scores = cw_model.layers[-1].concept_scores(\n",
    "                encoder(x_test),\n",
    "                aggregator=experiment_config['aggregator'],\n",
    "            ).numpy()\n",
    "            purity_score, purity_mat, oracle_mat = oracle.oracle_impurity_score(\n",
    "                c_soft=concept_scores,\n",
    "                c_true=y_test_concepts,\n",
    "                output_matrices=True,\n",
    "            )\n",
    "            purity_mats.append(purity_mat)\n",
    "            oracle_mats.append(oracle_mat)\n",
    "            purities.append(purity_score)\n",
    "            \n",
    "            print(f\"\\t\\t\\tDone {purity_score:.4f}\")\n",
    "\n",
    "            print(\"\\t\\tComputing non-oracle purity score...\")\n",
    "            non_oracle_purities.append(oracle.oracle_impurity_score(\n",
    "                c_soft=concept_scores,\n",
    "                c_true=y_test_concepts,\n",
    "                oracle_matrix=construct_trivial_auc_mat(\n",
    "                    experiment_config[\"data_concepts\"]\n",
    "                ),\n",
    "                purity_matrix=purity_mat,\n",
    "            ))\n",
    "            print(f\"\\t\\t\\tDone {non_oracle_purities[-1]:.4f}\")\n",
    "            \n",
    "            print(\"\\t\\tComputing similarity ratios...\")\n",
    "            similarity_ratio = oracle.concept_similarity_matrix(\n",
    "                concept_representations=list(map(\n",
    "                    lambda x: cw_model(x).numpy(), #[:, :len(concept_groups)],\n",
    "                    concept_groups, #exclusive_concept_groups,  #concept_groups\n",
    "                )),\n",
    "                compute_ratios=True,\n",
    "            )\n",
    "            fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "            im, cbar = utils.heatmap(\n",
    "                similarity_ratio,\n",
    "                [f\"$x_+$\", f\"$y_+$\", f\"$z_+$\"][:experiment_config[\"num_concepts\"]],\n",
    "                [f\"$x_+$\", f\"$y_+$\", f\"$z_+$\"][:experiment_config[\"num_concepts\"]],\n",
    "                ax=ax,\n",
    "                cmap=\"magma\",\n",
    "                cbarlabel=f\"Similarity Ratio\",\n",
    "                vmin=0,\n",
    "                vmax=1,\n",
    "            )\n",
    "            texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "            fig.tight_layout()\n",
    "\n",
    "            fig.suptitle(f\"Concept Axis Separability\", fontsize=25)\n",
    "            fig.subplots_adjust(top=0.85)\n",
    "            plt.show()\n",
    "            similarities.append(similarity_ratio)\n",
    "            \n",
    "            # Compute correlation matrices\n",
    "            print(\"\\t\\tComputing correlation matrix...\")\n",
    "            corr_mat = channels_corr_mat(cw_model(x_test).numpy())\n",
    "            correlations.append(corr_mat)\n",
    "            fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "            im, cbar = utils.heatmap(\n",
    "                np.abs(corr_mat),\n",
    "                [f\"$f_{i}$\" for i in range(corr_mat.shape[-1])],\n",
    "                [f\"$f_{i}$\" for i in range(corr_mat.shape[-1])],\n",
    "                ax=ax,\n",
    "                cmap=\"magma\",\n",
    "                cbarlabel=f\"Correlation Coef\",\n",
    "                vmin=0,\n",
    "                vmax=1,\n",
    "            )\n",
    "            texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "            fig.tight_layout()\n",
    "\n",
    "            fig.suptitle(f\"Latent Dimension Correlation\", fontsize=25)\n",
    "            fig.subplots_adjust(top=0.85)\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "\n",
    "            print(\"\\t\\tDone with trial\", trial + 1)\n",
    "\n",
    "        task_acc_mean, task_acc_std = np.mean(task_accs), np.std(task_accs)\n",
    "        experiment_variables[\"task_accuracies\"].append((task_acc_mean, task_acc_std))\n",
    "        print(f\"\\tTest task accuracy: {task_acc_mean:.4f} ± {task_acc_std:.4f}\")\n",
    "\n",
    "        concept_aucs_mean = np.mean(np.stack(c_aucs, axis=0), axis=0)\n",
    "        concept_aucs_std = np.std(np.stack(c_aucs, axis=0), axis=0)\n",
    "        experiment_variables[\"concept_aucs\"].append((concept_aucs_mean, concept_aucs_std))\n",
    "        print(f\"\\tConcept AUCS:\")\n",
    "        line = \"\\t\\t\"\n",
    "        for i in range(concept_aucs_mean.shape[0]):\n",
    "            line += f'{concept_aucs_mean[i]:.4f} ± {concept_aucs_std[i]:.4f}    '\n",
    "        print(line)\n",
    "\n",
    "\n",
    "        task_auc_mean, task_auc_std = np.mean(aucs), np.std(aucs)\n",
    "        experiment_variables[\"task_aucs\"].append((task_auc_mean, task_auc_std))\n",
    "        print(f\"\\tTest task AUC: {task_auc_mean:.4f} ± {task_auc_std:.4f}\")\n",
    "\n",
    "        purity_mats = np.stack(purity_mats, axis=0)\n",
    "        purity_mat_mean = np.mean(purity_mats, axis=0)\n",
    "        purity_mat_std = np.std(purity_mats, axis=0)\n",
    "        print(\"\\tPurity matrix:\")\n",
    "        for i in range(purity_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(purity_mat_mean.shape[1]):\n",
    "                line += f'{purity_mat_mean[i, j]:.4f} ± {purity_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"purity_matrices\"].append((purity_mat_mean, purity_mat_std))\n",
    "        \n",
    "        similarities = np.stack(similarities, axis=0)\n",
    "        similarities_mean = np.mean(similarities, axis=0)\n",
    "        similarities_std = np.std(similarities, axis=0)\n",
    "        print(\"\\tSimilarity ratio matrix:\")\n",
    "        for i in range(similarities_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(similarities_mean.shape[1]):\n",
    "                line += f'{similarities_mean[i, j]:.4f} ± {similarities_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "        \n",
    "        fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "        im, cbar = utils.heatmap(\n",
    "            similarities_mean,\n",
    "            [f\"$x_+$\", f\"$y_+$\", f\"$z_+$\"][:experiment_config[\"num_concepts\"]],\n",
    "            [f\"$x_+$\", f\"$y_+$\", f\"$z_+$\"][:experiment_config[\"num_concepts\"]],\n",
    "            ax=ax,\n",
    "            cmap=\"magma\",\n",
    "            cbarlabel=f\"Similarity Ratio\",\n",
    "            vmin=0,\n",
    "            vmax=1,\n",
    "        )\n",
    "        texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "        fig.tight_layout()\n",
    "\n",
    "        fig.suptitle(f\"Mean Concept Axis Separability\", fontsize=25)\n",
    "        fig.subplots_adjust(top=0.85)\n",
    "        plt.show()\n",
    "        \n",
    "        experiment_variables[\"similarity_ratio_matrices\"].append(\n",
    "            (similarities_mean, similarities_std)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        correlations = np.stack(correlations, axis=0)\n",
    "        correlations_mean = np.mean(correlations, axis=0)\n",
    "        correlations_std = np.std(correlations, axis=0)\n",
    "        print(\"\\tCorrelation ratio matrix:\")\n",
    "        for i in range(correlations_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(correlations_mean.shape[1]):\n",
    "                line += f'{correlations_mean[i, j]:.4f} ± {correlations_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "        \n",
    "        fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "        im, cbar = utils.heatmap(\n",
    "            np.abs(corr_mat),\n",
    "            [f\"$f_{i}$\" for i in range(corr_mat.shape[-1])],\n",
    "            [f\"$f_{i}$\" for i in range(corr_mat.shape[-1])],\n",
    "            ax=ax,\n",
    "            cmap=\"magma\",\n",
    "            cbarlabel=f\"Mean Correlation Coef\",\n",
    "            vmin=0,\n",
    "            vmax=1,\n",
    "        )\n",
    "        texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "        fig.tight_layout()\n",
    "\n",
    "        fig.suptitle(f\"Latent Dimension Correlation\", fontsize=25)\n",
    "        fig.subplots_adjust(top=0.85)\n",
    "        plt.show()\n",
    "\n",
    "        experiment_variables[\"correlation_matrices\"].append(\n",
    "            (correlations_mean, correlations_std)\n",
    "        )\n",
    "\n",
    "\n",
    "        oracle_mats = np.stack(oracle_mats, axis=0)\n",
    "        oracle_mat_mean = np.mean(oracle_mats, axis=0)\n",
    "        oracle_mat_std = np.std(oracle_mats, axis=0)\n",
    "        print(\"\\tOracle matrix:\")\n",
    "        for i in range(oracle_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(oracle_mat_mean.shape[1]):\n",
    "                line += f'{oracle_mat_mean[i, j]:.4f} ± {oracle_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"oracle_matrices\"].append((oracle_mat_mean, oracle_mat_std))\n",
    "\n",
    "        purity_mean, purity_std = np.mean(purities), np.std(purities)\n",
    "        experiment_variables[\"purity_scores\"].append((purity_mean, purity_std))\n",
    "        print(f\"\\tPurity score: {purity_mean:.4f} ± {purity_std:.4f}\")\n",
    "\n",
    "        non_oracle_purity_mean, non_oracle_purity_std = np.mean(non_oracle_purities), np.std(non_oracle_purities)\n",
    "        experiment_variables[\"non_oracle_purity_scores\"].append((non_oracle_purity_mean, non_oracle_purity_std))\n",
    "        print(f\"\\tNon-oracle purity score: {non_oracle_purity_mean:.4f} ± {non_oracle_purity_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables\n",
    "\n",
    "# HACK: deserialization messes up with custome methods so reusing this here\n",
    "def concept_scores(\n",
    "    self,\n",
    "    inputs,\n",
    "    aggregator='max_pool_mean',\n",
    "    concept_indices=None,\n",
    "    data_format=\"channels_last\",\n",
    "):\n",
    "    outputs = self(inputs, training=False)\n",
    "    if len(tf.shape(outputs)) == 2:\n",
    "        # Then the scores are already computed by our forward pass\n",
    "        scores = outputs\n",
    "    else:\n",
    "        if data_format == \"channels_last\":\n",
    "            # Then we will transpose to make things simpler so that\n",
    "            # downstream we can always assume it is channels first\n",
    "            # NHWC -> NCHW\n",
    "            outputs = tf.transpose(\n",
    "                outputs,\n",
    "                perm=[0, 3, 1, 2],\n",
    "            )\n",
    "\n",
    "        # Else, we need to do some aggregation\n",
    "        if aggregator == 'mean':\n",
    "            # Compute the mean over all channels\n",
    "            scores = tf.math.reduce_mean(outputs, axis=[2, 3])\n",
    "        elif aggregator == 'max_pool_mean':\n",
    "            # First downsample using a max pool and then continue with\n",
    "            # a mean\n",
    "            window_size = min(\n",
    "                2,\n",
    "                outputs.shape[-1],\n",
    "                outputs.shape[-2],\n",
    "            )\n",
    "            scores = tf.nn.max_pool(\n",
    "                outputs,\n",
    "                ksize=window_size,\n",
    "                strides=window_size,\n",
    "                padding=\"SAME\",\n",
    "                data_format=\"NCHW\",\n",
    "            )\n",
    "            scores = tf.math.reduce_mean(scores, axis=[2, 3])\n",
    "        elif aggregator == 'max':\n",
    "            # Simply select the maximum value across a given channel\n",
    "            scores = tf.math.reduce_max(outputs, axis=[2, 3])\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported aggregator {aggregator}.')\n",
    "\n",
    "    if concept_indices is not None:\n",
    "        return scores[:, concept_indices]\n",
    "    return scores\n",
    "\n",
    "def cw_bottleneck_predict_experiment_loop(\n",
    "    experiment_config,\n",
    "    load_from_cache=False,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    experiment_variables = dict(\n",
    "        latent_predictive_accuracies=[],\n",
    "        latent_predictive_aucs=[],\n",
    "    )\n",
    "\n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                print(\"Could not find\", f'\"{file_name}_means.npz\" or \"{file_name}_stds.npz\" in cache.')\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(experiment_config[\"covariances\"]):\n",
    "                print(\"Found\", len(loaded_means), \"means for variable\", var_name, \"vs\", len(experiment_config[\"covariances\"]), \"datasets\")\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)} ({file_name}).'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            print(experiment_variables)\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    utils.serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    for cov in experiment_config[\"covariances\"][start_ind:]:\n",
    "        print(\"Training with covariance:\", cov)\n",
    "        task_accs = []\n",
    "        aucs = []\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} with covariance {cov}\")\n",
    "            # First construct the dataset\n",
    "            (x_train, y_train, y_train_concepts) = produce_data(\n",
    "                experiment_config[\"train_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            (x_test, y_test, y_test_concepts) = produce_data(\n",
    "                experiment_config[\"test_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            complete_model = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/end_to_end_model_cov_{cov:.1f}_trial_{trial}\"\n",
    "                ),\n",
    "            )\n",
    "            complete_model.summary()\n",
    "            cw_output_model = tf.keras.Model(\n",
    "                complete_model.inputs,\n",
    "                [complete_model.layers[experiment_config[\"cw_layer\"]].output],\n",
    "                name=\"cw_output_model\",\n",
    "            )\n",
    "            \n",
    "            feature_predictive_decoder = construct_decoder(\n",
    "                units=experiment_config[\"latent_decoder_units\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "            )\n",
    "            \n",
    "            encoder_model = tf.keras.Model(\n",
    "                complete_model.inputs,\n",
    "                [complete_model.layers[experiment_config[\"cw_layer\"] - 1](complete_model.inputs)],\n",
    "                name=\"cw_output_model\",\n",
    "            )\n",
    "            \n",
    "            score_predictive_decoder = construct_decoder(\n",
    "                units=experiment_config[\"latent_decoder_units\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "            )\n",
    "            \n",
    "            score_predictive_decoder.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(experiment_config[\"learning_rate\"]),\n",
    "                loss=(\n",
    "                    tf.keras.losses.BinaryCrossentropy(from_logits=True) if (experiment_config[\"num_outputs\"] <= 2)\n",
    "                    else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "                ),\n",
    "                metrics=[\n",
    "                    \"binary_accuracy\" if (experiment_config[\"num_outputs\"] <= 2)\n",
    "                    else tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            print(\"\\tTraining score model\")\n",
    "            score_train_codes = concept_scores(\n",
    "                cw_output_model.layers[-1],\n",
    "                encoder_model(x_train),\n",
    "                aggregator=experiment_config['aggregator'],\n",
    "            ).numpy()[:, list(range(experiment_config[\"num_concepts\"]))]\n",
    "            score_test_codes = concept_scores(\n",
    "                cw_output_model.layers[-1],\n",
    "                encoder_model(x_test),\n",
    "                aggregator=experiment_config['aggregator'],\n",
    "            ).numpy()[:, list(range(experiment_config[\"num_concepts\"]))]\n",
    "            score_predictive_decoder.fit(\n",
    "                x=score_train_codes,\n",
    "                y=y_train,\n",
    "                epochs=experiment_config[\"predictor_max_epochs\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                verbose=verbosity,\n",
    "            )\n",
    "            \n",
    "            \n",
    "            print(\"\\tEvaluating score model\")\n",
    "            test_result = score_predictive_decoder.evaluate(\n",
    "                score_test_codes,\n",
    "                y_test,\n",
    "                verbose=0,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            task_accs.append(\n",
    "                test_result['sparse_top_k_categorical_accuracy']\n",
    "                if experiment_config['num_outputs'] > 1 else\n",
    "                test_result['binary_accuracy']\n",
    "            )\n",
    "\n",
    "            if experiment_config['num_outputs'] > 1:\n",
    "                # Then lets apply a softmax activation over all the probability\n",
    "                # classes\n",
    "                preds = scipy.special.softmax(\n",
    "                    score_predictive_decoder.predict(score_test_codes),\n",
    "                    axis=-1,\n",
    "                )\n",
    "\n",
    "                # And select just the labels that are in fact being used\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    y_test,\n",
    "                    preds,\n",
    "                    multi_class='ovo',\n",
    "                ))\n",
    "            else:\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    y_test,\n",
    "                    score_predictive_decoder.predict(score_test_codes),\n",
    "                ))\n",
    "            \n",
    "            print(\n",
    "                f\"\\t\\tTest auc = {aucs[-1]:.4f}, \"\n",
    "                f\"task accuracy = {task_accs[-1]:.4f}\"\n",
    "            )\n",
    "            \n",
    "            print(\"\\t\\tDone with trial\", trial + 1)\n",
    "\n",
    "        task_acc_mean, task_acc_std = np.mean(task_accs), np.std(task_accs)\n",
    "        experiment_variables[\"latent_predictive_accuracies\"].append((task_acc_mean, task_acc_std))\n",
    "        print(f\"\\tTest task accuracy: {task_acc_mean:.4f} ± {task_acc_std:.4f}\")\n",
    "\n",
    "        task_auc_mean, task_auc_std = np.mean(aucs), np.std(aucs)\n",
    "        experiment_variables[\"latent_predictive_aucs\"].append((task_auc_mean, task_auc_std))\n",
    "        print(f\"\\tTest task AUC: {task_auc_mean:.4f} ± {task_auc_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables\n",
    "\n",
    "def cw_bottleneck_concept_predict_experiment_loop(\n",
    "    experiment_config,\n",
    "    load_from_cache=False,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    experiment_variables = dict(\n",
    "        latent_avg_concept_predictive_accuracies=[],\n",
    "        latent_avg_concept_predictive_aucs=[],\n",
    "    )\n",
    "\n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                print(\"Could not find\", f'\"{file_name}_means.npz\" or \"{file_name}_stds.npz\" in cache.')\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(experiment_config[\"covariances\"]):\n",
    "                print(\"Found\", len(loaded_means), \"means for variable\", var_name, \"vs\", len(experiment_config[\"covariances\"]), \"datasets\")\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)} ({file_name}).'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            print(experiment_variables)\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    utils.serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    for cov in experiment_config[\"covariances\"][start_ind:]:\n",
    "        print(\"Training with covariance:\", cov)\n",
    "        avg_concept_accs = []\n",
    "        avg_concept_aucs = []\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} with covariance {cov}\")\n",
    "            # First construct the dataset\n",
    "            (x_train, y_train, c_train) = produce_data(\n",
    "                experiment_config[\"train_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            (x_test, y_test, c_test) = produce_data(\n",
    "                experiment_config[\"test_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            complete_model = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/end_to_end_model_cov_{cov:.1f}_trial_{trial}\"\n",
    "                ),\n",
    "            )\n",
    "            complete_model.summary()\n",
    "            cw_output_model = tf.keras.Model(\n",
    "                complete_model.inputs,\n",
    "                [complete_model.layers[experiment_config[\"cw_layer\"]].output],\n",
    "                name=\"cw_output_model\",\n",
    "            )\n",
    "            \n",
    "            \n",
    "            encoder_model = tf.keras.Model(\n",
    "                complete_model.inputs,\n",
    "                [complete_model.layers[experiment_config[\"cw_layer\"] - 1](complete_model.inputs)],\n",
    "                name=\"cw_output_model\",\n",
    "            )\n",
    "            \n",
    "            current_aucs = []\n",
    "            current_accs = []\n",
    "            score_train_codes = concept_scores(\n",
    "                cw_output_model.layers[-1],\n",
    "                encoder_model(x_train),\n",
    "                aggregator=experiment_config['aggregator'],\n",
    "            ).numpy()[:, list(range(experiment_config[\"num_concepts\"]))]\n",
    "            score_test_codes = concept_scores(\n",
    "                cw_output_model.layers[-1],\n",
    "                encoder_model(x_test),\n",
    "                aggregator=experiment_config['aggregator'],\n",
    "            ).numpy()[:, list(range(experiment_config[\"num_concepts\"]))]\n",
    "            for concept_idx in range(experiment_config[\"num_concepts\"]):\n",
    "            \n",
    "                score_predictive_decoder = construct_decoder(\n",
    "                    units=experiment_config[\"latent_decoder_units\"],\n",
    "                    num_outputs=1,\n",
    "                )\n",
    "\n",
    "                score_predictive_decoder.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(experiment_config[\"learning_rate\"]),\n",
    "                    loss=(\n",
    "                        tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "                    ),\n",
    "                    metrics=[\n",
    "                        \"binary_accuracy\"\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "                print(\"\\t\\tTraining score model for concept\", concept_idx)\n",
    "                score_predictive_decoder.fit(\n",
    "                    x=score_train_codes,\n",
    "                    y=c_train[:, concept_idx],\n",
    "                    epochs=experiment_config[\"predictor_max_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "\n",
    "\n",
    "                print(\"\\t\\tEvaluating score model\")\n",
    "                test_result = score_predictive_decoder.evaluate(\n",
    "                    score_test_codes,\n",
    "                    c_test[:, concept_idx],\n",
    "                    verbose=0,\n",
    "                    return_dict=True,\n",
    "                )\n",
    "                current_accs.append(\n",
    "                    test_result['binary_accuracy']\n",
    "                )\n",
    "\n",
    "                current_aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    c_test[:, concept_idx],\n",
    "                    score_predictive_decoder.predict(score_test_codes),\n",
    "                ))\n",
    "                print(\n",
    "                    f\"\\t\\t\\tTest concept AUC = {current_aucs[-1]:.4f}, \"\n",
    "                    f\"concept accuracy = {current_accs[-1]:.4f}\"\n",
    "                )\n",
    "            avg_concept_aucs.append(np.mean(current_aucs))\n",
    "            avg_concept_accs.append(np.mean(current_accs))\n",
    "            print(\n",
    "                f\"\\t\\tTest avg concept AUC = {avg_concept_aucs[-1]:.4f}, \"\n",
    "                f\"avg concept accuracy = {avg_concept_accs[-1]:.4f}\"\n",
    "            )\n",
    "            \n",
    "            print(\"\\t\\tDone with trial\", trial + 1)\n",
    "\n",
    "        avg_concept_acc_mean, avg_concept_acc_std = np.mean(avg_concept_accs), np.std(avg_concept_accs)\n",
    "        experiment_variables[\"latent_avg_concept_predictive_accuracies\"].append((avg_concept_acc_mean, avg_concept_acc_std))\n",
    "        print(f\"\\tTest task accuracy: {avg_concept_acc_mean:.4f} ± {avg_concept_acc_std:.4f}\")\n",
    "\n",
    "        avg_concept_auc_mean, avg_concept_auc_std = np.mean(avg_concept_aucs), np.std(avg_concept_aucs)\n",
    "        experiment_variables[\"latent_avg_concept_predictive_aucs\"].append((avg_concept_auc_mean, avg_concept_auc_std))\n",
    "        print(f\"\\tTest task AUC: {avg_concept_auc_mean:.4f} ± {avg_concept_auc_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(CW)\n",
    "reload(leakage)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "cw_covariance_experiment_config = dict(\n",
    "    batch_size=128,\n",
    "    max_epochs=300,\n",
    "    pre_train_epochs=0,\n",
    "    post_cw_train_epochs=300,\n",
    "    cw_train_freq=20,\n",
    "    cw_train_batch_steps=20,\n",
    "    learning_rate=1e-3,\n",
    "    encoder_units=[128, 64],\n",
    "    decoder_units=[128, 64],\n",
    "    num_outputs=1,\n",
    "    \n",
    "    latent_decoder_units=[128, 64],\n",
    "    predictor_max_epochs=300,\n",
    "    \n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        \"cw/purity_new\",\n",
    "    ),\n",
    "    input_shape=[7],\n",
    "    num_concepts=3,\n",
    "    latent_dims=0,\n",
    "    covariances=np.arange(0, 1, 0.1),\n",
    "    train_samples=2000,\n",
    "    test_samples=1000,\n",
    "    trials=NUM_TRIALS,\n",
    "    verbosity=0,\n",
    "    data_concepts=3,\n",
    "    cw_layer=2,\n",
    "    aggregator='max_pool_mean',\n",
    "    activation_mode='max_pool_mean',\n",
    "    concept_auc_freq=0,\n",
    "    cw_train_iterations=1,\n",
    "    holdout_fraction=0.1,\n",
    "    exclusive_concepts=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(cw_covariance_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "cw_covariance_figure_dir = os.path.join(cw_covariance_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(cw_covariance_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "cw_covariance_results = cw_experiment_loop(\n",
    "    cw_covariance_experiment_config,\n",
    "    load_from_cache=LOAD_FROM_CACHE,\n",
    ")\n",
    "print(\"task_accuracies:\", cw_covariance_results[\"task_accuracies\"])\n",
    "print(\"concept_aucs:\", cw_covariance_results[\"task_aucs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_covariance_results.update(cw_bottleneck_predict_experiment_loop(\n",
    "    cw_covariance_experiment_config,\n",
    "    load_from_cache=LOAD_FROM_CACHE,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_covariance_results.update(cw_bottleneck_concept_predict_experiment_loop(\n",
    "    cw_covariance_experiment_config,\n",
    "    load_from_cache=LOAD_FROM_CACHE,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCD Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_ccd_encoder(\n",
    "    input_shape,\n",
    "    units,\n",
    "    end_activation=\"sigmoid\",\n",
    "    latent_dims=0,\n",
    "    latent_act=None,  # Original paper used \"sigmoid\" but this is troublesome in deep architectures\n",
    "):\n",
    "    encoder_inputs = tf.keras.Input(shape=input_shape)\n",
    "    encoder_compute_graph = encoder_inputs\n",
    "    for i, units in enumerate(units):\n",
    "        encoder_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(encoder_compute_graph)\n",
    "\n",
    "    # And finally map this to the number of concepts we have in our set\n",
    "    encoder_compute_graph = tf.keras.layers.Flatten()(encoder_compute_graph)\n",
    "    encoder_compute_graph = tf.keras.layers.Dense(\n",
    "        latent_dims,\n",
    "        activation=latent_act,\n",
    "        name=\"encoder_bypass_channel\",\n",
    "    )(encoder_compute_graph)\n",
    "\n",
    "    # Now time to collapse all the concepts again back into a single vector\n",
    "    encoder_model = tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        encoder_compute_graph,\n",
    "        name=\"encoder\",\n",
    "    )\n",
    "    return encoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.OCACE.topicModel as CCD\n",
    "import concepts_xai.evaluation.metrics.oracle as oracle\n",
    "import concepts_xai.evaluation.metrics.completeness as completeness\n",
    "\n",
    "############################################################################\n",
    "## Experiment loop\n",
    "############################################################################\n",
    "\n",
    "def ccd_experiment_loop(\n",
    "    experiment_config,\n",
    "    load_from_cache=False,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    experiment_variables = dict(\n",
    "        task_accuracies=[],\n",
    "        task_aucs=[],\n",
    "        reconstruction_accuracies=[],\n",
    "        reconstruction_aucs=[],\n",
    "        purity_scores=[],\n",
    "        non_oracle_purity_scores=[],\n",
    "        purity_matrices=[],\n",
    "        aligned_purity_matrices=[],\n",
    "        oracle_matrices=[],\n",
    "        completeness_scores=[],\n",
    "        direct_completeness_scores=[],\n",
    "        mean_similarities=[],\n",
    "    )\n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(experiment_config[\"covariances\"]):\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)}.'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    utils.serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    for cov in experiment_config[\"covariances\"][start_ind:]:\n",
    "        print(\"Training with covariance:\", cov)\n",
    "        task_accs = []\n",
    "        recon_accs = []\n",
    "        aucs = []\n",
    "        recon_aucs = []\n",
    "        purity_mats = []\n",
    "        aligned_purity_mats = []\n",
    "        oracle_mats = []\n",
    "        purities = []\n",
    "        non_oracle_purities = []\n",
    "        compl_scores = []\n",
    "        dir_compl_scores = []\n",
    "        mean_sims = []\n",
    "        \n",
    "        channels_axis = (\n",
    "            -1 if experiment_config.get(\"data_format\", \"channels_last\") == \"channels_last\"\n",
    "            else 1\n",
    "        )\n",
    "        if experiment_config[\"num_outputs\"] == 1:\n",
    "            acc_fn = lambda y_true, y_pred: sklearn.metrics.roc_auc_score(\n",
    "                y_true,\n",
    "                y_pred\n",
    "            )\n",
    "        else:\n",
    "            acc_fn = lambda y_true, y_pred: sklearn.metrics.roc_auc_score(\n",
    "                tf.keras.utils.to_categorical(y_true),\n",
    "                scipy.special.softmax(y_pred, axis=-1),\n",
    "                multi_class='ovo',\n",
    "            )\n",
    "        \n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} for covariance {cov:.2f}\")\n",
    "\n",
    "            (x_train, y_train, c_train) = produce_data(\n",
    "                experiment_config[\"train_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            (x_test, y_test, c_test) = produce_data(\n",
    "                experiment_config[\"test_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            y_train = np.squeeze(y_train)\n",
    "            y_test = np.squeeze(y_test)\n",
    "            \n",
    "            # Proceed to do and end-to-end model in case we want to\n",
    "            # do some task-specific pretraining\n",
    "            end_to_end_model, encoder, decoder = construct_end_to_end_model(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                learning_rate=experiment_config[\"learning_rate\"],\n",
    "                encoder=construct_ccd_encoder(\n",
    "                    input_shape=experiment_config[\"input_shape\"],\n",
    "                    units=experiment_config[\"encoder_units\"],\n",
    "                    latent_act=experiment_config.get(\"latent_act\", None),\n",
    "                    latent_dims=experiment_config[\"latent_dims\"],\n",
    "                ),\n",
    "                decoder=construct_decoder(\n",
    "                    units=experiment_config[\"decoder_units\"],\n",
    "                    num_outputs=experiment_config[\"num_outputs\"],\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            print(\"\\tModel pre-training...\")\n",
    "            \n",
    "            early_stopping_monitor = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=experiment_config.get(\n",
    "                    \"early_stop_metric\",\n",
    "                    \"val_loss\",\n",
    "                ),\n",
    "                min_delta=experiment_config[\"min_delta\"],\n",
    "                patience=experiment_config[\"patience\"],\n",
    "                restore_best_weights=True,\n",
    "                verbose=2,\n",
    "                mode=experiment_config.get(\n",
    "                    \"early_stop_mode\",\n",
    "                    \"min\",\n",
    "                ),\n",
    "            )\n",
    "            end_to_end_model.fit(\n",
    "                x=x_train,\n",
    "                y=y_train,\n",
    "                epochs=experiment_config[\"max_epochs\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                callbacks=[\n",
    "                    early_stopping_monitor,\n",
    "                ],\n",
    "                validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                verbose=verbosity,\n",
    "            )\n",
    "            print(\"\\t\\tModel pre-training completed\")\n",
    "            print(\"\\tSerializing model\")\n",
    "            encoder.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/encoder_cov_{cov:.2f}__num_concepts_{experiment_config['num_concepts']}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            decoder.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/decoder_cov_{cov:.2f}__num_concepts_{experiment_config['num_concepts']}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            print(\"\\tEvaluating model\")\n",
    "            \n",
    "            test_result = end_to_end_model.evaluate(\n",
    "                x_test,\n",
    "                y_test,\n",
    "                verbose=0,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            if experiment_config[\"num_outputs\"] > 1:\n",
    "                task_accs.append(test_result['sparse_top_k_categorical_accuracy'])\n",
    "                \n",
    "                # Then lets apply a softmax activation over all the probability\n",
    "                # classes\n",
    "                preds = scipy.special.softmax(end_to_end_model.predict(x_test), axis=-1)\n",
    "\n",
    "                # And select just the labels that are in fact being used\n",
    "                one_hot_labels = tf.keras.utils.to_categorical(y_test)\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    one_hot_labels,\n",
    "                    preds,\n",
    "                    multi_class='ovo',\n",
    "                ))\n",
    "            else:\n",
    "                task_accs.append(test_result['binary_accuracy'])\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    y_test,\n",
    "                    end_to_end_model.predict(x_test),\n",
    "                ))\n",
    "            \n",
    "            # Now extract our concept vectors\n",
    "            if \"top_k\" not in experiment_config:\n",
    "                top_k = ccd_compute_k(y=y_train, batch_size=experiment_config[\"batch_size\"])\n",
    "            else:\n",
    "                top_k = experiment_config[\"top_k\"]\n",
    "            topic_model = CCD.TopicModel(\n",
    "                concepts_to_labels_model=decoder,\n",
    "                n_channels=experiment_config[\"latent_dims\"],\n",
    "                n_concepts=experiment_config[\"num_concepts\"],\n",
    "                threshold=experiment_config.get(\"threshold\", 0.5),\n",
    "                loss_fn=end_to_end_model.loss,\n",
    "                top_k=top_k,\n",
    "                lambda1=experiment_config.get(\"lambda1\", 0.1),\n",
    "                lambda2=experiment_config.get(\"lambda2\", 0.1),\n",
    "                seed=experiment_config.get(\"seed\", None),\n",
    "                eps=experiment_config.get(\"eps\", 1e-5),\n",
    "                data_format=experiment_config.get(\n",
    "                    \"data_format\",\n",
    "                    \"channels_last\"\n",
    "                ),\n",
    "                allow_gradient_flow_to_c2l=experiment_config.get(\n",
    "                    'allow_gradient_flow_to_c2l',\n",
    "                    False,\n",
    "                ),\n",
    "                acc_metric=(\n",
    "                    tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "                    if experiment_config[\"num_outputs\"] > 1 else\n",
    "                    tf.keras.metrics.BinaryAccuracy()\n",
    "                ),\n",
    "            )\n",
    "            topic_model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(\n",
    "                    experiment_config.get(\"learning_rate\", 1e-3),\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Train it for a few epochs\n",
    "            print(\"\\tTopic model training...\")\n",
    "            topic_model.fit(\n",
    "                x=encoder(x_train),\n",
    "                y=y_train,\n",
    "                epochs=experiment_config[\"topic_model_train_epochs\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                verbose=verbosity,\n",
    "            )\n",
    "            print(\"\\t\\tTopic model training completed\")\n",
    "            \n",
    "            print(\"\\tSerializing model\")\n",
    "            topic_model.g_model.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/topic_g_model_cov_{cov:.2f}_num_concepts_{experiment_config['num_concepts']}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            np.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/topic_vector_cov_{cov:.2f}_num_concepts_{experiment_config['num_concepts']}_trial_{trial}.npy\"\n",
    "                ),\n",
    "                topic_model.topic_vector.numpy(),\n",
    "            )\n",
    "            print(\"\\tEvaluating model\")\n",
    "            \n",
    "            topic_result = topic_model.evaluate(\n",
    "                encoder(x_test),\n",
    "                y_test,\n",
    "                verbose=0,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            \n",
    "            if experiment_config[\"num_outputs\"] > 1:\n",
    "                recon_accs.append(topic_result['accuracy'])\n",
    "                # Then lets apply a softmax activation over all the probability\n",
    "                # classes\n",
    "                preds = scipy.special.softmax(\n",
    "                    topic_model(encoder(x_test))[0],\n",
    "                    axis=-1,\n",
    "                )\n",
    "\n",
    "                # And select just the labels that are in fact being used\n",
    "                one_hot_labels = tf.keras.utils.to_categorical(y_test)\n",
    "                recon_aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    one_hot_labels,\n",
    "                    preds,\n",
    "                    multi_class='ovo',\n",
    "                ))\n",
    "            else:\n",
    "                recon_accs.append(topic_result['accuracy'])\n",
    "                recon_aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    y_test,\n",
    "                    topic_model(encoder(x_test))[0],\n",
    "                ))\n",
    "            mean_sims.append(topic_result['mean_sim'])\n",
    "            \n",
    "            print(\n",
    "                f\"\\t\\tTest auc = {aucs[-1]:.4f}, \"\n",
    "                f\"task accuracy = {task_accs[-1]:.4f}, \"\n",
    "                f\"task reconstruction accuracy = {recon_accs[-1]:.4f}, \"\n",
    "                f\"task reconstruction auc = {recon_aucs[-1]:.4f}, \"\n",
    "                f\"mean concept similarity = {mean_sims[-1]:.4f}\"\n",
    "            )\n",
    "            \n",
    "                        \n",
    "            # We start by extracting a completeness score for the extracted\n",
    "            # concept vectors\n",
    "            print(f\"\\t\\tComputing completeness scores...\")\n",
    "            compl_score, _ = completeness.completeness_score(\n",
    "                X=x_test,\n",
    "                y=y_test,\n",
    "                features_to_concepts_fn=encoder,\n",
    "                concepts_to_labels_model=decoder,\n",
    "                concept_vectors=np.transpose(topic_model.topic_vector.numpy()),\n",
    "                task_loss=end_to_end_model.loss,\n",
    "                channels_axis=channels_axis,\n",
    "                concept_score_fn=lambda f, c: completeness.dot_prod_concept_score(\n",
    "                    features=f,\n",
    "                    concept_vectors=c,\n",
    "                    channels_axis=channels_axis,\n",
    "                    beta=experiment_config.get(\"threshold\", 0.5),\n",
    "                ),\n",
    "                acc_fn=acc_fn,\n",
    "            )\n",
    "            compl_scores.append(compl_score)\n",
    "            \n",
    "            dir_compl_score, _ = completeness.direct_completeness_score(\n",
    "                X=x_test,\n",
    "                y=y_test,\n",
    "                features_to_concepts_fn=encoder,\n",
    "                concept_vectors=np.transpose(topic_model.topic_vector.numpy()),\n",
    "                task_loss=end_to_end_model.loss,\n",
    "                channels_axis=channels_axis,\n",
    "                concept_score_fn=lambda f, c: completeness.dot_prod_concept_score(\n",
    "                    features=f,\n",
    "                    concept_vectors=c,\n",
    "                    channels_axis=channels_axis,\n",
    "                    beta=experiment_config.get(\"threshold\", 0.5),\n",
    "                ),\n",
    "                acc_fn=acc_fn,\n",
    "            )\n",
    "            dir_compl_scores.append(dir_compl_score)\n",
    "            \n",
    "            print(\n",
    "                f\"\\t\\t\\tCompleteness Score: {compl_scores[-1]:.4f} \"\n",
    "                f\"and Direct Completeness Score: {dir_compl_scores[-1]:.4f}\"\n",
    "            )\n",
    "            \n",
    "            print(f\"\\t\\tComputing purity score...\")\n",
    "            concept_scores = topic_model.concept_scores(encoder(x_test)).numpy()\n",
    "            purity_score, (purity_mat, aligned_purity_mat), oracle_mat = oracle.oracle_impurity_score(\n",
    "                c_soft=concept_scores,\n",
    "                c_true=c_test,\n",
    "                output_matrices=True,\n",
    "                alignment_function=oracle.max_alignment_matrix,\n",
    "            )\n",
    "            \n",
    "            purity_mats.append(purity_mat)\n",
    "            aligned_purity_mats.append(aligned_purity_mat)\n",
    "            oracle_mats.append(oracle_mat)\n",
    "            purities.append(purity_score)\n",
    "            print(f\"\\t\\t\\tDone {purity_score:.4f}\")\n",
    "\n",
    "            print(\"\\t\\tComputing non-oracle purity score...\")\n",
    "            non_oracle_purities.append(oracle.oracle_impurity_score(\n",
    "                c_soft=concept_scores,\n",
    "                c_true=c_test,\n",
    "                alignment_function=oracle.max_alignment_matrix,\n",
    "                oracle_matrix=construct_trivial_auc_mat(\n",
    "                    c_test.shape[-1]\n",
    "                ),\n",
    "                purity_matrix=aligned_purity_mat,\n",
    "            ))\n",
    "            print(f\"\\t\\t\\tDone {non_oracle_purities[-1]:.4f}\")\n",
    "            print(\"\\t\\tDone with trial\", trial + 1)\n",
    "\n",
    "        task_acc_mean, task_acc_std = np.mean(task_accs), np.std(task_accs)\n",
    "        experiment_variables[\"task_accuracies\"].append((task_acc_mean, task_acc_std))\n",
    "        print(f\"\\tTest task accuracy: {task_acc_mean:.4f} ± {task_acc_std:.4f}\")\n",
    "\n",
    "        recon_acc_mean, recon_acc_std = np.mean(recon_accs), np.std(recon_accs)\n",
    "        experiment_variables[\"reconstruction_accuracies\"].append((recon_acc_mean, recon_acc_std))\n",
    "        print(f\"\\tTest reconstruction accuracy: {recon_acc_mean:.4f} ± {recon_acc_std:.4f}\")\n",
    "\n",
    "        task_auc_mean, task_auc_std = np.mean(aucs), np.std(aucs)\n",
    "        experiment_variables[\"task_aucs\"].append((task_auc_mean, task_auc_std))\n",
    "        print(f\"\\tTest task AUC: {task_auc_mean:.4f} ± {task_auc_std:.4f}\")\n",
    "        \n",
    "        recon_auc_mean, recon_auc_std = np.mean(recon_aucs), np.std(recon_aucs)\n",
    "        experiment_variables[\"reconstruction_aucs\"].append((recon_auc_mean, recon_auc_std))\n",
    "        print(f\"\\tTest reconstruction accuracy: {recon_auc_mean:.4f} ± {recon_auc_std:.4f}\")\n",
    "        \n",
    "        mean_sim_mean, mean_sim_std = np.mean(mean_sims), np.std(mean_sims)\n",
    "        experiment_variables[\"mean_similarities\"].append((mean_sim_mean, mean_sim_std))\n",
    "        print(f\"\\tMean concept similarity: {mean_sim_mean:.4f} ± {mean_sim_std:.4f}\")\n",
    "        \n",
    "        \n",
    "        compl_score_mean, compl_score_std = np.mean(compl_scores), np.std(compl_scores)\n",
    "        experiment_variables[\"completeness_scores\"].append((compl_score_mean, compl_score_std))\n",
    "        print(f\"\\tCompleteness Score: {compl_score_mean:.4f} ± {compl_score_std:.4f}\")\n",
    "        \n",
    "        dir_compl_score_mean, dir_compl_score_std = np.mean(dir_compl_scores), np.std(dir_compl_scores)\n",
    "        experiment_variables[\"direct_completeness_scores\"].append((dir_compl_score_mean, dir_compl_score_std))\n",
    "        print(f\"\\tDirect completeness Score: {dir_compl_score_mean:.4f} ± {dir_compl_score_std:.4f}\")\n",
    "\n",
    "        purity_mats = np.stack(purity_mats, axis=0)\n",
    "        purity_mat_mean = np.mean(purity_mats, axis=0)\n",
    "        purity_mat_std = np.std(purity_mats, axis=0)\n",
    "        print(\"\\tPurity matrix:\")\n",
    "        for i in range(purity_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(purity_mat_mean.shape[1]):\n",
    "                line += f'{purity_mat_mean[i, j]:.4f} ± {purity_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"purity_matrices\"].append((purity_mat_mean, purity_mat_std))\n",
    "        \n",
    "        aligned_purity_mats = np.stack(aligned_purity_mats, axis=0)\n",
    "        aligned_purity_mat_mean = np.mean(aligned_purity_mats, axis=0)\n",
    "        aligned_purity_mat_std = np.std(aligned_purity_mats, axis=0)\n",
    "        print(\"\\tAligned purity matrix:\")\n",
    "        for i in range(aligned_purity_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(aligned_purity_mat_mean.shape[1]):\n",
    "                line += f'{aligned_purity_mat_mean[i, j]:.4f} ± {aligned_purity_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"aligned_purity_matrices\"].append((aligned_purity_mat_mean, aligned_purity_mat_std))\n",
    "\n",
    "\n",
    "        oracle_mats = np.stack(oracle_mats, axis=0)\n",
    "        oracle_mat_mean = np.mean(oracle_mats, axis=0)\n",
    "        oracle_mat_std = np.std(oracle_mats, axis=0)\n",
    "        print(\"\\tOracle matrix:\")\n",
    "        for i in range(oracle_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(oracle_mat_mean.shape[1]):\n",
    "                line += f'{oracle_mat_mean[i, j]:.4f} ± {oracle_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"oracle_matrices\"].append((oracle_mat_mean, oracle_mat_std))\n",
    "\n",
    "        purity_mean, purity_std = np.mean(purities), np.std(purities)\n",
    "        experiment_variables[\"purity_scores\"].append((purity_mean, purity_std))\n",
    "        print(f\"\\tPurity score: {purity_mean:.4f} ± {purity_std:.4f}\")\n",
    "\n",
    "        non_oracle_purity_mean, non_oracle_purity_std = np.mean(non_oracle_purities), np.std(non_oracle_purities)\n",
    "        experiment_variables[\"non_oracle_purity_scores\"].append((non_oracle_purity_mean, non_oracle_purity_std))\n",
    "        print(f\"\\tNon-oracle purity score: {non_oracle_purity_mean:.4f} ± {non_oracle_purity_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables\n",
    "\n",
    "def ccd_bottleneck_concept_predict_experiment_loop(\n",
    "    experiment_config,\n",
    "    load_from_cache=False,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    experiment_variables = dict(\n",
    "        latent_avg_concept_predictive_accuracies=[],\n",
    "        latent_avg_concept_predictive_aucs=[],\n",
    "    )\n",
    "\n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                print(\"Could not find\", f'\"{file_name}_means.npz\" or \"{file_name}_stds.npz\" in cache.')\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(experiment_config[\"covariances\"]):\n",
    "                print(\"Found\", len(loaded_means), \"means for variable\", var_name, \"vs\", len(experiment_config[\"covariances\"]), \"datasets\")\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)} ({file_name}).'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            print(experiment_variables)\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    utils.serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    for cov in experiment_config[\"covariances\"][start_ind:]:\n",
    "        print(\"Training with covariance:\", cov)\n",
    "        avg_concept_accs = []\n",
    "        avg_concept_aucs = []\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} with covariance {cov}\")\n",
    "            # First construct the dataset\n",
    "            (x_train, y_train, c_train) = produce_data(\n",
    "                experiment_config[\"train_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            (x_test, y_test, c_test) = produce_data(\n",
    "                experiment_config[\"test_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            encoder = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/encoder_cov_{cov:.2f}__num_concepts_{experiment_config['num_concepts']}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            decoder = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/decoder_cov_{cov:.2f}__num_concepts_{experiment_config['num_concepts']}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            \n",
    "            g_model = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/topic_g_model_cov_{cov:.2f}_num_concepts_{experiment_config['num_concepts']}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            topic_vector = np.load(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/topic_vector_cov_{cov:.2f}_num_concepts_{experiment_config['num_concepts']}_trial_{trial}.npy\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            \n",
    "            # Now extract our concept vectors\n",
    "            topic_model = CCD.TopicModel(\n",
    "                concepts_to_labels_model=decoder,\n",
    "                n_channels=experiment_config[\"latent_dims\"],\n",
    "                n_concepts=experiment_config['num_concepts'],\n",
    "                threshold=experiment_config.get(\"threshold\", 0.5),\n",
    "                loss_fn=(\n",
    "                    tf.keras.losses.BinaryCrossentropy(from_logits=True) if (experiment_config[\"num_outputs\"] <= 2)\n",
    "                    else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "                ),\n",
    "                top_k=experiment_config.get(\"top_k\", 32),\n",
    "                lambda1=experiment_config.get(\"lambda1\", 0.1),\n",
    "                lambda2=experiment_config.get(\"lambda2\", 0.1),\n",
    "                seed=experiment_config.get(\"seed\", None),\n",
    "                eps=experiment_config.get(\"eps\", 1e-5),\n",
    "                data_format=experiment_config.get(\n",
    "                    \"data_format\",\n",
    "                    \"channels_last\"\n",
    "                ),\n",
    "                allow_gradient_flow_to_c2l=experiment_config.get(\n",
    "                    'allow_gradient_flow_to_c2l',\n",
    "                    False,\n",
    "                ),\n",
    "                acc_metric=(\n",
    "                    tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "                    if experiment_config[\"num_outputs\"] > 1 else\n",
    "                    tf.keras.metrics.BinaryAccuracy()\n",
    "                ),\n",
    "                initial_topic_vector=topic_vector,\n",
    "            )\n",
    "            \n",
    "            \n",
    "            concept_scores = topic_model.concept_scores(encoder(x_test)).numpy()\n",
    "            \n",
    "            current_aucs = []\n",
    "            current_accs = []\n",
    "            score_train_codes = topic_model.concept_scores(encoder(x_train)).numpy()\n",
    "            score_test_codes = topic_model.concept_scores(encoder(x_test)).numpy()\n",
    "            for concept_idx in range(experiment_config[\"data_concepts\"]):\n",
    "            \n",
    "                score_predictive_decoder = construct_decoder(\n",
    "                    units=experiment_config[\"latent_decoder_units\"],\n",
    "                    num_outputs=1,\n",
    "                )\n",
    "\n",
    "                score_predictive_decoder.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(experiment_config[\"learning_rate\"]),\n",
    "                    loss=(\n",
    "                        tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "                    ),\n",
    "                    metrics=[\n",
    "                        \"binary_accuracy\"\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "                print(\"\\t\\tTraining score model for concept\", concept_idx)\n",
    "                score_predictive_decoder.fit(\n",
    "                    x=score_train_codes,\n",
    "                    y=c_train[:, concept_idx],\n",
    "                    epochs=experiment_config[\"predictor_max_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "\n",
    "\n",
    "                print(\"\\t\\tEvaluating score model\")\n",
    "                test_result = score_predictive_decoder.evaluate(\n",
    "                    score_test_codes,\n",
    "                    c_test[:, concept_idx],\n",
    "                    verbose=0,\n",
    "                    return_dict=True,\n",
    "                )\n",
    "                current_accs.append(\n",
    "                    test_result['binary_accuracy']\n",
    "                )\n",
    "\n",
    "                current_aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    c_test[:, concept_idx],\n",
    "                    score_predictive_decoder.predict(score_test_codes),\n",
    "                ))\n",
    "                print(\n",
    "                    f\"\\t\\t\\tTest concept AUC = {current_aucs[-1]:.4f}, \"\n",
    "                    f\"concept accuracy = {current_accs[-1]:.4f}\"\n",
    "                )\n",
    "            avg_concept_aucs.append(np.mean(current_aucs))\n",
    "            avg_concept_accs.append(np.mean(current_accs))\n",
    "            print(\n",
    "                f\"\\t\\tTest avg concept AUC = {avg_concept_aucs[-1]:.4f}, \"\n",
    "                f\"avg concept accuracy = {avg_concept_accs[-1]:.4f}\"\n",
    "            )\n",
    "            \n",
    "            print(\"\\t\\tDone with trial\", trial + 1)\n",
    "\n",
    "        avg_concept_acc_mean, avg_concept_acc_std = np.mean(avg_concept_accs), np.std(avg_concept_accs)\n",
    "        experiment_variables[\"latent_avg_concept_predictive_accuracies\"].append((avg_concept_acc_mean, avg_concept_acc_std))\n",
    "        print(f\"\\tTest task accuracy: {avg_concept_acc_mean:.4f} ± {avg_concept_acc_std:.4f}\")\n",
    "\n",
    "        avg_concept_auc_mean, avg_concept_auc_std = np.mean(avg_concept_aucs), np.std(avg_concept_aucs)\n",
    "        experiment_variables[\"latent_avg_concept_predictive_aucs\"].append((avg_concept_auc_mean, avg_concept_auc_std))\n",
    "        print(f\"\\tTest task AUC: {avg_concept_auc_mean:.4f} ± {avg_concept_auc_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables\n",
    "\n",
    "def ccd_compute_k(y, batch_size):\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    avg_class_ratio = np.mean(counts) / y.shape[0]\n",
    "    return int((avg_class_ratio * batch_size) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCD Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(completeness)\n",
    "reload(CBM)\n",
    "reload(CCD)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "ccd_covariance_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=300,\n",
    "    topic_model_train_epochs=50,\n",
    "    trials=NUM_TRIALS,\n",
    "    learning_rate=1e-3,\n",
    "    \n",
    "    num_concepts=3,\n",
    "    input_shape=[7],\n",
    "    latent_dims=10,\n",
    "    encoder_units=[128, 64],\n",
    "    decoder_units=[128, 64],\n",
    "    latent_act=None,\n",
    "    \n",
    "    threshold=0.0,\n",
    "    lambda1=0.1,\n",
    "    lambda2=0.1,\n",
    "    eps=1e-5,\n",
    "    \n",
    "    latent_decoder_units=[128, 64],\n",
    "    predictor_max_epochs=300,\n",
    "    \n",
    "    num_outputs=1,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        \"ccd/purity\",\n",
    "    ),\n",
    "    covariances=np.arange(0, 1, 0.1),\n",
    "    train_samples=2000,\n",
    "    test_samples=1000,\n",
    "    verbosity=0,\n",
    "    data_concepts=3,\n",
    "    cw_layer=2,\n",
    "    aggregator='max_pool_mean',\n",
    "    activation_mode='max_pool_mean',\n",
    "    cw_train_freq=20,\n",
    "    concept_auc_freq=0,\n",
    "    cw_train_iterations=1,\n",
    "    holdout_fraction=0.1,\n",
    "    exclusive_concepts=True,\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(ccd_covariance_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "ccd_covariance_figure_dir = os.path.join(ccd_covariance_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(ccd_covariance_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "ccd_covariance_results = ccd_experiment_loop(\n",
    "    experiment_config=ccd_covariance_experiment_config,\n",
    "    load_from_cache=LOAD_FROM_CACHE,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"task_accuracies:\", ccd_covariance_results[\"task_accuracies\"])\n",
    "print(\"reconstruction_accuracies:\", ccd_covariance_results[\"reconstruction_accuracies\"])\n",
    "print(\"task_aucs:\", ccd_covariance_results[\"task_aucs\"])\n",
    "print(\"reconstruction_aucs:\", ccd_covariance_results[\"reconstruction_aucs\"])\n",
    "print(\"purity_scores:\", ccd_covariance_results[\"purity_scores\"])\n",
    "print(\"non_oracle_purity_scores:\", ccd_covariance_results[\"non_oracle_purity_scores\"])\n",
    "print(\"completeness_scores:\", ccd_covariance_results[\"completeness_scores\"])\n",
    "print(\"direct_completeness_scores:\", ccd_covariance_results[\"direct_completeness_scores\"])\n",
    "print(\"mean_similarities:\", ccd_covariance_results[\"mean_similarities\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccd_covariance_results.update(ccd_bottleneck_concept_predict_experiment_loop(\n",
    "    experiment_config=ccd_covariance_experiment_config,\n",
    "    load_from_cache=LOAD_FROM_CACHE,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(completeness)\n",
    "reload(CBM)\n",
    "reload(CCD)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "ccd_covariance_double_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=300,\n",
    "    topic_model_train_epochs=50,\n",
    "    trials=NUM_TRIALS,\n",
    "    learning_rate=1e-3,\n",
    "    \n",
    "    num_concepts=6,  # Let's extract twice as many concepts in here\n",
    "    input_shape=[7],\n",
    "    latent_dims=10,\n",
    "    encoder_units=[128, 64],\n",
    "    decoder_units=[128, 64],\n",
    "    latent_act=None,\n",
    "    \n",
    "    threshold=0.0,\n",
    "    lambda1=0.1,\n",
    "    lambda2=0.1,\n",
    "    eps=1e-5,\n",
    "    \n",
    "    num_outputs=1,\n",
    "    \n",
    "    latent_decoder_units=[128, 64],\n",
    "    predictor_max_epochs=300,\n",
    "    \n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        \"ccd/purity_double_concepts\",\n",
    "    ),\n",
    "    covariances=np.arange(0, 1, 0.1),\n",
    "    train_samples=2000,\n",
    "    test_samples=1000,\n",
    "    verbosity=0,\n",
    "    data_concepts=3,\n",
    "    cw_layer=2,\n",
    "    aggregator='max_pool_mean',\n",
    "    activation_mode='max_pool_mean',\n",
    "    cw_train_freq=20,\n",
    "    concept_auc_freq=0,\n",
    "    cw_train_iterations=1,\n",
    "    holdout_fraction=0.1,\n",
    "    exclusive_concepts=True,\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(ccd_covariance_double_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "ccd_covariance_double_figure_dir = os.path.join(ccd_covariance_double_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(ccd_covariance_double_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "ccd_covariance_double_results = ccd_experiment_loop(\n",
    "    experiment_config=ccd_covariance_double_experiment_config,\n",
    "    load_from_cache=LOAD_FROM_CACHE,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"task_accuracies:\", ccd_covariance_double_results[\"task_accuracies\"])\n",
    "print(\"reconstruction_accuracies:\", ccd_covariance_double_results[\"reconstruction_accuracies\"])\n",
    "print(\"task_aucs:\", ccd_covariance_double_results[\"task_aucs\"])\n",
    "print(\"reconstruction_aucs:\", ccd_covariance_double_results[\"reconstruction_aucs\"])\n",
    "print(\"purity_scores:\", ccd_covariance_double_results[\"purity_scores\"])\n",
    "print(\"non_oracle_purity_scores:\", ccd_covariance_double_results[\"non_oracle_purity_scores\"])\n",
    "print(\"completeness_scores:\", ccd_covariance_double_results[\"completeness_scores\"])\n",
    "print(\"direct_completeness_scores:\", ccd_covariance_double_results[\"direct_completeness_scores\"])\n",
    "print(\"mean_similarities:\", ccd_covariance_double_results[\"mean_similarities\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccd_covariance_double_results.update(ccd_bottleneck_concept_predict_experiment_loop(\n",
    "    experiment_config=ccd_covariance_double_experiment_config,\n",
    "    load_from_cache=LOAD_FROM_CACHE,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENN Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.SENN.base_senn as SENN\n",
    "import concepts_xai.methods.SENN.aggregators as aggregators\n",
    "reload(SENN)\n",
    "reload(aggregators)\n",
    "\n",
    "\n",
    "def construct_senn_coefficient_model(units, num_concepts, num_outputs):\n",
    "    decoder_layers = [tf.keras.layers.Flatten()] + [\n",
    "        tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation=tf.nn.relu,\n",
    "            name=f\"coefficient_model_dense_{i+1}\",\n",
    "        ) for i, units in enumerate(units)\n",
    "    ]\n",
    "    return tf.keras.Sequential(decoder_layers + [\n",
    "        tf.keras.layers.Dense(\n",
    "            num_concepts * num_outputs,\n",
    "            activation=None,\n",
    "            name=\"coefficient_model_output\",\n",
    "        ),\n",
    "        tf.keras.layers.Reshape([num_outputs, num_concepts])\n",
    "    ])\n",
    "\n",
    "def construct_senn_encoder(\n",
    "    input_shape,\n",
    "    units,\n",
    "    end_activation=\"sigmoid\",\n",
    "    latent_dims=0,\n",
    "    latent_act=None,  # Original paper used \"sigmoid\" but this is troublesome in deep architectures\n",
    "):\n",
    "    encoder_inputs = tf.keras.Input(shape=input_shape)\n",
    "    encoder_compute_graph = encoder_inputs\n",
    "    for i, units in enumerate(units):\n",
    "        encoder_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(encoder_compute_graph)\n",
    "\n",
    "    mean = tf.keras.layers.Dense(latent_dims, activation=None, name=\"means\")(encoder_compute_graph)\n",
    "    log_var = tf.keras.layers.Dense(latent_dims, activation=None, name=\"log_var\")(encoder_compute_graph)\n",
    "    senn_encoder = tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        mean,\n",
    "        name=\"senn_encoder\",\n",
    "    )\n",
    "    vae_encoder = tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        [mean, log_var],\n",
    "        name=\"vae_encoder\",\n",
    "    )\n",
    "    return senn_encoder, vae_encoder\n",
    "\n",
    "\n",
    "def construct_vae_decoder(\n",
    "    units,\n",
    "    output_shape,\n",
    "    latent_dims,\n",
    "):\n",
    "    \"\"\"CNN decoder architecture used in the 'Challenging Common Assumptions in the Unsupervised Learning\n",
    "       of Disentangled Representations' paper (https://arxiv.org/abs/1811.12359)\n",
    "\n",
    "       Note: model is uncompiled\n",
    "    \"\"\"\n",
    "\n",
    "    latent_inputs = tf.keras.Input(shape=(latent_dims,))\n",
    "    model_out = latent_inputs\n",
    "    for unit in units:\n",
    "        model_out = tf.keras.layers.Dense(\n",
    "            unit,\n",
    "            activation='relu',\n",
    "        )(model_out)\n",
    "    model_out = tf.keras.layers.Dense(\n",
    "        output_shape,\n",
    "        activation=None,\n",
    "    )(model_out)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=latent_inputs,\n",
    "        outputs=[model_out],\n",
    "    )\n",
    "\n",
    "\n",
    "def construct_senn_model(\n",
    "    concept_encoder,\n",
    "    concept_decoder,\n",
    "    coefficient_model,\n",
    "    num_outputs,\n",
    "    regularization_strength=0.1,\n",
    "    learning_rate=1e-3,\n",
    "    sparsity_strength=2e-5,\n",
    "):\n",
    "    def reconstruction_loss_fn(y_true, y_pred):\n",
    "#         return vae_losses.bernoulli_fn_wrapper()(y_true, concept_decoder(y_pred))\n",
    "        return tf.reduce_sum(\n",
    "            tf.square(y_true - concept_decoder(y_pred)),\n",
    "            [-1]\n",
    "        )\n",
    "    senn_model = SENN.SelfExplainingNN(\n",
    "        encoder_model=concept_encoder,\n",
    "        coefficient_model=coefficient_model,\n",
    "        aggregator_fn=(\n",
    "            aggregators.multiclass_additive_aggregator if (num_outputs >= 2)\n",
    "            else aggregators.scalar_additive_aggregator\n",
    "        ),\n",
    "        task_loss_fn=(\n",
    "            tf.keras.losses.BinaryCrossentropy(from_logits=True) if (num_outputs < 2)\n",
    "            else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        ),\n",
    "        reconstruction_loss_fn=reconstruction_loss_fn,\n",
    "        regularization_strength=regularization_strength,\n",
    "        sparsity_strength=sparsity_strength,\n",
    "        name=\"SENN\",\n",
    "        metrics=[\n",
    "            tf.keras.metrics.BinaryAccuracy() if (num_outputs < 2)\n",
    "            else tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "        ],\n",
    "    )\n",
    "    senn_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    )\n",
    "    return senn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.evaluation.metrics.oracle as oracle\n",
    "\n",
    "############################################################################\n",
    "## Experiment loop\n",
    "############################################################################\n",
    "\n",
    "def construct_trivial_auc_mat(num_concepts):\n",
    "    result = np.ones((num_concepts, num_concepts), dtype=np.float32) * 0.5\n",
    "    return result + np.eye(num_concepts, dtype=np.float32) * 0.5\n",
    "\n",
    "def get_argmax_concept_explanations(preds, class_theta_scores):\n",
    "    inds = np.argmax(preds, axis=-1)\n",
    "    result = np.take_along_axis(\n",
    "        class_theta_scores,\n",
    "        np.expand_dims(np.expand_dims(inds, axis=-1), axis=-1),\n",
    "        axis=1,\n",
    "    )\n",
    "    return np.squeeze(result, axis=1)\n",
    "\n",
    "def senn_experiment_loop(\n",
    "    experiment_config,\n",
    "    load_from_cache=False,\n",
    "    oracle_matrix_cache=None,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    oracle_matrix_cache = oracle_matrix_cache or {}\n",
    "    experiment_variables = dict(\n",
    "        task_accuracies=[],\n",
    "        task_aucs=[],\n",
    "        purity_scores=[],\n",
    "        non_oracle_purity_scores=[],\n",
    "        purity_matrices=[],\n",
    "        oracle_matrices=[],\n",
    "    )\n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(experiment_config[\"covariances\"]):\n",
    "                print(\"Found\", len(loaded_means), \"means for variable\", var_name, \"vs\", len(experiment_config[\"covariances\"]), \"datasets\")\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)}.'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    utils.serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "\n",
    "\n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    for cov in experiment_config[\"covariances\"][start_ind:]:\n",
    "        print(\"Training with covariance:\", cov)\n",
    "        task_accs = []\n",
    "        aucs = []\n",
    "        purity_mats = []\n",
    "        oracle_mats = []\n",
    "        purities = []\n",
    "        non_oracle_purities = []\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} for covariance {cov:.2f}\")\n",
    "            (x_train, y_train, c_train) = produce_data(\n",
    "                experiment_config[\"train_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            (x_test, y_test, c_test) = produce_data(\n",
    "                experiment_config[\"test_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            x_train = np.squeeze(x_train)\n",
    "            x_test = np.squeeze(x_test)\n",
    "            y_train = np.squeeze(y_train)\n",
    "            y_test = np.squeeze(y_test)\n",
    "\n",
    "            # Proceed to do and end-to-end model in case we want to\n",
    "            # do some task-specific pretraining\n",
    "            concept_encoder, vae_encoder = construct_senn_encoder(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                units=experiment_config[\"encoder_units\"],\n",
    "                latent_act=experiment_config.get(\"latent_act\", None),\n",
    "                latent_dims=experiment_config[\"latent_dims\"],\n",
    "            )\n",
    "            concept_decoder = construct_vae_decoder(\n",
    "                units=experiment_config[\"decoder_units\"],\n",
    "                output_shape=experiment_config[\"input_shape\"][-1],\n",
    "                latent_dims=experiment_config[\"latent_dims\"],\n",
    "            )\n",
    "            coefficient_model = construct_senn_coefficient_model(\n",
    "                units=experiment_config[\"coefficient_model_units\"],\n",
    "                num_concepts=experiment_config[\"latent_dims\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "            )\n",
    "            \n",
    "            if experiment_config.get(\"pretrain_autoencoder_epochs\"):\n",
    "                autoencoder = beta_vae.BetaVAE(\n",
    "                    encoder=vae_encoder,\n",
    "                    decoder=concept_decoder,\n",
    "                    loss_fn=vae_losses.bernoulli_fn_wrapper(),\n",
    "                    beta=experiment_config.get(\"beta\", 1),\n",
    "                )\n",
    "\n",
    "                autoencoder.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(\n",
    "                        experiment_config.get(\"learning_rate\", 1e-3)\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "                print(\"\\tAutoencoder pre-training...\")\n",
    "                autoencoder.fit(\n",
    "                    x=x_train,\n",
    "                    epochs=experiment_config[\"pretrain_autoencoder_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "                print(\"\\t\\tAutoencoder training completed\")\n",
    "\n",
    "            # Now time to actually construct and train the CBM\n",
    "            senn_model = construct_senn_model(\n",
    "                concept_encoder=concept_encoder,\n",
    "                concept_decoder=concept_decoder,\n",
    "                coefficient_model=coefficient_model,\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "                regularization_strength=experiment_config.get(\"regularization_strength\", 0.1),\n",
    "                learning_rate=experiment_config.get(\"learning_rate\", 1e-3),\n",
    "                sparsity_strength=experiment_config.get(\"sparsity_strength\", 2e-5),\n",
    "            )\n",
    "\n",
    "            early_stopping_monitor = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=experiment_config.get(\n",
    "                    \"early_stop_metric\",\n",
    "                    \"val_loss\",\n",
    "                ),\n",
    "                min_delta=experiment_config[\"min_delta\"],\n",
    "                patience=experiment_config[\"patience\"],\n",
    "                restore_best_weights=True,\n",
    "                verbose=2,\n",
    "                mode=experiment_config.get(\n",
    "                    \"early_stop_mode\",\n",
    "                    \"max\",\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            print(\"\\tSENN training...\")\n",
    "            senn_model.fit(\n",
    "                x=x_train,\n",
    "                y=y_train,\n",
    "                epochs=experiment_config[\"max_epochs\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                callbacks=[\n",
    "                    early_stopping_monitor,\n",
    "                ],\n",
    "                validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                verbose=verbosity,\n",
    "            )\n",
    "            print(\"\\t\\tSENN training completed\")\n",
    "            print(\"\\tSerializing model\")\n",
    "            concept_encoder.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/concept_encoder_{cov:.2f}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            concept_decoder.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/concept_decoder_{cov:.2f}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            coefficient_model.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/coefficient_model_{cov:.2f}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            print(\"\\tEvaluating model\")\n",
    "            test_result = senn_model.evaluate(\n",
    "                x_test,\n",
    "                y_test,\n",
    "                verbose=0,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            task_accs.append(\n",
    "                test_result['sparse_top_k_categorical_accuracy']\n",
    "                if experiment_config['num_outputs'] > 1 else\n",
    "                test_result['binary_accuracy']\n",
    "            )\n",
    "\n",
    "            if experiment_config['num_outputs'] > 1:\n",
    "                # Then lets apply a softmax activation over all the probability\n",
    "                # classes\n",
    "                preds = scipy.special.softmax(\n",
    "                    senn_model.predict(x_test)[0],\n",
    "                    axis=-1\n",
    "                )\n",
    "\n",
    "                # And select just the labels that are in fact being used\n",
    "                one_hot_labels = tf.keras.utils.to_categorical(y_test)\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    one_hot_labels,\n",
    "                    preds,\n",
    "                    multi_class='ovo',\n",
    "                ))\n",
    "            else:\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    y_test,\n",
    "                    senn_model.predict(x_test)[0],\n",
    "                ))\n",
    "\n",
    "            print(\n",
    "                f\"\\t\\tTest auc = {aucs[-1]:.4f}, \"\n",
    "                f\"task accuracy = {task_accs[-1]:.4f}\"\n",
    "            )\n",
    "\n",
    "            print(f\"\\t\\tComputing purity score...\")\n",
    "            x_test_preds, (_, x_test_theta_class_scores) = senn_model(x_test)\n",
    "            test_concept_scores = get_argmax_concept_explanations(\n",
    "                x_test_preds.numpy(),\n",
    "                x_test_theta_class_scores.numpy(),\n",
    "            )\n",
    "\n",
    "            purity_score, (purity_mat, aligned_purity_mat), oracle_mat = oracle.oracle_impurity_score(\n",
    "                c_soft=test_concept_scores,\n",
    "                c_true=c_test,\n",
    "                output_matrices=True,\n",
    "                alignment_function=oracle.max_alignment_matrix,\n",
    "            )\n",
    "            purity_mats.append(aligned_purity_mat)\n",
    "            oracle_mats.append(oracle_mat)\n",
    "            purities.append(purity_score)\n",
    "            print(f\"\\t\\t\\tDone {purity_score:.4f}\")\n",
    "\n",
    "            print(\"\\t\\tComputing non-oracle purity score...\")\n",
    "            non_oracle_purities.append(oracle.oracle_impurity_score(\n",
    "                c_soft=test_concept_scores,\n",
    "                c_true=c_test,\n",
    "                oracle_matrix=construct_trivial_auc_mat(\n",
    "                    c_test.shape[-1]\n",
    "                ),\n",
    "                alignment_function=oracle.max_alignment_matrix,\n",
    "                purity_matrix=aligned_purity_mat,\n",
    "            ))\n",
    "            print(f\"\\t\\t\\tDone {non_oracle_purities[-1]:.4f}\")\n",
    "            print(\"\\t\\tDone with trial\", trial + 1)\n",
    "\n",
    "        task_acc_mean, task_acc_std = np.mean(task_accs), np.std(task_accs)\n",
    "        experiment_variables[\"task_accuracies\"].append((task_acc_mean, task_acc_std))\n",
    "        print(f\"\\tTest task accuracy: {task_acc_mean:.4f} ± {task_acc_std:.4f}\")\n",
    "\n",
    "\n",
    "        task_auc_mean, task_auc_std = np.mean(aucs), np.std(aucs)\n",
    "        experiment_variables[\"task_aucs\"].append((task_auc_mean, task_auc_std))\n",
    "        print(f\"\\tTest task AUC: {task_auc_mean:.4f} ± {task_auc_std:.4f}\")\n",
    "\n",
    "        purity_mats = np.stack(purity_mats, axis=0)\n",
    "        purity_mat_mean = np.mean(purity_mats, axis=0)\n",
    "        purity_mat_std = np.std(purity_mats, axis=0)\n",
    "        print(\"\\tPurity matrix:\")\n",
    "        for i in range(purity_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(purity_mat_mean.shape[1]):\n",
    "                line += f'{purity_mat_mean[i, j]:.4f} ± {purity_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"purity_matrices\"].append((purity_mat_mean, purity_mat_std))\n",
    "\n",
    "\n",
    "        oracle_mats = np.stack(oracle_mats, axis=0)\n",
    "        oracle_mat_mean = np.mean(oracle_mats, axis=0)\n",
    "        oracle_mat_std = np.std(oracle_mats, axis=0)\n",
    "        print(\"\\tOracle matrix:\")\n",
    "        for i in range(oracle_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(oracle_mat_mean.shape[1]):\n",
    "                line += f'{oracle_mat_mean[i, j]:.4f} ± {oracle_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"oracle_matrices\"].append((oracle_mat_mean, oracle_mat_std))\n",
    "\n",
    "        purity_mean, purity_std = np.mean(purities), np.std(purities)\n",
    "        experiment_variables[\"purity_scores\"].append((purity_mean, purity_std))\n",
    "        print(f\"\\tPurity score: {purity_mean:.4f} ± {purity_std:.4f}\")\n",
    "\n",
    "        non_oracle_purity_mean, non_oracle_purity_std = np.mean(non_oracle_purities), np.std(non_oracle_purities)\n",
    "        experiment_variables[\"non_oracle_purity_scores\"].append((non_oracle_purity_mean, non_oracle_purity_std))\n",
    "        print(f\"\\tNon-oracle purity score: {non_oracle_purity_mean:.4f} ± {non_oracle_purity_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(CBM)\n",
    "reload(SENN)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "senn_covariance_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=300,\n",
    "    topic_model_train_epochs=50,\n",
    "    trials=NUM_TRIALS,\n",
    "    learning_rate=1e-3,\n",
    "    \n",
    "    num_concepts=3,\n",
    "    input_shape=[7],\n",
    "    latent_dims=10,\n",
    "    encoder_units=[128, 64],\n",
    "    decoder_units=[128, 64],\n",
    "    latent_act=None,\n",
    "    \n",
    "    coefficient_model_units=[64, 64],\n",
    "    latent_decoder_units=[64, 64],\n",
    "    \n",
    "    predictor_max_epochs=300,\n",
    "    \n",
    "    regularization_strength=0.1,\n",
    "    sparsity_strength=2e-5,\n",
    "    \n",
    "    num_outputs=2,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        \"senn/purity\",\n",
    "    ),\n",
    "    covariances=np.arange(0, 1, 0.1),\n",
    "    train_samples=2000,\n",
    "    test_samples=1000,\n",
    "    verbosity=0,\n",
    "    data_concepts=3,\n",
    "    cw_layer=2,\n",
    "    aggregator='max_pool_mean',\n",
    "    activation_mode='max_pool_mean',\n",
    "    cw_train_freq=20,\n",
    "    concept_auc_freq=0,\n",
    "    cw_train_iterations=1,\n",
    "    holdout_fraction=0.1,\n",
    "    exclusive_concepts=True,\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(senn_covariance_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "senn_covariance_figure_dir = os.path.join(senn_covariance_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(senn_covariance_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "senn_covariance_results = senn_experiment_loop(\n",
    "    experiment_config=senn_covariance_experiment_config,\n",
    "    load_from_cache=LOAD_FROM_CACHE,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"task_accuracies:\", senn_covariance_results[\"task_accuracies\"])\n",
    "print(\"task_aucs:\", senn_covariance_results[\"task_aucs\"])\n",
    "print(\"purity_scores:\", senn_covariance_results[\"purity_scores\"])\n",
    "print(\"non_oracle_purity_scores:\", senn_covariance_results[\"non_oracle_purity_scores\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(CBM)\n",
    "reload(SENN)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "senn_covariance_extended_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=300,\n",
    "    topic_model_train_epochs=50,\n",
    "    trials=NUM_TRIALS,\n",
    "    learning_rate=1e-3,\n",
    "\n",
    "    num_concepts=2*3,\n",
    "    input_shape=[7],\n",
    "    latent_dims=10,\n",
    "    encoder_units=[128, 64],\n",
    "    decoder_units=[128, 64],\n",
    "    latent_act=None,\n",
    "\n",
    "    coefficient_model_units=[64, 64],\n",
    "    latent_decoder_units=[64, 64],\n",
    "\n",
    "    predictor_max_epochs=300,\n",
    "\n",
    "    regularization_strength=0.1,\n",
    "    sparsity_strength=2e-5,\n",
    "\n",
    "    num_outputs=2,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        \"senn/purity_extended\",\n",
    "    ),\n",
    "    covariances=np.arange(0, 1, 0.1),\n",
    "    train_samples=2000,\n",
    "    test_samples=1000,\n",
    "    verbosity=0,\n",
    "    data_concepts=3,\n",
    "    cw_layer=2,\n",
    "    aggregator='max_pool_mean',\n",
    "    activation_mode='max_pool_mean',\n",
    "    cw_train_freq=20,\n",
    "    concept_auc_freq=0,\n",
    "    cw_train_iterations=1,\n",
    "    holdout_fraction=0.1,\n",
    "    exclusive_concepts=True,\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(senn_covariance_extended_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "senn_covariance_extended_figure_dir = os.path.join(senn_covariance_extended_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(senn_covariance_extended_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "senn_covariance_extended_results = senn_experiment_loop(\n",
    "    experiment_config=senn_covariance_extended_experiment_config,\n",
    "    load_from_cache=LOAD_FROM_CACHE,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"task_accuracies:\", senn_covariance_extended_results[\"task_accuracies\"])\n",
    "print(\"task_aucs:\", senn_covariance_extended_results[\"task_aucs\"])\n",
    "print(\"purity_scores:\", senn_covariance_extended_results[\"purity_scores\"])\n",
    "print(\"non_oracle_purity_scores:\", senn_covariance_extended_results[\"non_oracle_purity_scores\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset-wide Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot concept accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = np.arange(0, 10)\n",
    "real_values = np.array(list(map(lambda x: x*0.1, all_vars)))\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        base_results,\n",
    "        \"purity_scores\",\n",
    "        lambda x: x,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_covariance_results,\n",
    "        \"purity_scores\",\n",
    "        lambda x: x,\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_covariance_results,\n",
    "        \"purity_scores\",\n",
    "        lambda x: x,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_results,\n",
    "        \"purity_scores\",\n",
    "        lambda x: x,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_double_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_double_results,\n",
    "        \"purity_scores\",\n",
    "        lambda x: x,\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models * 2 + 5, 5))\n",
    "for i, (method_name, results, kword, transform_fn) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        results[kword],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        results[kword],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (all_vars - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=5,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=15) #, loc='upper center', bbox_to_anchor=(0.5,-0.2), ncol=(num_models - 1))\n",
    "\n",
    "plt.ylabel(\"Oracle Impurity\", fontsize=20)\n",
    "plt.xlabel(\"Cross-concept Covariance ($\\delta$)\", fontsize=20)\n",
    "plt.title(bold_text(\"Oracle Impurity (TabularToy($\\delta$))\"), fontsize=28)\n",
    "plt.xticks(all_vars, fontsize=15)\n",
    "ax.set_xticklabels(list(map(lambda x: f'{x:.1f}', real_values)), fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot concept accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = np.array([0, 9])\n",
    "real_values = np.array(list(map(lambda x: x*0.1, all_vars)))\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        base_results,\n",
    "        \"purity_scores\",\n",
    "        lambda x: x,\n",
    "        all_vars\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_covariance_results,\n",
    "        \"purity_scores\",\n",
    "        lambda x: x,\n",
    "        all_vars\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_covariance_results,\n",
    "        \"purity_scores\",\n",
    "        lambda x: x,\n",
    "        all_vars\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_results,\n",
    "        \"purity_scores\",\n",
    "        lambda x: x,\n",
    "        all_vars\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_double_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_double_results,\n",
    "        \"purity_scores\",\n",
    "        lambda x: x,\n",
    "        all_vars\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models*1.25, 3))\n",
    "for i, (method_name, results, kword, transform_fn, subsample) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[subsample],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[subsample],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=5,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=15, loc='upper center', bbox_to_anchor=(0.5,-0.2), ncol=(num_models - 1)//2)\n",
    "\n",
    "plt.ylabel(\"Oracle Impurity\", fontsize=20)\n",
    "plt.xlabel(\"Cross-concept Covariance ($\\delta$)\", fontsize=20)\n",
    "plt.title(bold_text(\"Oracle Impurity (TabularToy($\\delta$))\"), fontsize=25)\n",
    "plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "ax.set_xticklabels(list(map(lambda x: f'{x:.1f}', real_values)), fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot concept accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = np.array([5])\n",
    "real_values = np.array(list(map(lambda x: x*0.1, all_vars)))\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        base_results,\n",
    "        \"purity_scores\",\n",
    "        lambda x: x,\n",
    "        all_vars\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_covariance_results,\n",
    "        \"purity_scores\",\n",
    "        lambda x: x,\n",
    "        all_vars\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_covariance_results,\n",
    "        \"purity_scores\",\n",
    "        lambda x: x,\n",
    "        all_vars\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_results,\n",
    "        \"purity_scores\",\n",
    "        lambda x: x,\n",
    "        all_vars\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_double_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_double_results,\n",
    "        \"purity_scores\",\n",
    "        lambda x: x,\n",
    "        all_vars\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models, 5))\n",
    "for i, (method_name, results, kword, transform_fn, subsample) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[subsample],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[subsample],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=5,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=15, loc='upper center', bbox_to_anchor=(0.5,-0.1), ncol=(num_models - 1)//2)\n",
    "\n",
    "plt.ylabel(\"Oracle Impurity\", fontsize=20)\n",
    "# plt.xlabel(\"Cross-concept Covariance ($\\delta$)\", fontsize=20)\n",
    "plt.title(bold_text(\"Oracle Impurity (TabularToy($\\delta = 0.5$))\"), fontsize=28)\n",
    "plt.xticks([], fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot concept accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = np.arange(0, 10)\n",
    "real_values = np.array(list(map(lambda x: x*0.1, all_vars)))\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        base_results,\n",
    "        \"concept_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "        all_vars,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_covariance_results,\n",
    "        \"concept_aucs\",\n",
    "        lambda x: 100 * np.mean(x),\n",
    "        all_vars,\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_covariance_results,\n",
    "        \"concept_aucs\",\n",
    "        lambda x: 100 * np.mean(x),\n",
    "        all_vars,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_results,\n",
    "        \"aligned_purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "        all_vars,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_double_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_double_results,\n",
    "        \"aligned_purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "        all_vars,\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models * 2 + 5, 5))\n",
    "for i, (method_name, results, kword, transform_fn, subsample) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[subsample],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[subsample],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=5,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=15, loc='upper center', bbox_to_anchor=(0.5,-0.2), ncol=(num_models - 1))\n",
    "\n",
    "plt.ylabel(\"AUC (\\%)\", fontsize=20)\n",
    "plt.xlabel(\"Cross-concept Covariance ($\\delta$)\", fontsize=20)\n",
    "plt.title(bold_text(\"Mean Concept Accuracy (TabularToy($\\delta$))\"), fontsize=28)\n",
    "plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "ax.set_xticklabels(list(map(lambda x: f'{x:.1f}', real_values)), fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot concept accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = np.array([0, 9])\n",
    "real_values = np.array(list(map(lambda x: x*0.1, all_vars)))\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        base_results,\n",
    "        \"concept_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "        all_vars,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_covariance_results,\n",
    "        \"concept_aucs\",\n",
    "        lambda x: 100 * np.mean(x),\n",
    "        all_vars,\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_covariance_results,\n",
    "        \"concept_aucs\",\n",
    "        lambda x: 100 * np.mean(x),\n",
    "        all_vars,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_results,\n",
    "        \"aligned_purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "        all_vars,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_double_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_double_results,\n",
    "        \"aligned_purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "        all_vars,\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models * 1.5, 5))\n",
    "for i, (method_name, results, kword, transform_fn, subsample) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[subsample],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[subsample],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=5,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=15, loc='upper center', bbox_to_anchor=(0.5,-0.2), ncol=(num_models - 1)//2)\n",
    "\n",
    "plt.ylabel(\"AUC (\\%)\", fontsize=20)\n",
    "plt.xlabel(\"Cross-concept Covariance ($\\delta$)\", fontsize=20)\n",
    "plt.title(bold_text(\"Mean Concept Accuracy (TabularToy($\\delta$))\"), fontsize=28)\n",
    "plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "ax.set_xticklabels(list(map(lambda x: f'{x:.1f}', real_values)), fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot concept accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = np.array([5])\n",
    "real_values = np.array(list(map(lambda x: x*0.1, all_vars)))\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        base_results,\n",
    "        \"concept_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "        all_vars,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_covariance_results,\n",
    "        \"concept_aucs\",\n",
    "        lambda x: 100 * np.mean(x),\n",
    "        all_vars,\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_covariance_results,\n",
    "        \"concept_aucs\",\n",
    "        lambda x: 100 * np.mean(x),\n",
    "        all_vars,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_results,\n",
    "        \"aligned_purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "        all_vars,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_double_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_double_results,\n",
    "        \"aligned_purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "        all_vars,\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models, 5))\n",
    "for i, (method_name, results, kword, transform_fn, subsample) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[subsample],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[subsample],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=5,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=15, loc='upper center', bbox_to_anchor=(0.5,-0.1), ncol=(num_models - 1)//2)\n",
    "\n",
    "plt.ylabel(\"AUC (\\%)\", fontsize=20)\n",
    "# plt.xlabel(\"Cross-concept Covariance ($\\delta$)\", fontsize=20)\n",
    "plt.title(bold_text(\"Mean Concept Accuracy (TabularToy($\\delta = 0.5$))\"), fontsize=28)\n",
    "plt.xticks([], fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot concept accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = np.arange(0, 10)\n",
    "real_values = np.array(list(map(lambda x: x*0.1, all_vars)))\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        base_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_covariance_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_covariance_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_double_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_double_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models * 2, 5))\n",
    "for i, (method_name, results, kword, transform_fn) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=5,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=15, loc='upper center', bbox_to_anchor=(0.5,-0.2), ncol=(num_models - 1)//2)\n",
    "\n",
    "plt.ylabel(\"AUC (\\%)\", fontsize=20)\n",
    "plt.xlabel(\"Cross-concept Covariance ($\\delta$)\", fontsize=20)\n",
    "plt.title(bold_text(\"Downstream Task AUC (TabularToy($\\delta$))\"), fontsize=28)\n",
    "plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "ax.set_xticklabels(list(map(lambda x: f'{x:.1f}', real_values)), fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot concept accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = [0, 9]\n",
    "real_values = np.array(list(map(lambda x: x*0.1, all_vars)))\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        base_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_covariance_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_covariance_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_double_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_double_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models * 1.5, 5))\n",
    "for i, (method_name, results, kword, transform_fn) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=5,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=15, loc='upper center', bbox_to_anchor=(0.5,-0.2), ncol=(num_models - 1)//2)\n",
    "\n",
    "plt.ylabel(\"AUC (\\%)\", fontsize=20)\n",
    "plt.xlabel(\"Cross-concept Covariance ($\\delta$)\", fontsize=20)\n",
    "plt.title(bold_text(\"Downstream Task AUC (TabularToy($\\delta$))\"), fontsize=28)\n",
    "plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "ax.set_xticklabels(list(map(lambda x: f'{x:.1f}', real_values)), fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot concept accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = [5]\n",
    "real_values = np.array(list(map(lambda x: x*0.1, all_vars)))\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        base_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_covariance_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_covariance_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_double_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_double_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models, 5))\n",
    "for i, (method_name, results, kword, transform_fn) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=5,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=15, loc='upper center', bbox_to_anchor=(0.5,-0.2), ncol=(num_models - 1)//2)\n",
    "\n",
    "plt.ylabel(\"AUC (\\%)\", fontsize=20)\n",
    "if len(all_vars) > 1:\n",
    "    plt.xlabel(\"Cross-concept Covariance ($\\delta$)\", fontsize=20)\n",
    "    plt.title(bold_text(\"Downstream Task AUC (TabularToy($\\delta$))\"), fontsize=28)\n",
    "    plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "    ax.set_xticklabels(list(map(lambda x: f'{x:.1f}', real_values)), fontsize=15)\n",
    "else:\n",
    "    plt.title(bold_text(\"Downstream Task AUC (TabularToy($\\delta = \" + str(all_vars[0] * 0.1) + \"$))\"), fontsize=28)\n",
    "    plt.xticks([], fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot concept accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = np.arange(0, 10)\n",
    "real_values = np.array(list(map(lambda x: x*0.1, all_vars)))\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        base_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_covariance_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_covariance_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_double_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_double_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models * 2 + 5, 5))\n",
    "for i, (method_name, results, kword, transform_fn) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=5,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=15, loc='upper center', bbox_to_anchor=(0.5,-0.2), ncol=(num_models - 1))\n",
    "\n",
    "plt.ylabel(\"Accuracy (\\%)\", fontsize=20)\n",
    "if len(all_vars) > 1:\n",
    "    plt.xlabel(\"Cross-concept Covariance ($\\delta$)\", fontsize=20)\n",
    "    plt.title(bold_text(\"Downstream Task AUC (TabularToy($\\delta$))\"), fontsize=28)\n",
    "    plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "    ax.set_xticklabels(list(map(lambda x: f'{x:.1f}', real_values)), fontsize=15)\n",
    "else:\n",
    "    plt.title(bold_text(\"Downstream Task AUC (TabularToy($\\delta = \" + str(all_vars[0] * 0.1) + \"$))\"), fontsize=28)\n",
    "    plt.xticks([], fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot concept accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = [0, 9]\n",
    "real_values = np.array(list(map(lambda x: x*0.1, all_vars)))\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        base_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_covariance_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_covariance_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_double_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_double_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models * 1.5, 5))\n",
    "for i, (method_name, results, kword, transform_fn) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=5,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=15, loc='upper center', bbox_to_anchor=(0.5,-0.2), ncol=(num_models - 1)//2)\n",
    "\n",
    "plt.ylabel(\"Accuracy (\\%)\", fontsize=20)\n",
    "if len(all_vars) > 1:\n",
    "    plt.xlabel(\"Cross-concept Covariance ($\\delta$)\", fontsize=20)\n",
    "    plt.title(bold_text(\"Downstream Task AUC (TabularToy($\\delta$))\"), fontsize=28)\n",
    "    plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "    ax.set_xticklabels(list(map(lambda x: f'{x:.1f}', real_values)), fontsize=15)\n",
    "else:\n",
    "    plt.title(bold_text(\"Downstream Task AUC (TabularToy($\\delta = \" + str(all_vars[0] * 0.1) + \"$))\"), fontsize=28)\n",
    "    plt.xticks([], fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot concept accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = [5]\n",
    "real_values = np.array(list(map(lambda x: x*0.1, all_vars)))\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        base_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_covariance_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_covariance_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_double_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_double_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models, 5))\n",
    "for i, (method_name, results, kword, transform_fn) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=5,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=15, loc='upper center', bbox_to_anchor=(0.5,-0.2), ncol=(num_models - 1)//2)\n",
    "\n",
    "plt.ylabel(\"Accuracy (\\%)\", fontsize=20)\n",
    "if len(all_vars) > 1:\n",
    "    plt.xlabel(\"Cross-concept Covariance ($\\delta$)\", fontsize=20)\n",
    "    plt.title(bold_text(\"Downstream Task AUC (TabularToy($\\delta$))\"), fontsize=28)\n",
    "    plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "    ax.set_xticklabels(list(map(lambda x: f'{x:.1f}', real_values)), fontsize=15)\n",
    "else:\n",
    "    plt.title(bold_text(\"Downstream Task AUC (TabularToy($\\delta = \" + str(all_vars[0] * 0.1) + \"$))\"), fontsize=28)\n",
    "    plt.xticks([], fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot concept accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = np.arange(0, 10)\n",
    "real_values = np.array(list(map(lambda x: x*0.1, all_vars)))\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        base_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_covariance_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_covariance_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_double_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_double_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models, 5))\n",
    "for i, (method_name, results, kword, transform_fn) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=5,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=15, loc='upper center', bbox_to_anchor=(0.5,-0.2), ncol=(num_models - 1)//2)\n",
    "\n",
    "plt.ylabel(\"Accuracy (\\%)\", fontsize=20)\n",
    "if len(all_vars) > 1:\n",
    "    plt.xlabel(\"Cross-concept Covariance ($\\delta$)\", fontsize=20)\n",
    "    plt.title(bold_text(\"Downstream Task AUC (TabularToy($\\delta$))\"), fontsize=28)\n",
    "    plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "    ax.set_xticklabels(list(map(lambda x: f'{x:.1f}', real_values)), fontsize=15)\n",
    "else:\n",
    "    plt.title(bold_text(\"Downstream Task AUC (TabularToy($\\delta = \" + str(all_vars[0] * 0.1) + \"$))\"), fontsize=28)\n",
    "    plt.xticks([], fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot concept accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = np.arange(0, 10)\n",
    "real_values = np.array(list(map(lambda x: x*0.1, all_vars)))\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        base_results,\n",
    "        \"latent_predictive_aucs\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_covariance_results,\n",
    "        \"latent_predictive_aucs\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_covariance_results,\n",
    "        \"latent_predictive_aucs\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_results,\n",
    "        \"direct_completeness_scores\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_double_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_double_results,\n",
    "        \"direct_completeness_scores\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models * 2 + 5, 5))\n",
    "for i, (method_name, results, kword, transform_fn) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=5,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=15, loc='upper center', bbox_to_anchor=(0.5,-0.2), ncol=(num_models - 1))\n",
    "\n",
    "plt.ylabel(\"AUC (\\%)\", fontsize=20)\n",
    "if len(all_vars) > 1:\n",
    "    plt.xlabel(\"Cross-concept Covariance ($\\delta$)\", fontsize=20)\n",
    "    plt.title(bold_text(\"Downstream Task AUC from Concepts (TabularToy($\\delta$))\"), fontsize=28)\n",
    "    plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "    ax.set_xticklabels(list(map(lambda x: f'{x:.1f}', real_values)), fontsize=15)\n",
    "else:\n",
    "    plt.title(bold_text(\"Downstream Task AUC from Concepts (TabularToy($\\delta = \" + str(all_vars[0] * 0.1) + \"$))\"), fontsize=28)\n",
    "    plt.xticks([], fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot concept accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = [0, 9]\n",
    "real_values = np.array(list(map(lambda x: x*0.1, all_vars)))\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        base_results,\n",
    "        \"latent_predictive_aucs\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_covariance_results,\n",
    "        \"latent_predictive_aucs\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_covariance_results,\n",
    "        \"latent_predictive_aucs\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_results,\n",
    "        \"direct_completeness_scores\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_double_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_double_results,\n",
    "        \"direct_completeness_scores\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models * 2 + 5, 5))\n",
    "for i, (method_name, results, kword, transform_fn) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=5,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=15, loc='upper center', bbox_to_anchor=(0.5,-0.2), ncol=(num_models - 1))\n",
    "\n",
    "plt.ylabel(\"AUC (\\%)\", fontsize=20)\n",
    "if len(all_vars) > 1:\n",
    "    plt.xlabel(\"Cross-concept Covariance ($\\delta$)\", fontsize=20)\n",
    "    plt.title(bold_text(\"Downstream Task AUC from Concepts (TabularToy($\\delta$))\"), fontsize=28)\n",
    "    plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "    ax.set_xticklabels(list(map(lambda x: f'{x:.1f}', real_values)), fontsize=15)\n",
    "else:\n",
    "    plt.title(bold_text(\"Downstream Task AUC from Concepts (TabularToy($\\delta = \" + str(all_vars[0] * 0.1) + \"$))\"), fontsize=28)\n",
    "    plt.xticks([], fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot concept accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = [0, 9]\n",
    "real_values = np.array(list(map(lambda x: x*0.1, all_vars)))\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        base_results,\n",
    "        \"latent_predictive_accuracies\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_covariance_results,\n",
    "        \"latent_predictive_accuracies\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_covariance_results,\n",
    "        \"latent_predictive_accuracies\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_results,\n",
    "        \"direct_completeness_scores\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_covariance_double_experiment_config['num_concepts']})\",\n",
    "        ccd_covariance_double_results,\n",
    "        \"direct_completeness_scores\",\n",
    "        lambda x: x * 100,\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models * 2 + 5, 5))\n",
    "for i, (method_name, results, kword, transform_fn) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=5,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=15, loc='upper center', bbox_to_anchor=(0.5,-0.2), ncol=(num_models - 1))\n",
    "\n",
    "plt.ylabel(\"Accuracy (\\%)\", fontsize=20)\n",
    "if len(all_vars) > 1:\n",
    "    plt.xlabel(\"Cross-concept Covariance ($\\delta$)\", fontsize=20)\n",
    "    plt.title(bold_text(\"Downstream Task Accuracy from Concepts (TabularToy($\\delta$))\"), fontsize=28)\n",
    "    plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "    ax.set_xticklabels(list(map(lambda x: f'{x:.1f}', real_values)), fontsize=15)\n",
    "else:\n",
    "    plt.title(bold_text(\"Downstream Task Accuracy from Concepts (TabularToy($\\delta = \" + str(all_vars[0] * 0.1) + \"$))\"), fontsize=28)\n",
    "    plt.xticks([], fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
