{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Comparison Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import concepts_xai\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import rc\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "## Set seeds up for reproducibility\n",
    "################################################################################\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(87)\n",
    "tf.random.set_seed(87)\n",
    "np.random.seed(87)\n",
    "random.seed(87)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "## Global Variables Defining Experiment Flow\n",
    "################################################################################\n",
    "\n",
    "LATEX_SYMBOL = \"\"  # Change to \"$\" if working out of server\n",
    "RESULTS_DIR = \"results/metric_example_results\"\n",
    "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "rc('text', usetex=(LATEX_SYMBOL == \"$\"))\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "def serialize_results(results_dict, results_dir):\n",
    "    joblib.dump(results_dict, os.path.join(results_dir, \"raw_results.joblib\"))\n",
    "    for result_name, result_arr in results_dict.items():\n",
    "        if not isinstance(result_arr, (list, np.ndarray)):\n",
    "            for key, result_arr in result_arr.items():\n",
    "                np.savez(\n",
    "                    os.path.join(results_dir, f\"{result_name}_{key}_means.npz\"),\n",
    "                    *list(map(\n",
    "                        lambda x: x[0] if isinstance(x[0], np.ndarray) else np.array(x[0]),\n",
    "                        result_arr\n",
    "                    )),\n",
    "                )\n",
    "                np.savez(\n",
    "                    os.path.join(results_dir, f\"{result_name}_{key}_stds.npz\"),\n",
    "                    *list(map(\n",
    "                        lambda x: x[1] if isinstance(x[1], np.ndarray) else np.array(x[1]),\n",
    "                        result_arr\n",
    "                    )),\n",
    "                )\n",
    "        else:\n",
    "            np.savez(\n",
    "                os.path.join(results_dir, f\"{result_name}_means.npz\"),\n",
    "                *list(map(\n",
    "                    lambda x: x[0] if isinstance(x[0], np.ndarray) else np.array(x[0]),\n",
    "                    result_arr\n",
    "                )),\n",
    "            )\n",
    "            np.savez(\n",
    "                os.path.join(results_dir, f\"{result_name}_stds.npz\"),\n",
    "                *list(map(\n",
    "                    lambda x: x[1] if isinstance(x[1], np.ndarray) else np.array(x[1]),\n",
    "                    result_arr\n",
    "                )),\n",
    "            )\n",
    "\n",
    "def serialize_experiment_config(config, results_dir):\n",
    "    with open(\n",
    "        os.path.join(results_dir, \"config.yaml\"),\n",
    "        'w',\n",
    "    ) as f:\n",
    "        f.write(yaml.dump(config, sort_keys=True))\n",
    "\n",
    "        \n",
    "def deserialize_experiment_config(results_dir):\n",
    "    with open(os.path.join(results_dir, \"config.yaml\"), 'r') as file:\n",
    "        return yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## Generate Data\n",
    "############################################################################\n",
    "\n",
    "def produce_data_larger(samples, cov=0.0, num_concepts=3):\n",
    "    x = np.zeros((samples, 7), dtype=np.float32)\n",
    "    y = np.zeros((samples,), dtype=np.float32)\n",
    "    \n",
    "    # Sample the x, y, and z variables\n",
    "    mean = np.zeros((num_concepts,))\n",
    "    cov = np.eye(num_concepts)\n",
    "    cov += (np.ones((num_concepts, num_concepts)) - np.eye(num_concepts))  * cov \n",
    "    vars = np.random.multivariate_normal(\n",
    "        mean=mean,\n",
    "        cov=cov,\n",
    "        size=(samples,),\n",
    "    )\n",
    "    x_vars = vars[:, :1]\n",
    "    y_vars = vars[:, 1:2]\n",
    "    z_vars = vars[:, 2:3]\n",
    "    \n",
    "    # The features are just non-linear functions applied to each\n",
    "    # variable\n",
    "    features = [\n",
    "        np.sin(x_vars) + x_vars,\n",
    "        np.cos(x_vars) + x_vars,\n",
    "        np.sin(y_vars) + y_vars,\n",
    "        np.cos(y_vars) + y_vars,\n",
    "        np.sin(z_vars) + z_vars,\n",
    "        np.cos(z_vars) + z_vars,\n",
    "        x_vars**2 + y_vars**2 + z_vars**2,\n",
    "    ]\n",
    "    features = np.stack(features, axis=1)\n",
    "\n",
    "    # The concepts just check if the variables are positive\n",
    "    concepts = (vars > 0).astype(np.int32)\n",
    "    \n",
    "    # The labels are generated by checking if at least two of the\n",
    "    # latent concepts are greater than zero\n",
    "    labels = np.sum(concepts[:, :3], axis=-1)\n",
    "    labels = (labels > 1).astype(np.int32)\n",
    "    \n",
    "    # And that's it buds\n",
    "    return features, labels, concepts\n",
    "\n",
    "def produce_data(samples, cov=0.0, num_concepts=3):\n",
    "    x = np.zeros((samples, 7), dtype=np.float32)\n",
    "    y = np.zeros((samples,), dtype=np.float32)\n",
    "    \n",
    "    # Sample the x, y, and z variables\n",
    "    vars = np.random.multivariate_normal(\n",
    "        mean=[0, 0, 0],\n",
    "        cov=[\n",
    "            [1, cov, cov],\n",
    "            [cov, 1, cov],\n",
    "            [cov, cov, 1],\n",
    "        ],\n",
    "        size=(samples,),\n",
    "    )\n",
    "    x_vars = vars[:, :1]\n",
    "    y_vars = vars[:, 1:2]\n",
    "    z_vars = vars[:, 2:]\n",
    "    \n",
    "    # The features are just non-linear functions applied to each\n",
    "    # variable\n",
    "    features = [\n",
    "        np.sin(x_vars) + x_vars,\n",
    "        np.cos(x_vars) + x_vars,\n",
    "        np.sin(y_vars) + y_vars,\n",
    "        np.cos(y_vars) + y_vars,\n",
    "        np.sin(z_vars) + z_vars,\n",
    "        np.cos(z_vars) + z_vars,\n",
    "        x_vars**2 + y_vars**2 + z_vars**2,\n",
    "    ]\n",
    "    features = np.stack(features, axis=1)\n",
    "\n",
    "    # The concepts just check if the variables are positive\n",
    "    x_pos = (x_vars > 0).astype(np.int32)\n",
    "    y_pos = (y_vars > 0).astype(np.int32)\n",
    "    z_pos = (z_vars > 0).astype(np.int32)\n",
    "    concepts = np.squeeze(\n",
    "        np.stack([x_pos, y_pos, z_pos][:num_concepts], axis=1)\n",
    "    )\n",
    "    \n",
    "    # The labels are generated by checking if at least two of the\n",
    "    # latent concepts are greater than zero\n",
    "    labels = x_pos + y_pos + z_pos\n",
    "    labels = (labels > 1).astype(np.int32)\n",
    "    \n",
    "    # And that's it buds\n",
    "    return features, labels, concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from sklearn import svm\n",
    "\n",
    "########\n",
    "## NIS\n",
    "########\n",
    "\n",
    "def construct_trivial_auc_mat(num_concepts):\n",
    "    result = np.ones((num_concepts, num_concepts), dtype=np.float32) * 0.5\n",
    "    return result + np.eye(num_concepts, dtype=np.float32) * 0.5\n",
    "\n",
    "##############\n",
    "## SAP. Code taken from Locatello et al. https://github.com/google-research/disentanglement_lib\n",
    "##############\n",
    "\n",
    "def _compute_sap(mus, ys, mus_test, ys_test, continuous_factors):\n",
    "  \"\"\"Computes score based on both training and testing codes and factors.\"\"\"\n",
    "  score_matrix = compute_score_matrix(mus, ys, mus_test,\n",
    "                                      ys_test, continuous_factors)\n",
    "  # Score matrix should have shape [num_latents, num_factors].\n",
    "  assert score_matrix.shape[0] == mus.shape[0]\n",
    "  assert score_matrix.shape[1] == ys.shape[0]\n",
    "  scores_dict = {}\n",
    "  scores_dict[\"SAP_score\"] = compute_avg_diff_top_two(score_matrix)\n",
    "  return scores_dict\n",
    "\n",
    "\n",
    "def compute_sap_on_fixed_data(observations, labels, representation_function,\n",
    "                              train_percentage=0.2,\n",
    "                              continuous_factors=False,\n",
    "                              batch_size=100):\n",
    "  \"\"\"Computes the SAP score on the fixed set of observations and labels.\n",
    "  Args:\n",
    "    observations: Observations on which to compute the score. Observations have\n",
    "      shape (num_observations, 64, 64, num_channels).\n",
    "    labels: Observed factors of variations.\n",
    "    representation_function: Function that takes observations as input and\n",
    "      outputs a dim_representation sized representation for each observation.\n",
    "    train_percentage: Percentage of observations used for training.\n",
    "    continuous_factors: Whether factors should be considered continuous or\n",
    "      discrete.\n",
    "    batch_size: Batch size used to compute the representation.\n",
    "  Returns:\n",
    "    SAP computed on the provided observations and labels.\n",
    "  \"\"\"\n",
    "  labels = np.transpose(labels)\n",
    "  mus = utils.obtain_representation(observations, representation_function,\n",
    "                                    batch_size)\n",
    "  assert labels.shape[1] == observations.shape[0], \"Wrong labels shape.\"\n",
    "  assert mus.shape[1] == observations.shape[0], \"Wrong representation shape.\"\n",
    "  mus_train, mus_test = utils.split_train_test(\n",
    "      mus,\n",
    "      train_percentage)\n",
    "  ys_train, ys_test = utils.split_train_test(\n",
    "      labels,\n",
    "      train_percentage)\n",
    "  return _compute_sap(mus_train, ys_train, mus_test, ys_test,\n",
    "                      continuous_factors)[\"SAP_score\"]\n",
    "\n",
    "\n",
    "def compute_score_matrix(mus, ys, mus_test, ys_test, continuous_factors):\n",
    "  \"\"\"Compute score matrix as described in Section 3.\"\"\"\n",
    "  num_latents = mus.shape[0]\n",
    "  num_factors = ys.shape[0]\n",
    "  score_matrix = np.zeros([num_latents, num_factors])\n",
    "  for i in range(num_latents):\n",
    "    for j in range(num_factors):\n",
    "      mu_i = mus[i, :]\n",
    "      y_j = ys[j, :]\n",
    "      if continuous_factors:\n",
    "        # Attribute is considered continuous.\n",
    "        cov_mu_i_y_j = np.cov(mu_i, y_j, ddof=1)\n",
    "        cov_mu_y = cov_mu_i_y_j[0, 1]**2\n",
    "        var_mu = cov_mu_i_y_j[0, 0]\n",
    "        var_y = cov_mu_i_y_j[1, 1]\n",
    "        if var_mu > 1e-12:\n",
    "          score_matrix[i, j] = cov_mu_y * 1. / (var_mu * var_y)\n",
    "        else:\n",
    "          score_matrix[i, j] = 0.\n",
    "      else:\n",
    "        # Attribute is considered discrete.\n",
    "        mu_i_test = mus_test[i, :]\n",
    "        y_j_test = ys_test[j, :]\n",
    "        classifier = svm.LinearSVC(C=0.01, class_weight=\"balanced\")\n",
    "        classifier.fit(mu_i[:, np.newaxis], y_j)\n",
    "        pred = classifier.predict(mu_i_test[:, np.newaxis])\n",
    "        score_matrix[i, j] = np.mean(pred == y_j_test)\n",
    "  return score_matrix\n",
    "\n",
    "\n",
    "def compute_avg_diff_top_two(matrix):\n",
    "  sorted_matrix = np.sort(matrix, axis=0)\n",
    "  return np.mean(sorted_matrix[-1, :] - sorted_matrix[-2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "## Code taken from Ross et al. https://github.com/dtak/hierarchical-disentanglement\n",
    "##############\n",
    "\n",
    "\n",
    "\n",
    "##############\n",
    "## R4\n",
    "##############\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from collections import Counter\n",
    "\n",
    "###############################################################################\n",
    "#\n",
    "# R4 and R4c scores (our contribution)\n",
    "#\n",
    "# These metrics quantify the extent to which every dimension of a ground-truth\n",
    "# representation V can be mapped individually (via an invertible function) to\n",
    "# dimensions of a learned representation Z. They accomplish this by considering\n",
    "# the R^2 coefficient of determination in both directions and taking geometric\n",
    "# means.\n",
    "#\n",
    "# The conditional version (R4c) also takes into account the hierarchy, scoping\n",
    "# comparisons to cases where both learned and ground-truth factors are active,\n",
    "# and not penalizing minor differences in the distribution of continuous dims.\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "def activity_mask(v):\n",
    "    # Slight kludge to detect activity; could pass a separate mask variable\n",
    "    # instead\n",
    "    return (np.abs(v) > 1e-10).astype(int)\n",
    "\n",
    "def is_categorical(v, max_uniq=10):\n",
    "    # Also kind of a kludge, but assume a variable is categorical if it's\n",
    "    # integer-valued and there are few possible options. Could use the\n",
    "    # hierarchy object instead.\n",
    "    return len(np.unique(v)) <= max_uniq and np.allclose(v.astype(int), v)\n",
    "\n",
    "def sample_R2_oneway(inputs, targets, reg=GradientBoostingRegressor, kls=GradientBoostingClassifier):\n",
    "    if len(inputs) < 2:\n",
    "        # Handle edge case of nearly empty input\n",
    "        return 0\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(inputs.reshape(-1,1), targets)\n",
    "    n_uniq = min(len(np.unique(y_train)), len(np.unique(y_test)))\n",
    "\n",
    "    if n_uniq == 1:\n",
    "        # Handle edge case of only one target\n",
    "        return 1 \n",
    "    elif is_categorical(targets):\n",
    "        # Use a classifier for categorical data\n",
    "        y_train = y_train.astype(int)\n",
    "        y_test = y_test.astype(int)\n",
    "        model = kls()\n",
    "    else:\n",
    "        # Use a regressor otherwise\n",
    "        model = reg()\n",
    "\n",
    "    # Return the R^2 (or accuracy) score\n",
    "    return model.fit(x_train, y_train).score(x_test, y_test)\n",
    "\n",
    "def R2_oneway(inputs, targets, iters=5, **kw):\n",
    "    # Repeatedly compute R^2 over random splits\n",
    "    return np.mean([sample_R2_oneway(inputs, targets, **kw) for _ in range(iters)])\n",
    "\n",
    "def R2_bothways(x, y):\n",
    "    # Take the geometric mean of R^2 in both directions\n",
    "    r1 = max(0, R2_oneway(x,y))\n",
    "    r2 = max(0, R2_oneway(y,x))\n",
    "    return np.sqrt(r1*r2)\n",
    "\n",
    "def R4_scores(V, Z):\n",
    "    # For each dimension, find the best R2_bothways\n",
    "    scores = []\n",
    "\n",
    "    for i in range(V.shape[1]):\n",
    "        best = 0\n",
    "        for j in range(Z.shape[1]):\n",
    "            best = max(best, R2_bothways(V[:,i], Z[:,j]))\n",
    "        scores.append(best)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#\n",
    "# Mutual Information Gap (MIG) Baseline\n",
    "#\n",
    "# Technically not defined for continuous targets, but we discretize with 20-bin\n",
    "# histograms.\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "def estimate_mutual_information(X, Y, bins=20):\n",
    "  hist = np.histogram2d(X, Y, bins)[0] # approximate joint\n",
    "  info = mutual_info_score(None, None, contingency=hist)\n",
    "  return info / np.log(2) # bits\n",
    "\n",
    "def estimate_entropy(X, **kw):\n",
    "  return estimate_mutual_information(X, X, **kw)\n",
    "\n",
    "def MIG(Z_true, Z_learned, **kw):\n",
    "  K = Z_true.shape[1]\n",
    "  gap = 0\n",
    "  for k in range(K):\n",
    "    H = estimate_entropy(Z_true[:,k], **kw)\n",
    "    MIs = sorted([\n",
    "      estimate_mutual_information(Z_learned[:,j], Z_true[:,k], **kw)\n",
    "      for j in range(Z_learned.shape[1])\n",
    "    ], reverse=True)\n",
    "    gap += (MIs[0] - MIs[1]) / (H * K)\n",
    "  return gap\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#\n",
    "# DCI (Disentanglement, Completeness, Informativeness) Baseline\n",
    "#\n",
    "# Code adapted from https://github.com/google-research/disentanglement_lib,\n",
    "# original paper at https://openreview.net/forum?id=By-7dz-AZ.\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "def DCI(gen_factors, latents):\n",
    "  \"\"\"Computes score based on both training and testing codes and factors.\"\"\"\n",
    "  mus_train, mus_test, ys_train, ys_test = train_test_split(gen_factors, latents, test_size=0.1)\n",
    "  scores = {}\n",
    "  importance_matrix, train_err, test_err = compute_importance_gbt(mus_train, ys_train, mus_test, ys_test)\n",
    "  assert importance_matrix.shape[0] == mus_train.shape[1]\n",
    "  assert importance_matrix.shape[1] == ys_train.shape[1]\n",
    "  scores[\"informativeness_train\"] = train_err\n",
    "  scores[\"informativeness_test\"] = test_err\n",
    "  scores[\"disentanglement\"] = disentanglement(importance_matrix)\n",
    "  scores[\"completeness\"] = completeness(importance_matrix)\n",
    "  return scores[\"disentanglement\"], scores[\"completeness\"], scores[\"informativeness_test\"]\n",
    "\n",
    "def compute_importance_gbt(x_train, y_train, x_test, y_test):\n",
    "  \"\"\"Compute importance based on gradient boosted trees.\"\"\"\n",
    "  num_factors = y_train.shape[1]\n",
    "  num_codes = x_train.shape[1]\n",
    "  importance_matrix = np.zeros(shape=[num_codes, num_factors],\n",
    "                               dtype=np.float64)\n",
    "  train_loss = []\n",
    "  test_loss = []\n",
    "  for i in range(num_factors):\n",
    "    model = GradientBoostingRegressor()\n",
    "    model.fit(x_train, y_train[:,i])\n",
    "    importance_matrix[:, i] = np.abs(model.feature_importances_)\n",
    "    train_loss.append(model.score(x_train, y_train[:,i]))\n",
    "    test_loss.append(model.score(x_test, y_test[:,i]))\n",
    "  return importance_matrix, np.mean(train_loss), np.mean(test_loss)\n",
    "\n",
    "\n",
    "def disentanglement_per_code(importance_matrix):\n",
    "  \"\"\"Compute disentanglement score of each code.\"\"\"\n",
    "  # importance_matrix is of shape [num_codes, num_factors].\n",
    "  return 1. - scipy.stats.entropy(importance_matrix.T + 1e-11,\n",
    "                                  base=importance_matrix.shape[1])\n",
    "\n",
    "\n",
    "def disentanglement(importance_matrix):\n",
    "  \"\"\"Compute the disentanglement score of the representation.\"\"\"\n",
    "  per_code = disentanglement_per_code(importance_matrix)\n",
    "  if importance_matrix.sum() == 0.:\n",
    "    importance_matrix = np.ones_like(importance_matrix)\n",
    "  code_importance = importance_matrix.sum(axis=1) / importance_matrix.sum()\n",
    "\n",
    "  return np.sum(per_code*code_importance)\n",
    "\n",
    "def completeness_per_factor(importance_matrix):\n",
    "  \"\"\"Compute completeness of each factor.\"\"\"\n",
    "  # importance_matrix is of shape [num_codes, num_factors].\n",
    "  return 1. - scipy.stats.entropy(importance_matrix + 1e-11,\n",
    "                                  base=importance_matrix.shape[0])\n",
    "\n",
    "\n",
    "def completeness(importance_matrix):\n",
    "  \"\"\"\"Compute completeness of the representation.\"\"\"\n",
    "  per_factor = completeness_per_factor(importance_matrix)\n",
    "  if importance_matrix.sum() == 0.:\n",
    "    importance_matrix = np.ones_like(importance_matrix)\n",
    "  factor_importance = importance_matrix.sum(axis=0) / importance_matrix.sum()\n",
    "  return np.sum(per_factor*factor_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Comparison Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.evaluation.metrics.purity as purity\n",
    "import concepts_xai.evaluation.metrics.completeness as xai_completeness\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import concepts_xai.evaluation.metrics.niching as niching\n",
    "from collections import defaultdict\n",
    "\n",
    "############################################################################\n",
    "## Experiment loop\n",
    "############################################################################\n",
    "\n",
    "def generate_soft_activations(\n",
    "    c_test,\n",
    "    mode=\"random\", # \"random\", \"localized\", \"distributed\"\n",
    "    off_start_interval=0.0,\n",
    "    off_end_interval=0.5,\n",
    "    on_start_interval=0.5,\n",
    "    on_end_interval=1.0,\n",
    "):\n",
    "    num_partitions = 2**(c_test.shape[-1] - 1)\n",
    "    soft_activations = c_test.astype(np.float32)\n",
    "    for sample_idx in range(c_test.shape[0]):\n",
    "        for concept_idx in range(c_test.shape[-1]):\n",
    "            if c_test[sample_idx, concept_idx] == 1:\n",
    "                # Then the concept is ON\n",
    "                start_interval = on_start_interval\n",
    "                end_interval = on_end_interval\n",
    "            else:\n",
    "                start_interval = off_start_interval\n",
    "                end_interval = off_end_interval\n",
    "            if \"localized\" in mode:\n",
    "                partition_size = (end_interval - start_interval)/num_partitions\n",
    "                # Generate an index corresponding to the binary encoding of the remaining\n",
    "                # concepts\n",
    "                segment_index = ''\n",
    "                for j in range(c_test.shape[-1]):\n",
    "                    if j != concept_idx:\n",
    "                        segment_index += str(int(c_test[sample_idx, j]))\n",
    "                segment_index = int(segment_index, 2)\n",
    "                start_interval += segment_index * partition_size\n",
    "                end_interval = start_interval + partition_size\n",
    "            elif \"distributed\" in mode:\n",
    "                # Then let's distribute the knowledge of a bit being active across\n",
    "                # the other bits\n",
    "                partition_size = (end_interval - start_interval)/num_partitions\n",
    "                segment_index = ''\n",
    "                for j in np.random.permutation(c_test.shape[-1]):\n",
    "                    if j == concept_idx:\n",
    "                        continue\n",
    "                    if c_test[sample_idx, j] == 1:\n",
    "                        # Then we for sure mark its corresponding bit on\n",
    "                        segment_index += str(int(c_test[sample_idx, j]))\n",
    "                    else:\n",
    "                        # Else we randomly select its corresponding bit value\n",
    "                        segment_index += str(np.random.randint(2))\n",
    "                segment_index = int(segment_index, 2)\n",
    "                start_interval += segment_index * partition_size\n",
    "                end_interval = start_interval + partition_size\n",
    "            elif \"unaligned\" == mode:\n",
    "                # Then let's distribute the knowledge of a bit being active across\n",
    "                # the other bits\n",
    "                end_interval = 1.0\n",
    "                start_interval = 0.0\n",
    "                partition_size = (end_interval - start_interval)/num_partitions\n",
    "                segment_index = ''\n",
    "                for j in range(c_test.shape[-1]):\n",
    "                    if j != concept_idx:\n",
    "                        segment_index += str(int(c_test[sample_idx, j]))\n",
    "                segment_index = int(segment_index, 2)\n",
    "                start_interval += segment_index * partition_size\n",
    "                end_interval = start_interval + partition_size\n",
    "            if \"fixed\" in mode:\n",
    "                soft_activations[sample_idx, concept_idx] = (start_interval + end_interval)/2\n",
    "            else:\n",
    "                soft_activations[sample_idx, concept_idx] = np.random.uniform(\n",
    "                    start_interval,\n",
    "                    end_interval\n",
    "                )\n",
    "    return soft_activations\n",
    "    \n",
    "def metric_comp_experiment_loop(experiment_config):\n",
    "    experiment_variables = dict(\n",
    "        sap_scores=[],\n",
    "        mig_scores=[],\n",
    "        dci_disentanglement_scores=[],\n",
    "        dci_completeness_scores=[],\n",
    "        dci_informativeness_scores=[],\n",
    "        r4_scores=[],\n",
    "        \n",
    "        mean_concept_acc_scores=[],\n",
    "        \n",
    "        ois_scores=[],\n",
    "        nis_scores=[],\n",
    "        purity_matrices=[],\n",
    "        oracle_matrices=[],\n",
    "    )\n",
    "    experiment_config[\"data_concepts\"] = experiment_config.get(\n",
    "        \"data_concepts\",\n",
    "        experiment_config[\"num_concepts\"],\n",
    "    )\n",
    "    \n",
    "    # Let's save our config here either way\n",
    "    serialize_experiment_config(\n",
    "        experiment_config,\n",
    "        experiment_config[\"results_dir\"],\n",
    "    )\n",
    "    \n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    cov = experiment_config[\"covariance\"]\n",
    "    for mode in experiment_config[\"soft_mode\"]:\n",
    "        saps = []\n",
    "        migs = []\n",
    "        dci_completenesss = []\n",
    "        dci_disentanglements = []\n",
    "        dci_informativenesss = []\n",
    "        r4s = []\n",
    "        \n",
    "        mean_concept_accs = []\n",
    "        concept_completenesss = []\n",
    "        \n",
    "        purity_mats = []\n",
    "        oracle_mats = []\n",
    "        oiss = []\n",
    "        niss = []\n",
    "        oracle_mat = None\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} with covariance {cov} and mode {mode}\")\n",
    "            # First construct the dataset\n",
    "            if experiment_config[\"data_concepts\"] > 3:\n",
    "                (x_test, y_test, c_test) = produce_data_larger(\n",
    "                    experiment_config[\"test_samples\"],\n",
    "                    cov=cov,\n",
    "                    num_concepts=experiment_config[\"data_concepts\"],\n",
    "                )\n",
    "            else:\n",
    "                (x_test, y_test, c_test) = produce_data(\n",
    "                    experiment_config[\"test_samples\"],\n",
    "                    cov=cov,\n",
    "                    num_concepts=experiment_config[\"data_concepts\"],\n",
    "                )\n",
    "\n",
    "            print(\"\\tComputing soft activations...\")\n",
    "            soft_acts = generate_soft_activations(\n",
    "                c_test,\n",
    "                mode=mode,\n",
    "                on_start_interval=experiment_config.get('on_start_interval', 0.5),\n",
    "                on_end_interval=experiment_config.get('on_end_interval', 1.0),\n",
    "                off_start_interval=experiment_config.get('off_start_interval', 0.0),\n",
    "                off_end_interval=experiment_config.get('off_end_interval', 0.5),\n",
    "            )\n",
    "\n",
    "\n",
    "            print(f\"\\t\\tComputing MIG...\")\n",
    "            mig = MIG(Z_true=c_test, Z_learned=soft_acts, bins=experiment_config.get('num_bins', 10))\n",
    "            migs.append(mig)\n",
    "            print(f\"\\t\\t\\tDone MIG = {mig:.4f}\")\n",
    "\n",
    "\n",
    "            print(f\"\\t\\tComputing SAP...\")\n",
    "            sap = compute_sap_on_fixed_data(\n",
    "                observations=soft_acts,\n",
    "                labels=c_test,\n",
    "                representation_function=lambda x: x,\n",
    "                batch_size=experiment_config.get('batch_size', 64),\n",
    "            )\n",
    "            saps.append(sap)\n",
    "            print(f\"\\t\\t\\tDone SAP = {sap:.4f}\")\n",
    "\n",
    "\n",
    "            print(f\"\\t\\tComputing DCI...\")\n",
    "            dci_disentanglement, dci_completeness, dci_informativeness = DCI(\n",
    "                gen_factors=c_test,\n",
    "                latents=soft_acts,\n",
    "            )\n",
    "            dci_disentanglements.append(dci_disentanglement)\n",
    "            print(f\"\\t\\t\\tDone DCI disentanglement = {dci_disentanglement:.4f}\")\n",
    "            dci_completenesss.append(dci_completeness)\n",
    "            print(f\"\\t\\t\\tDone DCI completeness = {dci_completeness:.4f}\")\n",
    "            dci_informativenesss.append(dci_informativeness)\n",
    "            print(f\"\\t\\t\\tDone DCI informativeness = {dci_informativeness:.4f}\")\n",
    "            \n",
    "            print(f\"\\t\\tComputing R4...\")\n",
    "            r4 = R4_scores(V=c_test, Z=soft_acts)\n",
    "            r4s.append(r4)\n",
    "            print(f\"\\t\\t\\tDone R4 = {r4:.4f}\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            print(f\"\\t\\tComputing mean concept accuracy...\")\n",
    "            mean_concept_acc = accuracy_score(\n",
    "                y_true=c_test,\n",
    "                y_pred=(soft_acts > 0.5).astype(np.int32),\n",
    "            )\n",
    "            mean_concept_accs.append(mean_concept_acc)\n",
    "            print(f\"\\t\\t\\tDone mean concept acc = {mean_concept_acc:.4f}\")\n",
    "            \n",
    "            \n",
    "            print(f\"\\t\\tComputing NIS...\")\n",
    "            nis = niching.niche_impurity_score(\n",
    "                c_soft=soft_acts,\n",
    "                c_true=c_test,\n",
    "                delta_beta=0.05,\n",
    "            )\n",
    "            niss.append(nis)\n",
    "            print(f\"\\t\\t\\tDone NIS = {nis:.4f}\")\n",
    "            \n",
    "            print(f\"\\t\\tComputing OIS...\")\n",
    "            ois, purity_mat, oracle_mat = oracle.oracle_impurity_score(\n",
    "                c_soft=soft_acts,\n",
    "                c_true=c_test,\n",
    "                output_matrices=True,\n",
    "                oracle_matrix=oracle_mat,\n",
    "            )\n",
    "            purity_mats.append(purity_mat)\n",
    "            oracle_mats.append(oracle_mat)\n",
    "            oiss.append(ois)\n",
    "            print(f\"\\t\\t\\tDone OIS = {ois:.4f}\")\n",
    "\n",
    "\n",
    "            print(\"\\t\\tDone with trial\", trial + 1)\n",
    "\n",
    "        purity_mats = np.stack(purity_mats, axis=0)\n",
    "        purity_mat_mean = np.mean(purity_mats, axis=0)\n",
    "        purity_mat_std = np.std(purity_mats, axis=0)\n",
    "        print(\"\\tPurity matrix:\")\n",
    "        for i in range(purity_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(purity_mat_mean.shape[1]):\n",
    "                line += f'{purity_mat_mean[i, j]:.4f} ± {purity_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"purity_matrices\"].append((purity_mat_mean, purity_mat_std, purity_mats))\n",
    "\n",
    "\n",
    "        oracle_mats = np.stack(oracle_mats, axis=0)\n",
    "        oracle_mat_mean = np.mean(oracle_mats, axis=0)\n",
    "        oracle_mat_std = np.std(oracle_mats, axis=0)\n",
    "        print(\"\\tOracle matrix:\")\n",
    "        for i in range(oracle_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(oracle_mat_mean.shape[1]):\n",
    "                line += f'{oracle_mat_mean[i, j]:.4f} ± {oracle_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"oracle_matrices\"].append((oracle_mat_mean, oracle_mat_std, oracle_mats))\n",
    "\n",
    "        sap_mean, sap_std = np.mean(saps), np.std(saps)\n",
    "        experiment_variables[\"sap_scores\"].append((sap_mean, sap_std, saps))\n",
    "        print(f\"\\tSAP score: {sap_mean:.4f} ± {sap_std:.4f}\")\n",
    "\n",
    "        mig_mean, mig_std = np.mean(migs), np.std(migs)\n",
    "        experiment_variables[\"mig_scores\"].append((mig_mean, mig_std, migs))\n",
    "        print(f\"\\tMIG score: {mig_mean:.4f} ± {mig_std:.4f}\")\n",
    "\n",
    "        dci_disentanglement_mean, dci_disentanglement_std = np.mean(dci_disentanglements), np.std(dci_disentanglements)\n",
    "        experiment_variables[\"dci_disentanglement_scores\"].append((dci_disentanglement_mean, dci_disentanglement_std, dci_disentanglements))\n",
    "        print(f\"\\tDCI disentanglement score: {dci_disentanglement_mean:.4f} ± {dci_disentanglement_std:.4f}\")\n",
    "\n",
    "        dci_completeness_mean, dci_completeness_std = np.mean(dci_completenesss), np.std(dci_completenesss)\n",
    "        experiment_variables[\"dci_completeness_scores\"].append((dci_completeness_mean, dci_completeness_std, dci_completenesss))\n",
    "        print(f\"\\tDCI completeness score: {dci_completeness_mean:.4f} ± {dci_completeness_std:.4f}\")\n",
    "\n",
    "        dci_informativeness_mean, dci_informativeness_std = np.mean(dci_informativenesss), np.std(dci_informativenesss)\n",
    "        experiment_variables[\"dci_informativeness_scores\"].append((dci_informativeness_mean, dci_informativeness_std, dci_informativenesss))\n",
    "        print(f\"\\tDCI informativeness score: {dci_informativeness_mean:.4f} ± {dci_informativeness_std:.4f}\")\n",
    "        \n",
    "        r4_mean, r4_std = np.mean(r4s), np.std(r4s)\n",
    "        experiment_variables[\"r4_scores\"].append((r4_mean, r4_std, r4s))\n",
    "        print(f\"\\tR4 score: {r4_mean:.4f} ± {r4_std:.4f}\")\n",
    "\n",
    "        \n",
    "        mean_concept_acc_mean, mean_concept_acc_std = np.mean(mean_concept_accs), np.std(mean_concept_accs)\n",
    "        experiment_variables[\"mean_concept_acc_scores\"].append((mean_concept_acc_mean, mean_concept_acc_std, mean_concept_accs))\n",
    "        print(f\"\\tMean concept accuracy score: {mean_concept_acc_mean:.4f} ± {mean_concept_acc_std:.4f}\")\n",
    "        \n",
    "        ois_mean, ois_std = np.mean(oiss), np.std(oiss)\n",
    "        experiment_variables[\"ois_scores\"].append((ois_mean, ois_std, oiss))\n",
    "        print(f\"\\tOIS score: {ois_mean:.4f} ± {ois_std:.4f}\")\n",
    "        \n",
    "        nis_mean, nis_std = np.mean(niss), np.std(niss)\n",
    "        experiment_variables[\"nis_scores\"].append((nis_mean, nis_std, niss))\n",
    "        print(f\"\\tNIS score: {nis_mean:.4f} ± {nis_std:.4f}\")\n",
    "        \n",
    "        # And serialize the results\n",
    "        serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "experiment_config = dict(\n",
    "    trials=5,\n",
    "    batch_size=128,\n",
    "    num_outputs=1,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        \"covariance_0.25\"\n",
    "    ),\n",
    "    input_shape=[7],\n",
    "    num_bins=20,\n",
    "    test_samples=3000,\n",
    "    covariance=0.25,\n",
    "    soft_mode=[\"localized\", \"random\"],\n",
    "    on_start_interval=0.95,\n",
    "    on_end_interval=1.0,\n",
    "    off_start_interval=0.0,\n",
    "    off_end_interval=0.05,\n",
    "    verbosity=0,\n",
    "    num_concepts=5,\n",
    "    data_concepts=5,\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "figure_dir = os.path.join(experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "results = metric_comp_experiment_loop(\n",
    "    experiment_config,\n",
    ")\n",
    "print(\"ois_scores:\", list(map(lambda x: x[:2], results[\"ois_scores\"])))\n",
    "print(\"nis_scores:\", list(map(lambda x: x[:2], results[\"nis_scores\"])))\n",
    "print(\"sap_scores:\", list(map(lambda x: x[:2], results[\"sap_scores\"])))\n",
    "print(\"mig_scores:\", list(map(lambda x: x[:2], results[\"mig_scores\"])))\n",
    "print(\"r4_scores:\", list(map(lambda x: x[:2], results[\"r4_scores\"])))\n",
    "print(\"dci_disentanglement_scores:\", list(map(lambda x: x[:2], results[\"dci_disentanglement_scores\"])))\n",
    "print(\"dci_completeness_scores:\", list(map(lambda x: x[:2], results[\"dci_completeness_scores\"])))\n",
    "print(\"dci_informativeness_scores:\", list(map(lambda x: x[:2], results[\"dci_informativeness_scores\"])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "import scipy\n",
    "\n",
    "metrics = [\n",
    "    (\"ois_scores\", \"OIS (ours)\"),\n",
    "    (\"nis_scores\", \"NIS (ours)\"),\n",
    "    (\"sap_scores\", \"SAP\"),\n",
    "    (\"mig_scores\", \"MIG\"),\n",
    "    (\"r4_scores\", \"R4\"),\n",
    "    (\"dci_disentanglement_scores\", \"DCI Disentanglement\"),\n",
    "    (\"dci_completeness_scores\", \"DCI Completeness\"),\n",
    "    (\"dci_informativeness_scores\", \"DCI Informativeness\"),\n",
    "]\n",
    "\n",
    "tab = PrettyTable()\n",
    "tab.field_names = [\"\"] + list(map(lambda x: x[1], metrics))\n",
    "\n",
    "tab.add_row([\"Baseline\"] + list(map(lambda x: f'{results[x[0]][-1][0] * 100:.2f}% ± {results[x[0]][-1][1] * 100:.2f}%', metrics)))\n",
    "p_vals = {}\n",
    "for method_name, _ in metrics:\n",
    "    vals_null = np.array(results[method_name][-1][2]) * 100\n",
    "    vals_hyp = np.array(results[method_name][0][2]) * 100\n",
    "    p_vals[method_name] = scipy.stats.ttest_ind(\n",
    "        vals_null,\n",
    "        vals_hyp,\n",
    "        equal_var=False,\n",
    "        alternative='two-sided'\n",
    "    )[1]\n",
    "tab.add_row([\"Impure\"] + list(map(lambda x: f'{results[x[0]][0][0] * 100:.2f}% ± {results[x[0]][0][1] * 100:.2f}% (p = {p_vals[x[0]]:.2e})', metrics)))\n",
    "print(tab)    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
