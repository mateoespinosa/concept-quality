{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dIcmOacShy-p",
    "outputId": "e47e6328-6477-4d52-d914-4dbf25f7fb6b"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xD3cPbsJhy-r"
   },
   "source": [
    "# Measuring Effects of Interventions in CBM for 3dshapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwclGfwnhy-s"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYPJQZB-hy-t"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import concepts_xai\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import rc\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import utils\n",
    "import model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktrTSqfchy-u"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "## Set seeds up for reproducibility\n",
    "################################################################################\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(87)\n",
    "tf.random.set_seed(87)\n",
    "np.random.seed(87)\n",
    "random.seed(87)\n",
    "NUM_TRIALS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "## Global Variables Defining Experiment Flow\n",
    "################################################################################\n",
    "\n",
    "_LATEX_SYMBOL = \"\"  # Change to \"$\" if working out of server\n",
    "USE_DSPRITES = True\n",
    "if USE_DSPRITES:\n",
    "    RESULTS_DIR = \"intervention_results/dsprites\"\n",
    "    DATASETS_DIR = os.path.join(\"results/dsprites\", \"datasets/\")\n",
    "else:\n",
    "    RESULTS_DIR = \"intervention_results/shapes3d\"\n",
    "    DATASETS_DIR = os.path.join(\"results/shapes3d\", \"datasets/\")\n",
    "    \n",
    "Path(DATASETS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "rc('text', usetex=(_LATEX_SYMBOL == \"$\"))\n",
    "plt.style.use('seaborn-whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRdZhyY9hy-v"
   },
   "source": [
    "## Dependency Tasks Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDLX8kTIhy-v"
   },
   "outputs": [],
   "source": [
    "if not USE_DSPRITES:\n",
    "    import tensorflow_datasets as tfds\n",
    "\n",
    "    shapes3d_train_ds = tfds.load('shapes3d', split='train', shuffle_files=True)\n",
    "else:\n",
    "    import concepts_xai.datasets.dSprites as dsprites\n",
    "    import concepts_xai.datasets.latentFactorData as latentFactorData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel_categorical(x):\n",
    "    if len(x.shape) == 1:\n",
    "        _, unique_inds = np.unique(\n",
    "            x,\n",
    "            return_inverse=True,\n",
    "        )\n",
    "        return unique_inds\n",
    "    \n",
    "    result = x[:, :]\n",
    "    for i in range(x.shape[-1]):\n",
    "        _, unique_inds = np.unique(\n",
    "            x[:, i],\n",
    "            return_inverse=True,\n",
    "        )\n",
    "        result[:, i] = unique_inds\n",
    "    return result\n",
    "\n",
    "def cardinality_encoding(card_group_1, card_group_2):\n",
    "    result_to_encoding = {}\n",
    "    for i in card_group_1:\n",
    "        for j in card_group_2:\n",
    "            result_to_encoding[(i, j)] = len(result_to_encoding)\n",
    "    return result_to_encoding\n",
    "\n",
    "def extract_data(\n",
    "    filter_fn,\n",
    "    sample_map_fn=lambda x: x,\n",
    "    concept_map_fn=lambda x: x,\n",
    "    step=1,\n",
    "    label_fn=lambda ex: ex['label_shape'] * 8 + ex['label_scale'],\n",
    "    dataset_path=None,\n",
    "    force_rerun=False,\n",
    "):\n",
    "    if (not force_rerun) and dataset_path and os.path.exists(dataset_path):\n",
    "        # Them time to load up this dataset!\n",
    "        ds = np.load(dataset_path)\n",
    "        return ds[\"X\"], ds[\"y\"], ds[\"c\"]\n",
    "    num_entries = len(shapes3d_train_ds)\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    c_train = []\n",
    "    for i, ex in enumerate(tfds.as_numpy(\n",
    "        shapes3d_train_ds.shuffle(buffer_size=15000)\n",
    "    )):\n",
    "        if i % step != 0:\n",
    "            continue\n",
    "        concepts = [\n",
    "            ex['label_floor_hue'],\n",
    "            ex['label_wall_hue'],\n",
    "            ex['label_object_hue'],\n",
    "            ex['label_scale'],\n",
    "            ex['label_shape'],\n",
    "            ex['label_orientation'],\n",
    "        ]\n",
    "        if not filter_fn(concepts):\n",
    "            continue\n",
    "        print(i, end=\"\\r\")\n",
    "\n",
    "        x_train.append(sample_map_fn(ex['image']))\n",
    "        y_train.append(label_fn(ex))\n",
    "        c_train.append(concept_map_fn(concepts))\n",
    "    x_train = np.stack(x_train, axis=0) / 255.0\n",
    "    y_train = relabel_categorical(np.stack(y_train, axis=0))\n",
    "    c_train = relabel_categorical(np.stack(c_train, axis=0))\n",
    "    if dataset_path:\n",
    "        # Then serialize it to speed up things next time\n",
    "        np.savez(\n",
    "            dataset_path,\n",
    "            X=x_train,\n",
    "            y=y_train,\n",
    "            c=c_train,\n",
    "        )\n",
    "    return x_train, y_train, c_train\n",
    "\n",
    "\n",
    "def generate_dsprites_dataset(\n",
    "    label_fn,\n",
    "    filter_fn=None,\n",
    "    dataset_path=None,\n",
    "    concept_map_fn=lambda x: x,\n",
    "    sample_map_fn=lambda x: x,\n",
    "    dsprites_path=\"dsprites/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz\",\n",
    "    force_reload=False,\n",
    "):\n",
    "    if (not force_reload) and dataset_path and os.path.exists(dataset_path):\n",
    "        # Them time to load up this dataset!\n",
    "        ds = np.load(dataset_path)\n",
    "        return (\n",
    "            (ds[\"x_train\"], ds[\"y_train\"], ds[\"c_train\"]),\n",
    "            (ds[\"x_test\"], ds[\"y_test\"], ds[\"c_test\"])\n",
    "        )\n",
    "    \n",
    "    def _task_fn(x_data, c_data):\n",
    "        return latentFactorData.get_task_data(\n",
    "            x_data=x_data,\n",
    "            c_data=c_data,\n",
    "            label_fn=label_fn,\n",
    "            filter_fn=filter_fn,\n",
    "        )\n",
    "\n",
    "    loaded_dataset = dsprites.dSprites(\n",
    "        dataset_path=dsprites_path,\n",
    "        train_size=0.8,\n",
    "        random_state=42,\n",
    "        task=_task_fn,\n",
    "    )\n",
    "    _, _, _ = loaded_dataset.load_data()\n",
    "\n",
    "    x_train = sample_map_fn(loaded_dataset.x_train)\n",
    "    y_train = loaded_dataset.y_train\n",
    "    c_train = concept_map_fn(loaded_dataset.c_train)\n",
    "    \n",
    "    x_test = sample_map_fn(loaded_dataset.x_test)\n",
    "    y_test = loaded_dataset.y_test\n",
    "    c_test = concept_map_fn(loaded_dataset.c_test)\n",
    "    \n",
    "    if dataset_path:\n",
    "        # Then serialize it to speed up things next time\n",
    "        np.savez(\n",
    "            dataset_path,\n",
    "            x_train=x_train,\n",
    "            y_train=y_train,\n",
    "            c_train=c_train,\n",
    "            x_test=x_test,\n",
    "            y_test=y_test,\n",
    "            c_test=c_test,\n",
    "        )\n",
    "    return (x_train, y_train, c_train), (x_test, y_test, c_test),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_DSPRITES:\n",
    "    ############################################################################\n",
    "    ## Construct a binary concept task in the shapes3D dataset \n",
    "    ############################################################################\n",
    "    def count_class_balance(y):\n",
    "        one_hot = tf.keras.utils.to_categorical(y)\n",
    "        return np.sum(one_hot, axis=0) / one_hot.shape[0]\n",
    "\n",
    "    def multiclass_task_bin_concepts_map_fn(concepts):\n",
    "        return [\n",
    "            int(concepts[0] < 5),\n",
    "            int(concepts[1] < 5),\n",
    "            int(concepts[2] < 5),\n",
    "            int(concepts[3] < 4),\n",
    "            int(concepts[4] < 2),\n",
    "            int(concepts[5] < 7),\n",
    "        ]\n",
    "\n",
    "\n",
    "    def multiclass_task_bin_concepts_label_fn(concept_dict):\n",
    "        concept_vector = multiclass_task_bin_concepts_map_fn([\n",
    "            concept_dict['label_floor_hue'],\n",
    "            concept_dict['label_wall_hue'],\n",
    "            concept_dict['label_object_hue'],\n",
    "            concept_dict['label_scale'],\n",
    "            concept_dict['label_shape'],\n",
    "            concept_dict['label_orientation'],\n",
    "        ])\n",
    "        binary_label_encoding = [\n",
    "            concept_vector[0] or concept_vector[1],\n",
    "            concept_vector[2] or concept_vector[3],\n",
    "            concept_vector[4] or concept_vector[5],\n",
    "        ]\n",
    "        return int(\n",
    "            \"\".join(list(map(str, binary_label_encoding))),\n",
    "            2\n",
    "        )\n",
    "\n",
    "    def filter_fn_dep_0(concept):\n",
    "        ranges = [\n",
    "            list(range(0, 10, 2)),\n",
    "            list(range(0, 10, 2)),\n",
    "            list(range(0, 10, 2)),\n",
    "            list(range(0, 8)),\n",
    "            list(range(4)),\n",
    "            list(range(0, 15, 4)),\n",
    "        ]\n",
    "\n",
    "        concept_vector = multiclass_task_bin_concepts_map_fn(concept)\n",
    "        # First filter as in small dataset to constraint the size of the data a bit\n",
    "        return all([\n",
    "            (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "    floor_hue_wall_hue_sets_lower = [\n",
    "        list(np.random.permutation(7))[:5]\n",
    "        for _ in range(10)\n",
    "    ]\n",
    "\n",
    "    floor_hue_wall_hue_sets_upper = [\n",
    "        list(3 + np.random.permutation(7))[:5]\n",
    "        for _ in range(10)\n",
    "    ]\n",
    "\n",
    "    def filter_fn_dep_1(concept):\n",
    "        ranges = [\n",
    "            list(range(0, 10)),\n",
    "            list(range(0, 10, 2)),\n",
    "            list(range(0, 10, 2)),\n",
    "            list(range(0, 8)),\n",
    "            list(range(4)),\n",
    "            list(range(0, 15, 4)),\n",
    "        ]\n",
    "\n",
    "        concept_vector = multiclass_task_bin_concepts_map_fn(concept)\n",
    "\n",
    "        # First filter as in small dataset to constraint the size of the data a bit\n",
    "        if not all([\n",
    "            (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "        ]):\n",
    "            return False\n",
    "        if concept[0] < 5:\n",
    "            return concept[1] in floor_hue_wall_hue_sets_lower[concept[0]]\n",
    "        else:\n",
    "            return (concept[1] in floor_hue_wall_hue_sets_upper[concept[0]])\n",
    "\n",
    "\n",
    "        wall_hue_object_hue_sets_lower = [\n",
    "        list(np.random.permutation(7))[:5]\n",
    "        for _ in range(10)\n",
    "    ]\n",
    "    wall_hue_object_hue_sets_upper = [\n",
    "        list(3 + np.random.permutation(7))[:5]\n",
    "        for _ in range(10)\n",
    "    ]\n",
    "\n",
    "    def filter_fn_dep_2(concept):\n",
    "        ranges = [\n",
    "            list(range(0, 10)),\n",
    "            list(range(0, 10)),\n",
    "            list(range(0, 10, 2)),\n",
    "            list(range(0, 8)),\n",
    "            list(range(4)),\n",
    "            list(range(0, 15, 4)),\n",
    "        ]\n",
    "\n",
    "        concept_vector = multiclass_task_bin_concepts_map_fn(concept)\n",
    "        # First filter as in small dataset to constraint the size of the data a bit\n",
    "        if not all([\n",
    "            (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "        ]):\n",
    "            return False\n",
    "\n",
    "        if (concept[0] < 5):\n",
    "            if concept[1] not in floor_hue_wall_hue_sets_lower[concept[0]]:\n",
    "                return False\n",
    "        else:\n",
    "            if (concept[1] not in floor_hue_wall_hue_sets_upper[concept[0]]):\n",
    "                return False\n",
    "\n",
    "        if (concept[1] < 5):\n",
    "            if concept[2] not in wall_hue_object_hue_sets_lower[concept[1]]:\n",
    "                return False\n",
    "        else:\n",
    "            if (concept[2] not in wall_hue_object_hue_sets_upper[concept[1]]):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "        object_hue_scale_sets_lower = [\n",
    "        list(np.random.permutation(6))[:4]\n",
    "        for _ in range(10)\n",
    "    ]\n",
    "\n",
    "    object_hue_scale_sets_upper = [\n",
    "        list(2 + np.random.permutation(6))[:4]\n",
    "        for _ in range(10)\n",
    "    ]\n",
    "\n",
    "    def filter_fn_dep_3(concept):\n",
    "        ranges = [\n",
    "            list(range(0, 10)),\n",
    "            list(range(0, 10)),\n",
    "            list(range(0, 10)),\n",
    "            list(range(0, 8)),\n",
    "            list(range(4)),\n",
    "            list(range(0, 15, 4)),\n",
    "        ]\n",
    "\n",
    "        concept_vector = multiclass_task_bin_concepts_map_fn(concept)\n",
    "\n",
    "        # First filter as in small dataset to constraint the size of the data a bit\n",
    "        if not all([\n",
    "            (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "        ]):\n",
    "            return False\n",
    "\n",
    "        if (concept[0] < 5):\n",
    "            if concept[1] not in floor_hue_wall_hue_sets_lower[concept[0]]:\n",
    "                return False\n",
    "        else:\n",
    "            if (concept[1] not in floor_hue_wall_hue_sets_upper[concept[0]]):\n",
    "                return False\n",
    "\n",
    "        if (concept[1] < 5):\n",
    "            if concept[2] not in wall_hue_object_hue_sets_lower[concept[1]]:\n",
    "                return False\n",
    "        else:\n",
    "            if (concept[2] not in wall_hue_object_hue_sets_upper[concept[1]]):\n",
    "                return False\n",
    "\n",
    "        if (concept[2] < 5):\n",
    "            if concept[3] not in object_hue_scale_sets_lower[concept[2]]:\n",
    "                return False\n",
    "        else:\n",
    "            if (concept[3] not in object_hue_scale_sets_upper[concept[2]]):\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "    scale_shape_sets_lower = [\n",
    "        list(np.random.permutation(3))[:2]\n",
    "        for _ in range(8)\n",
    "    ]\n",
    "\n",
    "    scale_shape_sets_upper = [\n",
    "        list(1 + np.random.permutation(3))[:2]\n",
    "        for _ in range(8)\n",
    "    ]\n",
    "\n",
    "    def filter_fn_dep_4(concept):\n",
    "        ranges = [\n",
    "            list(range(0, 10)),\n",
    "            list(range(0, 10)),\n",
    "            list(range(0, 10)),\n",
    "            list(range(0, 8)),\n",
    "            list(range(4)),\n",
    "            list(range(0, 15, 2)),\n",
    "        ]\n",
    "\n",
    "        concept_vector = multiclass_task_bin_concepts_map_fn(concept)\n",
    "\n",
    "        # First filter as in small dataset to constraint the size of the data a bit\n",
    "        if not all([\n",
    "            (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "        ]):\n",
    "            return False\n",
    "\n",
    "        if (concept[0] < 5):\n",
    "            if concept[1] not in floor_hue_wall_hue_sets_lower[concept[0]]:\n",
    "                return False\n",
    "        else:\n",
    "            if (concept[1] not in floor_hue_wall_hue_sets_upper[concept[0]]):\n",
    "                return False\n",
    "\n",
    "        if (concept[1] < 5):\n",
    "            if concept[2] not in wall_hue_object_hue_sets_lower[concept[1]]:\n",
    "                return False\n",
    "        else:\n",
    "            if (concept[2] not in wall_hue_object_hue_sets_upper[concept[1]]):\n",
    "                return False\n",
    "\n",
    "        if (concept[2] < 5):\n",
    "            if concept[3] not in object_hue_scale_sets_lower[concept[2]]:\n",
    "                return False\n",
    "        else:\n",
    "            if (concept[3] not in object_hue_scale_sets_upper[concept[2]]):\n",
    "                return False\n",
    "\n",
    "        if (concept[3] < 4):\n",
    "            if concept[4] not in scale_shape_sets_lower[concept[3]]:\n",
    "                return False\n",
    "        else:\n",
    "            if (concept[4] not in scale_shape_sets_upper[concept[3]]):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "    shape_rotation_sets_lower = [\n",
    "        list(np.random.permutation(9))[:7]\n",
    "        for _ in range(4)\n",
    "    ]\n",
    "\n",
    "    shape_rotation_sets_upper = [\n",
    "        list(6 + np.random.permutation(9))[:7]\n",
    "        for _ in range(4)\n",
    "    ]\n",
    "\n",
    "    def filter_fn_dep_5(concept):\n",
    "        ranges = [\n",
    "            list(range(0, 10)),\n",
    "            list(range(0, 10)),\n",
    "            list(range(0, 10)),\n",
    "            list(range(0, 8)),\n",
    "            list(range(4)),\n",
    "            list(range(0, 15)),\n",
    "        ]\n",
    "\n",
    "        concept_vector = multiclass_task_bin_concepts_map_fn(concept)\n",
    "\n",
    "        # First filter as in small dataset to constraint the size of the data a bit\n",
    "        if not all([\n",
    "            (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "        ]):\n",
    "            return False\n",
    "\n",
    "        if (concept[0] < 5):\n",
    "            if concept[1] not in floor_hue_wall_hue_sets_lower[concept[0]]:\n",
    "                return False\n",
    "        else:\n",
    "            if (concept[1] not in floor_hue_wall_hue_sets_upper[concept[0]]):\n",
    "                return False\n",
    "\n",
    "        if (concept[1] < 5):\n",
    "            if concept[2] not in wall_hue_object_hue_sets_lower[concept[1]]:\n",
    "                return False\n",
    "        else:\n",
    "            if (concept[2] not in wall_hue_object_hue_sets_upper[concept[1]]):\n",
    "                return False\n",
    "\n",
    "        if (concept[2] < 5):\n",
    "            if concept[3] not in object_hue_scale_sets_lower[concept[2]]:\n",
    "                return False\n",
    "        else:\n",
    "            if (concept[3] not in object_hue_scale_sets_upper[concept[2]]):\n",
    "                return False\n",
    "\n",
    "        if (concept[3] < 4):\n",
    "            if concept[4] not in scale_shape_sets_lower[concept[3]]:\n",
    "                return False\n",
    "        else:\n",
    "            if (concept[4] not in scale_shape_sets_upper[concept[3]]):\n",
    "                return False\n",
    "\n",
    "        if (concept[4] < 2):\n",
    "            if concept[5] not in shape_rotation_sets_lower[concept[4]]:\n",
    "                return False\n",
    "        else:\n",
    "            if (concept[5] not in shape_rotation_sets_upper[concept[4]]):\n",
    "                return False\n",
    "        return True\n",
    "else:\n",
    "    def count_class_balance(y):\n",
    "        one_hot = tf.keras.utils.to_categorical(y)\n",
    "        return np.sum(one_hot, axis=0) / one_hot.shape[0]\n",
    "\n",
    "    def multiclass_binary_concepts_map_fn(concepts):\n",
    "        new_concepts = np.zeros((concepts.shape[0], 5))\n",
    "        # We will have 5 concepts:\n",
    "        # (0) \"is it ellipse or square?\"\n",
    "        new_concepts[:, 0] = (concepts[:, 0] < 2).astype(np.int)\n",
    "\n",
    "        # (1) \"is_size < 3?\"\n",
    "        num_sizes = len(set(concepts[:, 1]))\n",
    "        new_concepts[:, 1] = (concepts[:, 1] < num_sizes/2).astype(np.int)\n",
    "\n",
    "        # (2) \"is rotation < PI/2?\"\n",
    "        num_rots = len(set(concepts[:, 2]))\n",
    "        new_concepts[:, 2] = (concepts[:, 2] < num_rots/2).astype(np.int)\n",
    "\n",
    "        # (3) \"is x <= 16?\"\n",
    "        num_x_coords = len(set(concepts[:, 3]))\n",
    "        new_concepts[:, 3] = (concepts[:, 3] < num_x_coords // 2).astype(np.int)\n",
    "\n",
    "        # (4) \"is y <= 16?\"\n",
    "        num_y_coords = len(set(concepts[:, 4]))\n",
    "        new_concepts[:, 4] = (concepts[:, 4] < num_y_coords // 2).astype(np.int)\n",
    "\n",
    "        return new_concepts\n",
    "\n",
    "    def _get_concept_vector(c_data):\n",
    "        return np.array([\n",
    "            # First check if it is an ellipse or a square\n",
    "            int(c_data[0] < 2),\n",
    "            # Now check that it is \"small\"\n",
    "            int(c_data[1] < 3),\n",
    "            # And it has not been rotated more than PI/2 radians\n",
    "            int(c_data[2] < 20),\n",
    "            # Finally, check whether it is in not in the the upper-left quadrant\n",
    "            int(c_data[3] < 15),\n",
    "            int(c_data[4] < 15),\n",
    "        ])\n",
    "\n",
    "    def multiclass_task_label_fn(c_data):\n",
    "        # Our task will be a binary task where we are interested in determining\n",
    "        # whether an image is a \"small\" ellipse not in the upper-left\n",
    "        # quadrant that has been rotated less than 3*PI/2 radians\n",
    "        concept_vector = _get_concept_vector(c_data)\n",
    "        binary_label_encoding = [\n",
    "            concept_vector[0] or concept_vector[1],\n",
    "            concept_vector[2] or concept_vector[3],\n",
    "            concept_vector[4],\n",
    "        ]\n",
    "        return int(\n",
    "            \"\".join(list(map(str, binary_label_encoding))),\n",
    "            2\n",
    "        )\n",
    "\n",
    "    def dep_0_filter_fn(concept):\n",
    "        ranges = [\n",
    "            list(range(3)),\n",
    "            list(range(0, 6, 2)),\n",
    "            list(range(0, 40, 4)),\n",
    "            list(range(0, 32, 2)),\n",
    "            list(range(0, 32, 2)),\n",
    "        ]\n",
    "        return all([\n",
    "            (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "    scale_shape_sets_lower = [\n",
    "        list(np.random.permutation(4))[:3] for i in range(3)\n",
    "    ]\n",
    "\n",
    "    scale_shape_sets_upper = [\n",
    "        list(2 + np.random.permutation(4))[:3] for i in range(3)\n",
    "    ]\n",
    "    def dep_1_filter_fn(concept):\n",
    "        ranges = [\n",
    "            list(range(3)),\n",
    "            list(range(6)),\n",
    "            list(range(0, 40, 4)),\n",
    "            list(range(0, 32, 2)),\n",
    "            list(range(0, 32, 2)),\n",
    "        ]\n",
    "\n",
    "        concept_vector = _get_concept_vector(concept)\n",
    "\n",
    "        # First filter as in small dataset to constraint the size of the data a bit\n",
    "        if not all([\n",
    "            (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "        ]):\n",
    "            return False\n",
    "        if concept_vector[0]:\n",
    "            if concept[1] not in scale_shape_sets_lower[concept[0]]:\n",
    "                return False\n",
    "        else:\n",
    "            if concept[0] not in scale_shape_sets_upper[concept[0]]:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "    rotation_scale_sets_lower = [\n",
    "        list(np.random.permutation(30))[:20] for i in range(6)\n",
    "    ]\n",
    "\n",
    "    rotation_scale_sets_upper = [\n",
    "        list(10 + np.random.permutation(30))[:20] for i in range(6)\n",
    "    ]\n",
    "    def dep_2_filter_fn(concept):\n",
    "        ranges = [\n",
    "            list(range(3)),\n",
    "            list(range(6)),\n",
    "            list(range(0, 40, 2)),\n",
    "            list(range(0, 32, 2)),\n",
    "            list(range(0, 32, 2)),\n",
    "        ]\n",
    "\n",
    "        concept_vector = _get_concept_vector(concept)\n",
    "\n",
    "        # First filter as in small dataset to constraint the size of the data a bit\n",
    "        if not all([\n",
    "            (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "        ]):\n",
    "            return False\n",
    "        if concept_vector[0]:\n",
    "            if concept[1] not in scale_shape_sets_lower[concept[0]]:\n",
    "                return False\n",
    "        else:\n",
    "            if concept[0] not in scale_shape_sets_upper[concept[0]]:\n",
    "                return False\n",
    "\n",
    "        if concept_vector[1]:\n",
    "            if concept[2] not in rotation_scale_sets_lower[concept[1]]:\n",
    "                return False\n",
    "        else:\n",
    "            if concept[2] not in rotation_scale_sets_upper[concept[1]]:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "    x_pos_rotation_sets_lower = [\n",
    "        list(np.random.permutation(20))[:16]\n",
    "        for i in range(40)\n",
    "    ]\n",
    "\n",
    "    x_pos_rotation_sets_upper = [\n",
    "        list(12 + np.random.permutation(20))[:16]\n",
    "        for i in range(40)\n",
    "    ]\n",
    "    def dep_3_filter_fn(concept):\n",
    "        ranges = [\n",
    "            list(range(3)),\n",
    "            list(range(6)),\n",
    "            list(range(0, 40, 2)),\n",
    "            list(range(0, 32)),\n",
    "            list(range(0, 32, 2)),\n",
    "        ]\n",
    "\n",
    "        concept_vector = _get_concept_vector(concept)\n",
    "\n",
    "        # First filter as in small dataset to constraint the size of the data a bit\n",
    "        if not all([\n",
    "            (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "        ]):\n",
    "            return False\n",
    "        if concept_vector[0]:\n",
    "            if concept[1] not in scale_shape_sets_lower[concept[0]]:\n",
    "                return False\n",
    "        else:\n",
    "            if concept[0] not in scale_shape_sets_upper[concept[0]]:\n",
    "                return False\n",
    "\n",
    "        if concept_vector[1]:\n",
    "            if concept[2] not in rotation_scale_sets_lower[concept[1]]:\n",
    "                return False\n",
    "        else:\n",
    "            if concept[2] not in rotation_scale_sets_upper[concept[1]]:\n",
    "                return False\n",
    "\n",
    "        if concept_vector[2]:\n",
    "            if concept[3] not in x_pos_rotation_sets_lower[concept[2]]:\n",
    "                return False\n",
    "        else:\n",
    "            if concept[3] not in x_pos_rotation_sets_upper[concept[2]]:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    y_pos_x_pos_sets_lower = [\n",
    "        list(np.random.permutation(20))[:16]\n",
    "        for i in range(32)\n",
    "    ]\n",
    "\n",
    "    y_pos_x_pos_sets_upper = [\n",
    "        list(12 + np.random.permutation(20))[:16]\n",
    "        for i in range(32)\n",
    "    ]\n",
    "    def dep_4_filter_fn(concept):\n",
    "        ranges = [\n",
    "            list(range(3)),\n",
    "            list(range(6)),\n",
    "            list(range(0, 40, 2)),\n",
    "            list(range(0, 32)),\n",
    "            list(range(0, 32)),\n",
    "        ]\n",
    "\n",
    "        concept_vector = _get_concept_vector(concept)\n",
    "\n",
    "        # First filter as in small dataset to constraint the size of the data a bit\n",
    "        if not all([\n",
    "            (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "        ]):\n",
    "            return False\n",
    "        if concept_vector[0]:\n",
    "            if concept[1] not in scale_shape_sets_lower[concept[0]]:\n",
    "                return False\n",
    "        else:\n",
    "            if concept[0] not in scale_shape_sets_upper[concept[0]]:\n",
    "                return False\n",
    "\n",
    "        if concept_vector[1]:\n",
    "            if concept[2] not in rotation_scale_sets_lower[concept[1]]:\n",
    "                return False\n",
    "        else:\n",
    "            if concept[2] not in rotation_scale_sets_upper[concept[1]]:\n",
    "                return False\n",
    "\n",
    "        if concept_vector[2]:\n",
    "            if concept[3] not in x_pos_rotation_sets_lower[concept[2]]:\n",
    "                return False\n",
    "        else:\n",
    "            if concept[3] not in x_pos_rotation_sets_upper[concept[2]]:\n",
    "                return False\n",
    "\n",
    "        if concept_vector[3]:\n",
    "            if concept[4] not in y_pos_x_pos_sets_lower[concept[3]]:\n",
    "                return False\n",
    "        else:\n",
    "            if concept[4] not in y_pos_x_pos_sets_upper[concept[3]]:\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_FORCE_RERUN = False\n",
    "if not USE_DSPRITES:\n",
    "    _FORCE_RERUN = False\n",
    "if not USE_DSPRITES:\n",
    "    def balanced_multiclass_task_bin_concepts_label_fn(concept_dict):\n",
    "        concept_vector = multiclass_task_bin_concepts_map_fn([\n",
    "            concept_dict['label_floor_hue'],\n",
    "            concept_dict['label_wall_hue'],\n",
    "            concept_dict['label_object_hue'],\n",
    "            concept_dict['label_scale'],\n",
    "            concept_dict['label_shape'],\n",
    "            concept_dict['label_orientation'],\n",
    "        ])\n",
    "        if concept_vector[4] == 0:\n",
    "            offset = 0\n",
    "            binary_label_encoding = [\n",
    "                concept_vector[0],\n",
    "                concept_vector[1],\n",
    "            ]\n",
    "        else:\n",
    "            offset = 4\n",
    "            binary_label_encoding = [\n",
    "                concept_vector[2],\n",
    "                concept_vector[3],\n",
    "                concept_vector[5],\n",
    "            ]\n",
    "\n",
    "        return offset + int(\n",
    "            \"\".join(list(map(str, binary_label_encoding))),\n",
    "            2\n",
    "        )\n",
    "\n",
    "    data_x_train, data_y_train, data_c_train = extract_data(\n",
    "        filter_fn_dep_0,\n",
    "        dataset_path=os.path.join(DATASETS_DIR, \"balanced_multiclass_task_bin_concepts_dep_0_concepts.npz\"),\n",
    "        concept_map_fn=multiclass_task_bin_concepts_map_fn,\n",
    "        label_fn=balanced_multiclass_task_bin_concepts_label_fn,\n",
    "        force_rerun=_FORCE_RERUN,\n",
    "    )\n",
    "\n",
    "    data_x_train, data_x_test, data_y_train, data_y_test, data_c_train, data_c_test = train_test_split(\n",
    "        data_x_train,\n",
    "        data_y_train,\n",
    "        data_c_train,\n",
    "        test_size=0.2,\n",
    "    )\n",
    "\n",
    "    print(\"data_x_train.shape =\", data_x_train.shape)\n",
    "    print(\"data_x_train label distribution:\", count_class_balance(data_y_train))\n",
    "    print(\"data_x_train concept distribution:\", np.sum(data_c_train, axis=0)/data_c_train.shape[0])\n",
    "    print(\"data_x_train exclusive concept distribution:\", np.sum(data_c_train[np.sum(data_c_train, axis=-1) == 1], axis=0)/data_c_train.shape[0])\n",
    "    print(\"data_x_test.shape =\", data_x_test.shape)\n",
    "    print(\"data_c_test.shape =\", data_c_test.shape)\n",
    "    data_train = (data_x_train, data_y_train, data_c_train)\n",
    "    data_test = (data_x_test, data_y_test, data_c_test)\n",
    "\n",
    "    dep_0_corr_mat = np.ones(\n",
    "        (\n",
    "            data_train[2].shape[-1],\n",
    "            len(set(data_train[1]))\n",
    "        )\n",
    "    )\n",
    "    for c in range(dep_0_corr_mat.shape[0]):\n",
    "        for l in range(dep_0_corr_mat.shape[1]):\n",
    "            dep_0_corr_mat[c][l] = np.corrcoef(\n",
    "                data_train[2][:, c],\n",
    "                (data_train[1] == l).astype(np.int32),\n",
    "            )[0, 1]\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "    im, cbar = utils.heatmap(\n",
    "        np.abs(dep_0_corr_mat),\n",
    "        [f\"$c_{i}$\" for i in range(dep_0_corr_mat.shape[0])],\n",
    "        [f\"$l_{i}$\" for i in range(dep_0_corr_mat.shape[1])],\n",
    "        ax=ax,\n",
    "        cmap=\"magma\",\n",
    "        cbarlabel=f\"Correlation Coef\",\n",
    "        vmin=0, #-1,\n",
    "        vmax=1,\n",
    "    )\n",
    "    texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.suptitle(f\"3dshapes Concept-Label Absolute Correlations ($\\lambda = 0$)\", fontsize=25)\n",
    "    fig.subplots_adjust(top=0.85)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\")\n",
    "else:\n",
    "    def balanced_multiclass_task_label_fn(c_data):\n",
    "        # Our task will be a binary task where we are interested in determining\n",
    "        # whether an image is a \"small\" ellipse not in the upper-left\n",
    "        # quadrant that has been rotated less than 3*PI/2 radians\n",
    "        concept_vector = _get_concept_vector(c_data)\n",
    "        threshold = 0\n",
    "        if concept_vector[0] == 1:\n",
    "            binary_label_encoding = [\n",
    "                concept_vector[1],\n",
    "                concept_vector[2],\n",
    "            ]\n",
    "        else:\n",
    "            threshold = 4\n",
    "            binary_label_encoding = [\n",
    "                concept_vector[3],\n",
    "                concept_vector[4],\n",
    "            ]\n",
    "        return threshold + int(\n",
    "            \"\".join(list(map(str, binary_label_encoding))),\n",
    "            2\n",
    "        )\n",
    "\n",
    "    data_train, data_test = generate_dsprites_dataset(\n",
    "        label_fn=balanced_multiclass_task_label_fn,\n",
    "        filter_fn=dep_0_filter_fn,\n",
    "        dataset_path=os.path.join(DATASETS_DIR, \"balanced_multiclass_task_bin_concepts_dep_0_complete_dataset.npz\"),\n",
    "        dsprites_path=\"dsprites/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz\",\n",
    "        concept_map_fn=multiclass_binary_concepts_map_fn,\n",
    "    )\n",
    "\n",
    "    dep_0_corr_mat = np.ones(\n",
    "        (\n",
    "            data_train[2].shape[-1],\n",
    "            len(set(data_train[1]))\n",
    "        )\n",
    "    )\n",
    "    for c in range(dep_0_corr_mat.shape[0]):\n",
    "        for l in range(dep_0_corr_mat.shape[1]):\n",
    "            dep_0_corr_mat[c][l] = np.corrcoef(\n",
    "                data_train[2][:, c],\n",
    "                (data_train[1] == l).astype(np.int32),\n",
    "            )[0, 1]\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "    im, cbar = utils.heatmap(\n",
    "        np.abs(dep_0_corr_mat),\n",
    "        [f\"$c_{i}$\" for i in range(dep_0_corr_mat.shape[0])],\n",
    "        [f\"$l_{i}$\" for i in range(dep_0_corr_mat.shape[1])],\n",
    "        ax=ax,\n",
    "        cmap=\"magma\",\n",
    "        cbarlabel=f\"Correlation Coef\",\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "    )\n",
    "    texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.suptitle(f\"dSprites Concept-Label Absolute Correlations ($\\lambda = 0$)\", fontsize=25)\n",
    "    fig.subplots_adjust(top=0.85)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"data_dataset train size:\", data_train[0].shape[0])\n",
    "    print(\"data_dataset train concept size:\", data_train[2].shape)\n",
    "    print(\"\\tTrain balance:\", count_class_balance(data_train[1]))\n",
    "    print(\"\\tConcept balance:\", np.sum(data_train[2], axis=0)/data_train[2].shape[0])\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wciMdtP9hy-1"
   },
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTjkp5B3hy-6",
    "outputId": "35f4b4fe-7295-4747-a9d4-c52f4c75dab3"
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## Build CBM\n",
    "############################################################################\n",
    "import concepts_xai.methods.CBM.CBModel as CBM\n",
    "\n",
    "def construct_cbm(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    num_outputs,\n",
    "    alpha=0.1,\n",
    "    learning_rate=1e-3,\n",
    "    latent_dims=0,\n",
    "    encoder_output_logits=False,\n",
    "):\n",
    "    model_factory = CBM.BypassJointCBM if latent_dims else CBM.JointConceptBottleneckModel\n",
    "    cbm_model = model_factory(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        task_loss=(\n",
    "            tf.keras.losses.BinaryCrossentropy(from_logits=True) if (num_outputs <= 2)\n",
    "            else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        ),\n",
    "        name=\"joint_cbm\",\n",
    "        metrics=[\n",
    "            tf.keras.metrics.BinaryAccuracy() if (num_outputs <= 2)\n",
    "            else tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "        ],\n",
    "        alpha=alpha,\n",
    "        pass_concept_logits=encoder_output_logits,\n",
    "    )\n",
    "\n",
    "    ############################################################################\n",
    "    ## Compile CBM Model\n",
    "    ############################################################################\n",
    "\n",
    "    cbm_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    )\n",
    "    return cbm_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import joblib\n",
    "import concepts_xai.evaluation.metrics.oracle as oracle\n",
    "import concepts_xai.evaluation.metrics.niching as niching\n",
    "\n",
    "def construct_trivial_auc_mat(num_concepts):\n",
    "    result = np.ones((num_concepts, num_concepts), dtype=np.float32) * 0.5\n",
    "    return result + np.eye(num_concepts, dtype=np.float32) * 0.5\n",
    "    \n",
    "    \n",
    "def cbm_intervention(\n",
    "    cbm_model,\n",
    "    x_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    c_test,\n",
    "    intervention_concepts,\n",
    "):\n",
    "    # matrix mapping (concept, label) to the value we should set it\n",
    "    # to during an intervention\n",
    "    n_concepts = c_test.shape[-1]\n",
    "    intervention_values = np.zeros((n_concepts, 2))\n",
    "    if cbm_model.pass_concept_logits:\n",
    "        # Then the token value we will use for intervening\n",
    "        # will be based on the percentile of values in the\n",
    "        # training data\n",
    "        c_train_pred = cbm_model.encoder(x_train)\n",
    "        for i in range(n_concepts):\n",
    "            low_percentile, high_percentile = np.percentile(c_train_pred[:, i], [5, 95])\n",
    "            intervention_values[i, 0] = low_percentile\n",
    "            intervention_values[i, 1] = high_percentile\n",
    "    else:\n",
    "        # Else we use hard thresholds of 0 and 1 for everything as we are using\n",
    "        # real probabilities\n",
    "        intervention_values[:, 1] = 1.0\n",
    "    \n",
    "    # Now time to make our concept predictions\n",
    "    c_test_pred = cbm_model.encoder(x_test).numpy()\n",
    "    for int_concept in intervention_concepts:\n",
    "        c_test_pred[:, int_concept] = (\n",
    "            intervention_values[int_concept, 0] * (c_test[:, int_concept] == 0) +\n",
    "            intervention_values[int_concept, 1] * (c_test[:, int_concept] == 1)\n",
    "        )\n",
    "    \n",
    "    # Make the actual task predictions\n",
    "    # Extend c_test_pred so that it has its complement probability as well\n",
    "    y_test_preds = cbm_model.predict_from_concepts(c_test_pred).numpy()\n",
    "    y_test_preds = scipy.special.softmax(y_test_preds, axis=-1)\n",
    "    \n",
    "    # Time to evaluate these predictions\n",
    "    return {\n",
    "        \"accuracy\": sklearn.metrics.accuracy_score(\n",
    "            y_test,\n",
    "            np.argmax(y_test_preds, axis=-1),\n",
    "        ),\n",
    "        \"auc\": sklearn.metrics.roc_auc_score(\n",
    "            tf.keras.utils.to_categorical(y_test),\n",
    "            y_test_preds,\n",
    "            multi_class='ovo',\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    \n",
    "def cbm_mixed_capacity_intervention_experiment_loop(\n",
    "    train_data,\n",
    "    test_data,\n",
    "    experiment_config,\n",
    "    load_from_cache=False,\n",
    "):\n",
    "    experiment_variables = dict(\n",
    "        task_accuracies=[],\n",
    "        task_aucs=[],\n",
    "        concept_accuracies=[],\n",
    "        oiss=[],\n",
    "        purity_matrices=[],\n",
    "        oracle_matrices=[],\n",
    "        \n",
    "        intervention_task_accuracies=[],\n",
    "        intervention_task_aucs=[],\n",
    "\n",
    "        niss=[],\n",
    "        \n",
    "        current_experiment_idx=0,\n",
    "    )\n",
    "    experiment_config[\"data_concepts\"] = experiment_config.get(\n",
    "        \"data_concepts\",\n",
    "        experiment_config[\"num_concepts\"],\n",
    "    )\n",
    "    \n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    x_train, y_train, c_train = train_data\n",
    "    x_test, y_test, c_test = test_data\n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = os.path.exists(\n",
    "            os.path.join(experiment_config['results_dir'], 'results.joblib')\n",
    "        )\n",
    "        if cached:\n",
    "            results = joblib.load(\n",
    "                os.path.join(experiment_config['results_dir'], 'results.joblib')\n",
    "            )\n",
    "            start_ind = results['current_experiment_idx']\n",
    "            complete_cache = True\n",
    "            for varname in experiment_variables.keys():\n",
    "                if varname == 'current_experiment_idx':\n",
    "                    continue\n",
    "                if (varname not in results):\n",
    "                    complete_cache = False\n",
    "                    continue\n",
    "                if len(results[varname]) != start_ind:\n",
    "                    complete_cache = False\n",
    "                    start_ind = min(len(results[varname]), start_ind)\n",
    "            if complete_cache:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    utils.serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return results\n",
    "            else:\n",
    "                # Else we have a partial initialization so let's use it\n",
    "                experiment_variables = results\n",
    "    \n",
    "    # Let's save our config here either way\n",
    "    utils.serialize_experiment_config(\n",
    "        experiment_config,\n",
    "        experiment_config[\"results_dir\"],\n",
    "    )\n",
    "    \n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    for units in experiment_config[\"model_units\"][start_ind:]:\n",
    "        print(\"Training with units:\", [units, units//2])\n",
    "        task_accs = []\n",
    "        concept_accs = []\n",
    "        aucs = []\n",
    "        purity_mats = []\n",
    "        oracle_mats = []\n",
    "        oiss = []\n",
    "        non_oracle_purities = []\n",
    "        niss = []\n",
    "        \n",
    "        intervention_task_accuracies = []\n",
    "        intervention_task_aucs = []\n",
    "        \n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']}\")\n",
    "            # First proceed to do an end-to-end model in case we want to\n",
    "            # do some task-specific pretraining\n",
    "            encoder_units = (\n",
    "                [units, units//2] if experiment_config[\"encoder_experiment\"]\n",
    "                else experiment_config[\"encoder_units\"]\n",
    "            )\n",
    "            decoder_units = (\n",
    "                [units, units//2] if (not experiment_config[\"encoder_experiment\"])\n",
    "                else experiment_config[\"decoder_units\"]\n",
    "            )\n",
    "            \n",
    "            end_to_end_model, encoder, decoder = model_utils.construct_end_to_end_model(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "                learning_rate=experiment_config[\"learning_rate\"],\n",
    "                encoder=model_utils.construct_encoder(\n",
    "                    input_shape=experiment_config[\"input_shape\"],\n",
    "                    filter_groups=experiment_config[\"encoder_filter_groups\"],\n",
    "                    units=encoder_units,\n",
    "                    concept_cardinality=experiment_config[\"concept_cardinality\"],\n",
    "                    drop_prob=experiment_config.get(\"drop_prob\", 0.5),\n",
    "                    max_pool_window=experiment_config.get(\"max_pool_window\", (2, 2)),\n",
    "                    max_pool_stride=experiment_config.get(\"max_pool_stride\", (2, 2)),\n",
    "                    latent_dims=experiment_config.get(\"latent_dims\", 0),\n",
    "                    output_logits=experiment_config.get(\"encoder_output_logits\", False),\n",
    "                ),\n",
    "                decoder=model_utils.construct_decoder(\n",
    "                    units=decoder_units,\n",
    "                    num_outputs=experiment_config[\"num_outputs\"],\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            if experiment_config.get(\"pre_train_epochs\"):\n",
    "                print(\"\\tModel pre-training...\")\n",
    "                end_to_end_model.fit(\n",
    "                    x=x_train,\n",
    "                    y=y_train,\n",
    "                    epochs=experiment_config[\"pre_train_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "                print(\"\\t\\tModel pre-training completed\")\n",
    "            \n",
    "            # Now time to actually construct and train the CBM\n",
    "            cbm_model = construct_cbm(\n",
    "                encoder=encoder,\n",
    "                decoder=decoder,\n",
    "                alpha=experiment_config[\"alpha\"],\n",
    "                learning_rate=experiment_config[\"learning_rate\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "                latent_dims=experiment_config.get(\"latent_dims\", 0),\n",
    "                encoder_output_logits=experiment_config.get(\"encoder_output_logits\", False),\n",
    "            )\n",
    "\n",
    "            early_stopping_monitor = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=experiment_config.get(\n",
    "                    \"early_stop_metric\",\n",
    "                    \"val_concept_accuracy\",\n",
    "                ),\n",
    "                min_delta=experiment_config[\"min_delta\"],\n",
    "                patience=experiment_config[\"patience\"],\n",
    "                restore_best_weights=True,\n",
    "                verbose=2,\n",
    "                mode=experiment_config.get(\n",
    "                    \"early_stop_mode\",\n",
    "                    \"max\",\n",
    "                ),\n",
    "            )\n",
    "            if experiment_config[\"warmup_epochs\"]:\n",
    "                print(\"\\tWarmup training...\")\n",
    "                cbm_model.fit(\n",
    "                    x=x_train,\n",
    "                    y=(y_train, c_train),\n",
    "                    epochs=experiment_config[\"warmup_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "                print(\"\\t\\tWarmup training completed\")\n",
    "\n",
    "\n",
    "            print(\"\\tCBM training...\")\n",
    "            cbm_model.fit(\n",
    "                x=x_train,\n",
    "                y=(y_train, c_train),\n",
    "                epochs=experiment_config[\"max_epochs\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                callbacks=[\n",
    "                    early_stopping_monitor,\n",
    "                ],\n",
    "                validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                verbose=verbosity,\n",
    "            )\n",
    "            print(\"\\t\\tCBM training completed\")\n",
    "            print(\"\\tSerializing model\")\n",
    "            encoder.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/encoder_{'_'.join(map(str, encoder_units))}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            decoder.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/decoder_{'_'.join(map(str, decoder_units))}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            print(\"\\tEvaluating model\")\n",
    "            test_result = cbm_model.evaluate(\n",
    "                x_test,\n",
    "                (\n",
    "                    y_test,\n",
    "                    c_test[:, :experiment_config[\"num_concepts\"]],\n",
    "                ),\n",
    "                verbose=0,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            task_accs.append(\n",
    "                test_result['sparse_top_k_categorical_accuracy']\n",
    "                if experiment_config['num_outputs'] > 1 else\n",
    "                test_result['binary_accuracy']\n",
    "            )\n",
    "            concept_accs.append(test_result['concept_accuracy'])\n",
    "            \n",
    "            if experiment_config['num_outputs'] > 1:\n",
    "                # Then lets apply a softmax activation over all the probability\n",
    "                # classes\n",
    "                preds = scipy.special.softmax(cbm_model.predict(x_test)[0], axis=-1)\n",
    "\n",
    "                # And select just the labels that are in fact being used\n",
    "                one_hot_labels = tf.keras.utils.to_categorical(y_test)\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    one_hot_labels,\n",
    "                    preds,\n",
    "                    multi_class='ovo',\n",
    "                ))\n",
    "            else:\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    y_test,\n",
    "                    cbm_model.predict(x_test)[0],\n",
    "                ))\n",
    "            \n",
    "            print(\n",
    "                f\"\\t\\tTest auc = {aucs[-1]:.4f}, \"\n",
    "                f\"test concept accuracy = {concept_accs[-1]:.4f}, \"\n",
    "                f\"task accuracy = {task_accs[-1]:.4f}\"\n",
    "            )\n",
    "\n",
    "            print(f\"\\t\\tComputing impurity scores...\")\n",
    "            c_train_pred = cbm_model.encoder(x_train).numpy()\n",
    "            c_test_pred = cbm_model.encoder(x_test).numpy()\n",
    "\n",
    "            # Concept niching scores\n",
    "            nis = niching.niche_impurity_score(\n",
    "                c_soft=c_test_pred,\n",
    "                c_true=c_test,\n",
    "                c_soft_train=c_train_pred,\n",
    "                c_true_train=c_train,\n",
    "            )\n",
    "            \n",
    "            print(\"\\t\\t\\tNIS:\", nis)\n",
    "            niss.append(nis)\n",
    "            \n",
    "            print(\"\\t\\tComputing intervention accuracies...\")\n",
    "            current_intervention_task_acc = []\n",
    "            current_intervention_task_auc = []\n",
    "            if experiment_config.get('random_intervention_order', False):\n",
    "                intervention_order = np.random.permutation(experiment_config['num_concepts'])\n",
    "            else:\n",
    "                intervention_order = range(experiment_config['num_concepts'])\n",
    "            for i in range(len(intervention_order) + 1):\n",
    "                corrected_concepts = intervention_order[:i]\n",
    "                intervention_result = cbm_intervention(\n",
    "                    cbm_model=cbm_model,\n",
    "                    x_train=x_train,\n",
    "                    x_test=x_test,\n",
    "                    y_test=y_test,\n",
    "                    c_test=c_test[:, :experiment_config[\"num_concepts\"]],\n",
    "                    intervention_concepts=corrected_concepts,\n",
    "                )\n",
    "                current_intervention_task_acc.append(intervention_result['accuracy'])\n",
    "                current_intervention_task_auc.append(intervention_result['auc'])\n",
    "            \n",
    "            print(\"\\t\\t\\tIntervention AUCs:\", current_intervention_task_auc)\n",
    "            print(\"\\t\\t\\tIntervention Accuraciess:\", current_intervention_task_acc)\n",
    "            intervention_task_accuracies.append(current_intervention_task_acc)\n",
    "            intervention_task_aucs.append(current_intervention_task_auc)\n",
    "\n",
    "            print(f\"\\t\\tComputing OIS...\")\n",
    "            soft_acts = (\n",
    "                np.concatenate(cbm_model.encoder(x_test), axis=-1)\n",
    "                if experiment_config[\"latent_dims\"] else c_test_pred\n",
    "            )\n",
    "            ois, purity_mat, oracle_mat = oracle.oracle_impurity_score(\n",
    "                c_soft=soft_acts,\n",
    "                c_true=c_test,\n",
    "                output_matrices=True,\n",
    "            )\n",
    "            purity_mats.append(purity_mat)\n",
    "            oracle_mats.append(oracle_mat)\n",
    "            oiss.append(ois)\n",
    "            print(f\"\\t\\t\\tDone {ois:.4f}\")\n",
    "\n",
    "        task_acc_mean, task_acc_std = np.mean(task_accs), np.std(task_accs)\n",
    "        experiment_variables[\"task_accuracies\"].append((task_acc_mean, task_acc_std))\n",
    "        print(f\"\\tTest task accuracy: {task_acc_mean:.4f} ± {task_acc_std:.4f}\")\n",
    "\n",
    "        concept_acc_mean, concept_acc_std = np.mean(concept_accs), np.std(concept_accs)\n",
    "        experiment_variables[\"concept_accuracies\"].append((concept_acc_mean, concept_acc_std))\n",
    "        print(f\"\\tTest concept accuracy: {concept_acc_mean:.4f} ± {concept_acc_std:.4f}\")\n",
    "\n",
    "\n",
    "        task_auc_mean, task_auc_std = np.mean(aucs), np.std(aucs)\n",
    "        experiment_variables[\"task_aucs\"].append((task_auc_mean, task_auc_std))\n",
    "        print(f\"\\tTest task AUC: {task_auc_mean:.4f} ± {task_auc_std:.4f}\")\n",
    "\n",
    "        purity_mats = np.stack(purity_mats, axis=0)\n",
    "        purity_mat_mean = np.mean(purity_mats, axis=0)\n",
    "        purity_mat_std = np.std(purity_mats, axis=0)\n",
    "        print(\"\\tPurity matrix:\")\n",
    "        for i in range(purity_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(purity_mat_mean.shape[1]):\n",
    "                line += f'{purity_mat_mean[i, j]:.4f} ± {purity_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"purity_matrices\"].append((purity_mat_mean, purity_mat_std))\n",
    "\n",
    "\n",
    "        oracle_mats = np.stack(oracle_mats, axis=0)\n",
    "        oracle_mat_mean = np.mean(oracle_mats, axis=0)\n",
    "        oracle_mat_std = np.std(oracle_mats, axis=0)\n",
    "        print(\"\\tOracle matrix:\")\n",
    "        for i in range(oracle_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(oracle_mat_mean.shape[1]):\n",
    "                line += f'{oracle_mat_mean[i, j]:.4f} ± {oracle_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"oracle_matrices\"].append((oracle_mat_mean, oracle_mat_std))\n",
    "\n",
    "        ois_mean, ois_std = np.mean(oiss), np.std(oiss)\n",
    "        experiment_variables[\"oiss\"].append((ois_mean, ois_std))\n",
    "        print(f\"\\tOIS: {ois_mean:.4f} ± {ois_std:.4f}\")\n",
    "\n",
    "        \n",
    "        nis_mean, nis_std = np.mean(niss), np.std(niss)\n",
    "        experiment_variables[\"niss\"].append((nis_mean, nis_std))\n",
    "        print(f\"\\tNIS: {nis_mean:.4f} ± {nis_std:.4f}\")\n",
    "\n",
    "        \n",
    "        # Finally, we end with intervention accuracies\n",
    "        interventaion_auc_hist = np.stack(intervention_task_aucs, axis=0)\n",
    "        interventaion_auc_mean = np.mean(interventaion_auc_hist, axis=0)\n",
    "        interventaion_auc_std = np.std(interventaion_auc_hist, axis=0)\n",
    "        experiment_variables[\"intervention_task_aucs\"].append((interventaion_auc_mean, interventaion_auc_std))\n",
    "        line = \"\\tIntervention AUCs: [\"\n",
    "        for j in range(interventaion_auc_mean.shape[0]):\n",
    "            line += f'{interventaion_auc_mean[j]:.4f} ± {interventaion_auc_std[j]:.4f},  '\n",
    "        print(line + \"]\")\n",
    "        \n",
    "        interventaion_acc_hist = np.stack(intervention_task_accuracies, axis=0)\n",
    "        interventaion_acc_mean = np.mean(interventaion_acc_hist, axis=0)\n",
    "        interventaion_acc_std = np.std(interventaion_acc_hist, axis=0)\n",
    "        experiment_variables[\"intervention_task_accuracies\"].append((interventaion_acc_mean, interventaion_acc_std))\n",
    "        line = \"\\tIntervention Accuracies: [\"\n",
    "        for j in range(interventaion_acc_mean.shape[0]):\n",
    "            line += f'{interventaion_acc_mean[j]:.4f} ± {interventaion_acc_std[j]:.4f},  '\n",
    "        print(line + \"]\")\n",
    "        \n",
    "        # Increase the experiment counter\n",
    "        experiment_variables[\"current_experiment_idx\"] += 1\n",
    "\n",
    "        # And serialize the results\n",
    "        joblib.dump(\n",
    "            experiment_variables,\n",
    "            os.path.join(experiment_config[\"results_dir\"], 'results.joblib'),\n",
    "        )\n",
    "    return experiment_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Model Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "base_experiment_config = dict(\n",
    "    batch_size=64 if USE_DSPRITES else 32,\n",
    "    max_epochs=100,\n",
    "    pre_train_epochs=0,\n",
    "    warmup_epochs=0,\n",
    "    trials=NUM_TRIALS,\n",
    "    alpha=1,\n",
    "    learning_rate=1e-3,\n",
    "    encoder_filter_groups=[\n",
    "        [(8, (7, 7))],\n",
    "        [(16, (5, 5))],\n",
    "        [(32, (3, 3))],\n",
    "        [(64, (3, 3))]\n",
    "    ],\n",
    "    encoder_units=[64, 64],\n",
    "   \n",
    "    model_units=[4, 8, 16, 24, 32, 64],\n",
    "    encoder_experiment=False,\n",
    "\n",
    "    latent_decoder_units=[64, 64],\n",
    "    predictor_max_epochs=100,\n",
    "    concept_predictor_max_epochs=100,\n",
    "    \n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    pax_pool_stride=2,\n",
    "    decoder_units=[64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(data_train[1])) if len(set(data_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    concept_cardinality=[1 for _ in range(data_train[2].shape[-1])],\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(RESULTS_DIR, \"cbm/base_complete\"),\n",
    "    input_shape=data_train[0].shape[1:],\n",
    "    num_concepts=data_train[2].shape[-1],\n",
    "    latent_dims=0,\n",
    "    holdout_fraction=0.1,\n",
    "    verbosity=0,\n",
    "    early_stop_metric=\"val_concept_accuracy\",\n",
    "    early_stop_mode=\"max\",\n",
    "    encoder_output_logits=False,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "base_results = cbm_mixed_capacity_intervention_experiment_loop(\n",
    "    data_train,\n",
    "    data_test,\n",
    "    base_experiment_config,\n",
    "    load_from_cache=True,\n",
    ")\n",
    "print(\"task_accuracies:\", base_results[\"task_accuracies\"])\n",
    "print(\"concept_accuracies:\", base_results[\"concept_accuracies\"])\n",
    "print(\"task_aucs:\", base_results[\"task_aucs\"])\n",
    "print(\"ois:\", base_results[\"oiss\"])\n",
    "print(\"nis:\", base_results[\"niss\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Logits CBM Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "from_logits_experiment_config = dict(\n",
    "    batch_size=64 if USE_DSPRITES else 32,\n",
    "    max_epochs=100,\n",
    "    pre_train_epochs=0,\n",
    "    warmup_epochs=0,\n",
    "    trials=NUM_TRIALS,\n",
    "    alpha=1,\n",
    "    learning_rate=1e-3,\n",
    "    encoder_filter_groups=[\n",
    "        [(8, (7, 7))],\n",
    "        [(16, (5, 5))],\n",
    "        [(32, (3, 3))],\n",
    "        [(64, (3, 3))]\n",
    "    ],\n",
    "    encoder_units=[64, 64],\n",
    "   \n",
    "    model_units=[4, 8, 16, 24, 32, 64],\n",
    "    encoder_experiment=False,\n",
    "\n",
    "    latent_decoder_units=[64, 64],\n",
    "    predictor_max_epochs=100,\n",
    "    concept_predictor_max_epochs=100,\n",
    "    \n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    pax_pool_stride=2,\n",
    "    decoder_units=[64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(data_train[1])) if len(set(data_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    concept_cardinality=[1 for _ in range(data_train[2].shape[-1])],\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(RESULTS_DIR, \"cbm/from_logits_complete\"),\n",
    "    input_shape=data_train[0].shape[1:],\n",
    "    num_concepts=data_train[2].shape[-1],\n",
    "    latent_dims=0,\n",
    "    holdout_fraction=0.1,\n",
    "    verbosity=0,\n",
    "    early_stop_metric=\"val_concept_accuracy\",\n",
    "    early_stop_mode=\"max\",\n",
    "    encoder_output_logits=True,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "from_logits_results = cbm_mixed_capacity_intervention_experiment_loop(\n",
    "    data_train,\n",
    "    data_test,\n",
    "    from_logits_experiment_config,\n",
    "    load_from_cache=True,\n",
    ")\n",
    "print(\"task_accuracies:\", from_logits_results[\"task_accuracies\"])\n",
    "print(\"concept_accuracies:\", from_logits_results[\"concept_accuracies\"])\n",
    "print(\"task_aucs:\", from_logits_results[\"task_aucs\"])\n",
    "print(\"oiss:\", from_logits_results[\"oiss\"])\n",
    "print(\"niss:\", from_logits_results[\"niss\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATEX_SYMBOL = \"\"\n",
    "\n",
    "def bold_text(x):\n",
    "    if LATEX_SYMBOL == \"$\":\n",
    "        return r\"$\\textbf{\" + x + \"}$\"\n",
    "    return x\n",
    "\n",
    "def plot_intervention_results(\n",
    "    results_1,\n",
    "    cand_1_idx,\n",
    "    cand_1_name,\n",
    "    results_2,\n",
    "    cand_2_idx,\n",
    "    cand_2_name,\n",
    "    num_concepts=5,\n",
    "    savepath=None,\n",
    "    show_impurity=True,\n",
    "    start_y=50,\n",
    "    scatter_start_y=65,\n",
    "    delta_y=5,\n",
    "    max_y=100,\n",
    "    scatter_max_y=100,\n",
    "    fig_width=38,\n",
    "    fig_height=8,\n",
    "    legend_fontsize=30,\n",
    "    impurity_start_y=0,\n",
    "    impurity_delta_y=5,\n",
    "    impurity_max_y=85,\n",
    "    axis_fontsize=23,\n",
    "):\n",
    "    clrs = sns.color_palette(\"tab10\", 10)\n",
    "    color_map = {}\n",
    "    scale = 1\n",
    "    fig, axs = plt.subplots(1, 3 if show_impurity else 2, figsize=(fig_width, fig_height))\n",
    "    ax = axs[0]\n",
    "    x_labels = ['Task', 'Concept']\n",
    "    y_vals_cand_1 = [\n",
    "        # The task accuracy\n",
    "        np.array(results_1['task_accuracies'][cand_1_idx][0]) * 100,\n",
    "        # The concept accuracy\n",
    "        np.array(results_1['concept_accuracies'][cand_1_idx][0]) * 100,\n",
    "    ]\n",
    "    y_errs_cand_1 = [\n",
    "        # The task accuracy\n",
    "        np.array(results_1['task_accuracies'][cand_1_idx][1]) * 100,\n",
    "        # The concept accuracy\n",
    "        np.array(results_1['concept_accuracies'][cand_1_idx][1]) * 100,\n",
    "    ]\n",
    "\n",
    "    y_vals_cand_2 = [\n",
    "        # The task accuracy\n",
    "        np.array(results_2['task_accuracies'][cand_2_idx][0]) * 100,\n",
    "        # The concept accuracy\n",
    "        np.array(results_2['concept_accuracies'][cand_2_idx][0]) * 100,\n",
    "    ]\n",
    "    y_errs_cand_2 = [\n",
    "        # The task accuracy\n",
    "        np.array(results_2['task_accuracies'][cand_2_idx][1]) * 100,\n",
    "        # The concept accuracy\n",
    "        np.array(results_2['concept_accuracies'][cand_2_idx][1]) * 100,\n",
    "    ]\n",
    "\n",
    "    all_models = [\n",
    "        (cand_1_name, y_vals_cand_1, y_errs_cand_1),\n",
    "        (cand_2_name, y_vals_cand_2, y_errs_cand_2),\n",
    "    ]\n",
    "    num_models = len(all_models) + 1\n",
    "    center = np.zeros(len(x_labels))\n",
    "    for i, (method_name, accs, stds) in enumerate(all_models):\n",
    "        accs = np.array(accs)\n",
    "        stds = np.array(stds)\n",
    "        print(method_name, accs, stds)\n",
    "        print(\"accs =\", accs)\n",
    "        print(\"stds =\", stds)\n",
    "        if method_name not in color_map:\n",
    "            color_map[method_name] = clrs[len(color_map)]\n",
    "        color = color_map[method_name]\n",
    "        x_coords = scale * (np.arange(0, len(x_labels)) - (1/2 - 1/(2 * num_models)) + i/num_models)\n",
    "        center += x_coords/len(all_models)\n",
    "        ax.bar(\n",
    "            x_coords,\n",
    "            accs,\n",
    "            width=scale/num_models,\n",
    "            color=color,\n",
    "            align='center',\n",
    "            label=method_name,\n",
    "            yerr=2*stds,\n",
    "            capsize=5,\n",
    "            edgecolor=\"black\",\n",
    "        )\n",
    "    ax.set_ylabel(\"Accuracy (\\%)\", fontsize=axis_fontsize)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_title(bold_text(\"Accuracy Comparison\"), fontsize=axis_fontsize)\n",
    "    ax.set_xticks(center)\n",
    "    ax.set_xticklabels(x_labels, fontsize=20)\n",
    "    ax.set_yticks(np.arange(start_y, max_y+delta_y, delta_y))\n",
    "    ax.set_yticklabels(list(map(lambda x: f\"{x}\", np.arange(start_y, max_y + delta_y, delta_y))), fontsize=18)\n",
    "    ax.set_ylim((start_y, max_y + delta_y))\n",
    "    ax.grid(False)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    if show_impurity:\n",
    "        start_y = impurity_start_y\n",
    "        delta_y = impurity_delta_y\n",
    "        max_y = impurity_max_y\n",
    "        ax = axs[1]\n",
    "        x_labels = [\n",
    "            'OIS',\n",
    "            'NIS',\n",
    "        ]\n",
    "        y_vals_cand_1 = [\n",
    "            # The OIS\n",
    "            np.array(results_1['oiss'][cand_1_idx][0]) * 100,\n",
    "            # The NIS\n",
    "            np.array(results_1['niss'][cand_1_idx][0]) * 100,\n",
    "        ]\n",
    "        y_errs_cand_1 = [\n",
    "            # The OIS\n",
    "            np.array(results_1['oiss'][cand_1_idx][1]) * 100,\n",
    "            # The NIS\n",
    "            np.array(results_1['niss'][cand_1_idx][1]) * 100,\n",
    "        ]\n",
    "\n",
    "        y_vals_cand_2 = [\n",
    "            # The OIS\n",
    "            np.array(results_2['oiss'][cand_2_idx][0]) * 100,\n",
    "            # The NIS\n",
    "            np.array(results_2['niss'][cand_2_idx][0]) * 100,\n",
    "        ]\n",
    "        y_errs_cand_2 = [\n",
    "            # The OIS\n",
    "            np.array(results_2['oiss'][cand_2_idx][1]) * 100,\n",
    "            # The NIS\n",
    "            np.array(results_2['niss'][cand_2_idx][1]) * 100,\n",
    "        ]\n",
    "\n",
    "        all_models = [\n",
    "            (cand_1_name, y_vals_cand_1, y_errs_cand_1),\n",
    "            (cand_2_name, y_vals_cand_2, y_errs_cand_2),\n",
    "        ]\n",
    "        num_models = len(all_models) + 1\n",
    "        center = np.zeros(len(x_labels))\n",
    "        for i, (method_name, accs, stds) in enumerate(all_models):\n",
    "            accs = np.array(accs)\n",
    "            stds = np.array(stds)\n",
    "            print(method_name, accs, stds)\n",
    "            if method_name not in color_map:\n",
    "                color_map[method_name] = clrs[len(color_map)]\n",
    "            color = color_map[method_name]\n",
    "            x_coords = scale * (np.arange(0, len(x_labels)) - (1/2 - 1/(2 * num_models)) + i/num_models)\n",
    "            center += x_coords/len(all_models)\n",
    "            ax.bar(\n",
    "                x_coords,\n",
    "                accs,\n",
    "                width=scale/num_models,\n",
    "                color=color,\n",
    "                align='center',\n",
    "                label=method_name,\n",
    "                yerr=2*stds,\n",
    "                capsize=5,\n",
    "                edgecolor=\"black\",\n",
    "            )\n",
    "        ax.set_ylabel(\"Score (\\%)\", fontsize=axis_fontsize)\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_title(bold_text(\"Score Comparison\"), fontsize=axis_fontsize)\n",
    "        ax.set_xticks(center)\n",
    "        ax.set_xticklabels(x_labels, fontsize=20)\n",
    "        ax.set_yticks(np.arange(start_y, max_y+delta_y, delta_y))\n",
    "        ax.set_yticklabels(list(map(lambda x: f\"{x}\", np.arange(start_y, max_y + delta_y, delta_y))), fontsize=18)\n",
    "        ax.set_ylim((start_y, max_y + delta_y))\n",
    "        ax.grid(False)\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    ax = axs[2 if show_impurity else 1]\n",
    "    x_labels = list(map(str, np.arange(0, num_concepts + 1)))\n",
    "    x_coords = np.arange(0, num_concepts + 1)\n",
    "    y_vals_cand_1 = 100 * np.array(results_1['intervention_task_accuracies'][cand_1_idx][0])\n",
    "    y_errs_cand_1 = 100 * np.array(results_1['intervention_task_accuracies'][cand_1_idx][1])\n",
    "\n",
    "    y_vals_cand_2 = 100 * np.array(results_2['intervention_task_accuracies'][cand_2_idx][0])\n",
    "    y_errs_cand_2 = 100 * np.array(results_2['intervention_task_accuracies'][cand_2_idx][1])\n",
    "\n",
    "    all_models = [\n",
    "        (cand_1_name, y_vals_cand_1, y_errs_cand_1),\n",
    "        (cand_2_name, y_vals_cand_2, y_errs_cand_2),\n",
    "    ]\n",
    "    num_models = len(all_models) + 1\n",
    "    for i, (method_name, accs, stds) in enumerate(all_models):\n",
    "        accs = np.array(accs)\n",
    "        stds = np.array(stds)\n",
    "        print(method_name, accs, stds)\n",
    "        if method_name not in color_map:\n",
    "            color_map[method_name] = clrs[len(color_map)]\n",
    "        color = color_map[method_name]\n",
    "        ax.plot(\n",
    "            x_coords,\n",
    "            accs,\n",
    "            c=color,\n",
    "            zorder=1,\n",
    "        )\n",
    "        plt.scatter(\n",
    "            x_coords,\n",
    "            accs,\n",
    "            s=50,\n",
    "            color=color,\n",
    "            label=method_name,\n",
    "            zorder=2,\n",
    "        )\n",
    "        ax.fill_between(\n",
    "            x_coords,\n",
    "            accs - stds,\n",
    "            accs + stds,\n",
    "            alpha=0.3,\n",
    "            facecolor=color,\n",
    "        )\n",
    "    ax.set_ylabel(\"Test Accuracy (\\%)\", fontsize=axis_fontsize)\n",
    "    ax.set_xlabel(\"Concepts Intervened\", fontsize=axis_fontsize)\n",
    "    ax.set_title(bold_text(\"Effects of Intervention\"), fontsize=axis_fontsize)\n",
    "    ax.set_xticks(x_coords)\n",
    "    ax.set_xticklabels(x_labels, fontsize=20)\n",
    "    ax.set_yticks(np.arange(scatter_start_y, scatter_max_y+delta_y, delta_y))\n",
    "    ax.set_yticklabels(list(map(lambda x: f\"{x}\", np.arange(scatter_start_y, scatter_max_y + delta_y, delta_y))), fontsize=18)\n",
    "    ax.set_ylim((scatter_start_y, scatter_max_y + delta_y))\n",
    "    ax.grid(False)\n",
    "\n",
    "    lgd = fig.legend(handles, labels, fontsize=legend_fontsize, loc='upper center', bbox_to_anchor=(0.5,0.03), ncol=(num_models - 1))\n",
    "    if savepath:\n",
    "        plt.savefig(\n",
    "            savepath,\n",
    "            bbox_extra_artists=(lgd,),\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbm_intervention_capacity_shapes3d = {}\n",
    "cbm_intervention_capacity_shapes3d[\"base\"] = joblib.load(os.path.join(\n",
    "    \"intervention_results/dsprites\",\n",
    "    \"cbm/base_complete/results.joblib\"\n",
    "))\n",
    "cbm_intervention_capacity_shapes3d[\"logits\"] = joblib.load(os.path.join(\n",
    "    \"intervention_results/dsprites\",\n",
    "    \"cbm/from_logits_complete/results.joblib\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacities = [4, 8, 16, 24, 32]\n",
    "cand_1_idx = 2\n",
    "cand_1_set = \"logits\"\n",
    "cand_2_idx = 2\n",
    "cand_2_set = \"base\"\n",
    "plot_intervention_results(\n",
    "    results_1=cbm_intervention_capacity_shapes3d[cand_1_set],\n",
    "    cand_1_idx=cand_1_idx,\n",
    "    cand_1_name=f\"Joint-CBM{'-Logits' if cand_1_set == 'logits' else ''}\",\n",
    "    results_2=cbm_intervention_capacity_shapes3d[cand_2_set],\n",
    "    cand_2_idx=cand_2_idx,\n",
    "    cand_2_name=f\"Joint-CBM{'-Logits' if cand_2_set == 'logits' else ''}\",\n",
    "    num_concepts=5,\n",
    "    delta_y=10,\n",
    "    fig_width=17,\n",
    "    fig_height=4,\n",
    "    legend_fontsize=20,\n",
    "    impurity_delta_y=10,\n",
    "    axis_fontsize=19,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacities = [4, 8, 16, 24, 32]\n",
    "cand_2_idx = 4\n",
    "cand_2_set = \"logits\"\n",
    "cand_1_idx = 1\n",
    "cand_1_set = cand_1_set\n",
    "plot_intervention_results(\n",
    "    results_1=cbm_intervention_capacity_shapes3d[cand_1_set],\n",
    "    cand_1_idx=cand_1_idx,\n",
    "    cand_1_name=f\"Joint-CBM{'-Logits' if cand_1_set == 'logits' else ''} (Label Predictor MLP Layers [{capacities[cand_1_idx]}, {capacities[cand_1_idx]//2}])\",\n",
    "    results_2=cbm_intervention_capacity_shapes3d[cand_2_set],\n",
    "    cand_2_idx=cand_2_idx,\n",
    "    cand_2_name=f\"Joint-CBM{'-Logits' if cand_2_set == 'logits' else ''} (Label Predictor MLP Layers [{capacities[cand_2_idx]}, {capacities[cand_2_idx]//2}])\",\n",
    "    num_concepts=5,\n",
    "    delta_y=10,\n",
    "    fig_width=18,\n",
    "    fig_height=4,\n",
    "    legend_fontsize=20,\n",
    "    impurity_delta_y=10,\n",
    "    axis_fontsize=18,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "purity_dsprites.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
