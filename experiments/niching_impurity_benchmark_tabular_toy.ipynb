{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purity Correlation Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib\n",
    "import concepts_xai\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import rc\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "from tensorflow.keras.models import load_model\n",
    "from joblib import dump, load\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "import utils\n",
    "import concepts_xai.evaluation.metrics.niching as niching\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "## Set seeds up for reproducibility\n",
    "################################################################################\n",
    "utils.reseed(87)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "## Global Variables Defining Experiment Flow\n",
    "################################################################################\n",
    "NUM_CONCEPTS = 5\n",
    "INPUT_SHAPE = [12]\n",
    "FROM_CACHE = True\n",
    "_LATEX_SYMBOL = \"\"\n",
    "BASE_DIR = '.'\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"results/toy_tabular\")\n",
    "NICHING_RESULTS_DIR = os.path.join(BASE_DIR, \"results_concept_niching_integrated/toy_tabular\")\n",
    "rc('text', usetex=(_LATEX_SYMBOL == \"$\"))\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold_text(x):\n",
    "    if _LATEX_SYMBOL == \"$\":\n",
    "        return r\"$\\textbf{\" + x + \"}$\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## Generate Data\n",
    "############################################################################\n",
    "def produce_data(samples, cov=0.0, num_concepts=NUM_CONCEPTS):\n",
    "    x = np.zeros((samples, INPUT_SHAPE[0]), dtype=np.float32)\n",
    "    y = np.zeros((samples,), dtype=np.float32)\n",
    "    \n",
    "    # Sample the x, y, and z variables\n",
    "    vars = np.random.multivariate_normal(\n",
    "        mean=[0, 0, 0, 0, 0],\n",
    "        cov=[\n",
    "            [1, cov, cov, 0, 0],\n",
    "            [cov, 1, cov, 0, 0],\n",
    "            [cov, cov, 1, 0, 0],\n",
    "            [0, 0, 0, 1, cov],\n",
    "            [0, 0, 0, cov, 1],\n",
    "        ],\n",
    "        size=(samples,),\n",
    "    )\n",
    "    x_vars = vars[:, :1]\n",
    "    y_vars = vars[:, 1:2]\n",
    "    z_vars = vars[:, 2:3]\n",
    "    a_vars = vars[:, 3:4]\n",
    "    b_vars = vars[:, 4:]\n",
    "    \n",
    "    # The features are just non-linear functions applied to each\n",
    "    # variable\n",
    "    features = [\n",
    "        np.sin(x_vars) + x_vars,\n",
    "        np.cos(x_vars) + x_vars,\n",
    "        np.sin(y_vars) + y_vars,\n",
    "        np.cos(y_vars) + y_vars,\n",
    "        np.sin(z_vars) + z_vars,\n",
    "        np.cos(z_vars) + z_vars,\n",
    "        x_vars**2 + y_vars**2 + z_vars**2,\n",
    "        np.sin(a_vars) + a_vars,\n",
    "        np.cos(a_vars) + a_vars,\n",
    "        np.sin(b_vars) + b_vars,\n",
    "        np.cos(b_vars) + b_vars,\n",
    "        a_vars**2 + b_vars**2\n",
    "    ]\n",
    "    features = np.stack(features, axis=1)\n",
    "\n",
    "    # The concepts just check if the variables are positive\n",
    "    x_pos = (x_vars > 0).astype(np.int32)\n",
    "    y_pos = (y_vars > 0).astype(np.int32)\n",
    "    z_pos = (z_vars > 0).astype(np.int32)\n",
    "    a_pos = (a_vars > 0).astype(np.int32)\n",
    "    b_pos = (b_vars > 0).astype(np.int32)\n",
    "    concepts = np.squeeze(\n",
    "        np.stack([x_pos, y_pos, z_pos, a_pos, b_pos][:num_concepts], axis=1)\n",
    "    ).astype(np.float32)\n",
    "    \n",
    "    # The labels are generated by checking if at least two of the\n",
    "    # latent concepts are greater than zero\n",
    "    labels = np.zeros((samples, 2))\n",
    "    labels[:, 0] = (x_pos + y_pos + z_pos).squeeze()\n",
    "    labels[:, 1] = (a_pos + b_pos).squeeze()\n",
    "    labels = (labels > 1).astype(np.int32)\n",
    "    \n",
    "    # And that's it buds\n",
    "    return features.squeeze(), labels.argmax(axis=1), concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the encoder model\n",
    "def construct_encoder(\n",
    "    input_shape,\n",
    "    units,\n",
    "    num_concepts,\n",
    "    end_activation=\"sigmoid\",\n",
    "    latent_dims=0,\n",
    "    output_logits=False,\n",
    "):\n",
    "    encoder_inputs = tf.keras.Input(shape=input_shape)\n",
    "    encoder_compute_graph = encoder_inputs\n",
    "    for i, units in enumerate(units):\n",
    "        encoder_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(encoder_compute_graph)\n",
    "\n",
    "    # And finally map this to the number of concepts we have in our set\n",
    "    encoder_compute_graph = tf.keras.layers.Flatten()(encoder_compute_graph)\n",
    "    if latent_dims:\n",
    "        bypass = tf.keras.layers.Dense(\n",
    "            latent_dims,\n",
    "            activation=end_activation,\n",
    "            name=\"encoder_bypass_channel\",\n",
    "        )(encoder_compute_graph)\n",
    "    else:\n",
    "        bypass = None\n",
    "    encoder_compute_graph = tf.keras.layers.Dense(\n",
    "        num_concepts,\n",
    "        activation=None if output_logits else \"sigmoid\",\n",
    "        name=\"encoder_concept_outputs\",\n",
    "    )(encoder_compute_graph)\n",
    "\n",
    "    # Now time to collapse all the concepts again back into a single vector\n",
    "    encoder_model = tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        encoder_compute_graph if bypass is None else [encoder_compute_graph, bypass],\n",
    "        name=\"encoder\",\n",
    "    )\n",
    "    return encoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## Build concepts-to-labels model\n",
    "############################################################################\n",
    "\n",
    "def construct_decoder(units, num_outputs=1,):\n",
    "    decoder_layers = [\n",
    "        tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation=tf.nn.relu,\n",
    "            name=f\"decoder_dense_{i+1}\",\n",
    "        ) for i, units in enumerate(units)\n",
    "    ]\n",
    "    decoder_model = tf.keras.Sequential(decoder_layers + [\n",
    "        tf.keras.layers.Dense(\n",
    "            num_outputs,\n",
    "            activation=None,\n",
    "            name=\"decoder_model_output\",\n",
    "        )\n",
    "    ])\n",
    "    return decoder_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBM Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.CBM.CBModel as CBM\n",
    "reload(CBM)\n",
    "\n",
    "############################################################################\n",
    "## Build CBM\n",
    "############################################################################\n",
    "\n",
    "def construct_cbm(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    latent_dims=0,\n",
    "    alpha=0.1,\n",
    "    learning_rate=1e-3,\n",
    "    encoder_output_logits=False,\n",
    "):\n",
    "    model_factory = CBM.BypassJointCBM if latent_dims else CBM.JointConceptBottleneckModel\n",
    "    cbm_model = model_factory(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        task_loss=(\n",
    "            tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        ),\n",
    "        name=\"joint_cbm\",\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()],\n",
    "        alpha=alpha,\n",
    "        pass_concept_logits=encoder_output_logits,\n",
    "    )\n",
    "\n",
    "    ############################################################################\n",
    "    ## Compile CBM Model\n",
    "    ############################################################################\n",
    "\n",
    "    cbm_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    )\n",
    "    return cbm_model\n",
    "\n",
    "# Construct the complete model\n",
    "def construct_end_to_end_model(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    input_shape,\n",
    "    learning_rate=1e-3,\n",
    "):\n",
    "    model_inputs = tf.keras.Input(shape=input_shape)\n",
    "    encoder_out = encoder(model_inputs)\n",
    "    if isinstance(encoder_out, list):\n",
    "        encoder_out = tf.concat(encoder_out, axis=-1)\n",
    "    model_compute_graph = decoder(encoder_out)\n",
    "    # Now time to collapse all the concepts again back into a single vector\n",
    "    model = tf.keras.Model(\n",
    "        model_inputs,\n",
    "        model_compute_graph,\n",
    "        name=\"complete_model\",\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[\"binary_accuracy\"],\n",
    "    )\n",
    "    return model, encoder, decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBM-Prob Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.evaluation.metrics.oracle as oracle\n",
    "\n",
    "############################################################################\n",
    "## Experiment loop\n",
    "############################################################################\n",
    "\n",
    "def construct_trivial_auc_mat(num_concepts):\n",
    "    result = np.ones((num_concepts, num_concepts), dtype=np.float32) * 0.5\n",
    "    return result + np.eye(num_concepts, dtype=np.float32) * 0.5\n",
    "    \n",
    "def cbm_experiment_loop(experiment_config, load_from_cache=False):\n",
    "    utils.reseed(87)\n",
    "    experiment_config[\"data_concepts\"] = experiment_config.get(\n",
    "        \"data_concepts\",\n",
    "        experiment_config[\"num_concepts\"],\n",
    "    )\n",
    "    experiment_variables = dict(\n",
    "        config = experiment_config,\n",
    "        task_accuracies = [],\n",
    "        concept_accuracies = [],\n",
    "        niss=[],\n",
    "    )\n",
    "    res_dir = experiment_config['niching_results_dir']\n",
    "    if load_from_cache:\n",
    "        if os.path.exists(os.path.join(res_dir, 'results_niching.joblib')):\n",
    "            experiment_variables = load(os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    count = 0\n",
    "    start_ind = 0\n",
    "    \n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"niching_results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    for cov in experiment_config[\"covariances\"][start_ind:]:\n",
    "        print(\"Training with covariance:\", cov)\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} with covariance {cov}\")\n",
    "            # First construct the dataset\n",
    "            (x_train, y_train, y_train_concepts) = produce_data(\n",
    "                experiment_config[\"train_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            (x_test, y_test, y_test_concepts) = produce_data(\n",
    "                experiment_config[\"test_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            \n",
    "            # Then proceed to do and end-to-end model in case we want to\n",
    "            # do some task-specific pretraining\n",
    "            end_to_end_model, encoder, decoder = construct_end_to_end_model(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                encoder=construct_encoder(\n",
    "                    input_shape=experiment_config[\"input_shape\"],\n",
    "                    units=experiment_config[\"encoder_units\"],\n",
    "                    num_concepts=experiment_config[\"num_concepts\"],\n",
    "                    end_activation=\"sigmoid\",\n",
    "                    latent_dims=experiment_config[\"latent_dims\"],\n",
    "                    output_logits=experiment_config.get(\"encoder_output_logits\", False),\n",
    "                ),\n",
    "                decoder=construct_decoder(\n",
    "                    units=experiment_config[\"decoder_units\"],\n",
    "                    num_outputs=experiment_config[\"num_outputs\"],\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            if experiment_config.get(\"pre_train_epochs\"):\n",
    "                print(\"\\tModel pre-training...\")\n",
    "                end_to_end_model.fit(\n",
    "                    x=x_train,\n",
    "                    y=y_train,\n",
    "                    epochs=experiment_config[\"pre_train_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "                print(\"\\t\\tModel pre-training completed\")\n",
    "            \n",
    "            # Now time to actually construct and train the CBM\n",
    "            cbm_model = construct_cbm(\n",
    "                encoder=encoder,\n",
    "                decoder=decoder,\n",
    "                alpha=experiment_config[\"alpha\"],\n",
    "                learning_rate=experiment_config[\"learning_rate\"],\n",
    "                latent_dims=experiment_config.get(\"latent_dims\", 0),\n",
    "                encoder_output_logits=experiment_config.get(\"encoder_output_logits\", False),\n",
    "            )\n",
    "\n",
    "            early_stopping_monitor = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_concept_accuracy\",\n",
    "                min_delta=experiment_config[\"min_delta\"],\n",
    "                patience=experiment_config[\"patience\"],\n",
    "                restore_best_weights=True,\n",
    "                verbose=2,\n",
    "                mode='max',\n",
    "            )\n",
    "            if experiment_config[\"warmup_epochs\"]:\n",
    "                print(\"\\tWarmup training...\")\n",
    "                cbm_model.fit(\n",
    "                    x=x_train,\n",
    "                    y=(\n",
    "                        y_train,\n",
    "                        y_train_concepts[:, :experiment_config[\"num_concepts\"]],\n",
    "                    ),\n",
    "                    epochs=experiment_config[\"warmup_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "                print(\"\\t\\tWarmup training completed\")\n",
    "\n",
    "\n",
    "            print(\"\\tCBM training...\")\n",
    "            cbm_model.fit(\n",
    "                x=x_train,\n",
    "                y=(\n",
    "                    y_train,\n",
    "                    y_train_concepts[:, :experiment_config[\"num_concepts\"]],\n",
    "                ),\n",
    "                epochs=experiment_config[\"max_epochs\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                callbacks=[\n",
    "                    early_stopping_monitor,\n",
    "                ],\n",
    "                validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                verbose=verbosity,\n",
    "            )\n",
    "            print(\"\\t\\tCBM training completed\")\n",
    "            print(\"\\tEvaluating model\")\n",
    "            c_train_pred = cbm_model.encoder(x_train).numpy()\n",
    "            c_test_pred = cbm_model.encoder(x_test).numpy()\n",
    "            \n",
    "            experiment_variables['niss'].append(\n",
    "                niching.niche_impurity_score(\n",
    "                    c_soft=c_test_pred,\n",
    "                    c_true=y_test_concepts,\n",
    "                    c_soft_train=c_train_pred,\n",
    "                    c_true_train=y_train_concepts,\n",
    "                )\n",
    "            )\n",
    "            print(f'\\t\\tNIS: {experiment_variables[\"niss\"][-1]:.2f}')\n",
    "            \n",
    "            os.makedirs(res_dir, exist_ok=True)\n",
    "            dump(experiment_variables, os.path.join(res_dir, 'results_niching.joblib'))\n",
    "\n",
    "    return experiment_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(niching)\n",
    "reload(CBM)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "cbm_base_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=300,\n",
    "    warmup_epochs=0,\n",
    "    pre_train_epochs=0,\n",
    "    trials=5,\n",
    "    alpha=0.1,\n",
    "    learning_rate=1e-3,\n",
    "    encoder_units=[128, 64],\n",
    "    decoder_units=[128, 64],\n",
    "    num_outputs=1,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        \"cbm/base\"\n",
    "    ),\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    num_concepts=NUM_CONCEPTS,\n",
    "    latent_dims=0,\n",
    "    holdout_fraction=0.1,\n",
    "    train_samples=2000,\n",
    "    test_samples=1000,\n",
    "    covariances=np.arange(0, 1, 0.1),\n",
    "    verbosity=0,\n",
    "    delta_beta=0.05,\n",
    "    encoder_output_logits=False,\n",
    ")\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "cbm_base_results = cbm_experiment_loop(\n",
    "    cbm_base_experiment_config,\n",
    "    load_from_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(niching)\n",
    "reload(CBM)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "cbm_from_logits_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=300,\n",
    "    warmup_epochs=0,\n",
    "    pre_train_epochs=0,\n",
    "    trials=5,\n",
    "    alpha=0.1,\n",
    "    learning_rate=1e-3,\n",
    "    encoder_units=[128, 64],\n",
    "    decoder_units=[128, 64],\n",
    "    num_outputs=1,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        \"cbm/from_logits\"\n",
    "    ),\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    num_concepts=NUM_CONCEPTS,\n",
    "    latent_dims=0,\n",
    "    holdout_fraction=0.1,\n",
    "    train_samples=2000,\n",
    "    test_samples=1000,\n",
    "    covariances=np.arange(0, 1, 0.1),\n",
    "    verbosity=0,\n",
    "    delta_beta=0.05,\n",
    "    encoder_output_logits=True,\n",
    ")\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "cbm_from_logits_results = cbm_experiment_loop(\n",
    "    cbm_from_logits_experiment_config,\n",
    "    load_from_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept Whitening Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.CW.CWLayer as CW\n",
    "\n",
    "def construct_cw_model(\n",
    "    input_shape,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    learning_rate=1e-3,\n",
    "    activation=tf.keras.activations.relu,\n",
    "    activation_mode='max_pool_mean',\n",
    "):\n",
    "    model_inputs = tf.keras.Input(shape=input_shape)\n",
    "    cw_layer = CW.ConceptWhiteningLayer(\n",
    "        activation_mode=activation_mode,\n",
    "    )\n",
    "    cw_model = tf.keras.Model(\n",
    "        model_inputs,\n",
    "        cw_layer(encoder(model_inputs)),\n",
    "        name=\"cw_model\",\n",
    "    )\n",
    "    \n",
    "    # Now time to collapse all the concepts again back into a single vector\n",
    "    model = tf.keras.Model(\n",
    "        model_inputs,\n",
    "        decoder(activation(cw_layer(encoder(model_inputs)))),\n",
    "        name=\"complete_model\",\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[\"binary_accuracy\"],\n",
    "    )\n",
    "    return model, cw_model\n",
    "\n",
    "def channels_corr_mat(outputs):\n",
    "    if len(outputs.shape) == 2:\n",
    "        outputs = np.expand_dims(\n",
    "            np.expand_dims(outputs, axis=1),\n",
    "            axis=1,\n",
    "        )\n",
    "    # Change (N, H, W, C) to (C, N, H, W)\n",
    "    outputs = np.transpose(outputs, [3, 0, 1, 2])\n",
    "    # Change (C, N, H, W) to (C, NxHxW)\n",
    "    cnhw_shape = outputs.shape\n",
    "    outputs = np.transpose(np.reshape(outputs, [cnhw_shape[0], -1]))\n",
    "    outputs -= np.mean(outputs, axis=0, keepdims=True)\n",
    "    outputs = outputs / np.std(outputs, axis=0, keepdims=True)\n",
    "    return np.dot(outputs.transpose(), outputs) / outputs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.evaluation.metrics.leakage as leakage\n",
    "\n",
    "def concept_scores(\n",
    "    cw_layer,\n",
    "    inputs,\n",
    "    aggregator='max_pool_mean',\n",
    "    concept_indices=None,\n",
    "):\n",
    "    outputs = cw_layer(inputs, training=False)\n",
    "    if len(tf.shape(outputs)) == 2:\n",
    "        # Then the scores are already computed by our forward pass\n",
    "        scores = outputs\n",
    "    else:\n",
    "        if cw_layer.data_format == \"channels_last\":\n",
    "            # Then we will transpose to make things simpler so that\n",
    "            # downstream we can always assume it is channels first\n",
    "            # NHWC -> NCHW\n",
    "            outputs = tf.transpose(\n",
    "                outputs,\n",
    "                perm=[0, 3, 1, 2],\n",
    "            )\n",
    "\n",
    "        # Else, we need to do some aggregation\n",
    "        if aggregator == 'mean':\n",
    "            # Compute the mean over all channels\n",
    "            scores = tf.math.reduce_mean(outputs, axis=[2, 3])\n",
    "        elif aggregator == 'max_pool_mean':\n",
    "            # First downsample using a max pool and then continue with\n",
    "            # a mean\n",
    "            window_size = min(\n",
    "                2,\n",
    "                outputs.shape[-1],\n",
    "                outputs.shape[-2],\n",
    "            )\n",
    "            scores = tf.nn.max_pool(\n",
    "                outputs,\n",
    "                ksize=window_size,\n",
    "                strides=window_size,\n",
    "                padding=\"SAME\",\n",
    "                data_format=\"NCHW\",\n",
    "            )\n",
    "            scores = tf.math.reduce_mean(scores, axis=[2, 3])\n",
    "        elif aggregator == 'max':\n",
    "            # Simply select the maximum value across a given channel\n",
    "            scores = tf.math.reduce_max(outputs, axis=[2, 3])\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported aggregator {aggregator}.')\n",
    "\n",
    "    if concept_indices is not None:\n",
    "        return scores[:, concept_indices]\n",
    "    return scores\n",
    "\n",
    "\n",
    "def cw_experiment_loop(experiment_config, load_from_cache=False):\n",
    "    utils.reseed(87)\n",
    "    experiment_variables = dict(\n",
    "        config = experiment_config,\n",
    "        task_accuracies = [],\n",
    "        concept_accuracies = [],\n",
    "        niss=[],\n",
    "    )\n",
    "    res_dir = experiment_config['niching_results_dir']\n",
    "    if load_from_cache:\n",
    "        if os.path.exists(os.path.join(res_dir, 'results_niching.joblib')):\n",
    "            experiment_variables = load(os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            return experiment_variables\n",
    "        \n",
    "    count = 0\n",
    "    start_ind = 0\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"niching_results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    )#.mkdir(parents=True, exist_ok=True)\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    for cov in experiment_config[\"covariances\"][start_ind:]:\n",
    "        print(\"Training with covariance:\", cov)\n",
    "        \n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']}\")\n",
    "\n",
    "            # First construct the dataset\n",
    "            (x_train, y_train, y_train_concepts) = produce_data(\n",
    "                experiment_config[\"train_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            (x_test, y_test, y_test_concepts) = produce_data(\n",
    "                experiment_config[\"test_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            x_true_inds = (y_train_concepts[:, 0] == 1)\n",
    "            y_true_inds = (y_train_concepts[:, 1] == 1)\n",
    "            z_true_inds = (y_train_concepts[:, 2] == 1)\n",
    "            x_group_inds = np.logical_and(\n",
    "                x_true_inds,\n",
    "                np.logical_and(\n",
    "                    np.logical_not(y_true_inds),\n",
    "                    np.logical_not(z_true_inds),\n",
    "                )\n",
    "            )\n",
    "            y_group_inds = np.logical_and(\n",
    "                y_true_inds,\n",
    "                np.logical_and(\n",
    "                    np.logical_not(x_true_inds),\n",
    "                    np.logical_not(z_true_inds),\n",
    "                )\n",
    "            )\n",
    "            z_group_inds = np.logical_and(\n",
    "                z_true_inds,\n",
    "                np.logical_and(\n",
    "                    np.logical_not(x_true_inds),\n",
    "                    np.logical_not(y_true_inds),\n",
    "                )\n",
    "            )\n",
    "            exclusive_concept_groups = [\n",
    "                x_train[x_group_inds, :],\n",
    "                x_train[y_group_inds, :],\n",
    "                x_train[z_group_inds, :],\n",
    "            ][:experiment_config[\"data_concepts\"]]\n",
    "            \n",
    "            if not experiment_config.get(\"exclusive_concepts\", False):\n",
    "                x_group_inds = x_true_inds\n",
    "                y_group_inds = y_true_inds\n",
    "                z_group_inds = z_true_inds\n",
    "            concept_groups = [\n",
    "                x_train[x_group_inds, :],\n",
    "                x_train[y_group_inds, :],\n",
    "                x_train[z_group_inds, :],\n",
    "            ][:experiment_config[\"data_concepts\"]]\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Construct our CW model\n",
    "            encoder = construct_encoder(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                units=experiment_config[\"encoder_units\"],\n",
    "                num_concepts=experiment_config[\"num_concepts\"],\n",
    "                end_activation=None,\n",
    "                latent_dims=experiment_config[\"latent_dims\"],\n",
    "            )\n",
    "            decoder = construct_decoder(\n",
    "                units=experiment_config[\"decoder_units\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "            )\n",
    "            \n",
    "            model, cw_model = construct_cw_model(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                encoder=encoder,\n",
    "                decoder=decoder,\n",
    "                learning_rate=experiment_config[\"learning_rate\"],\n",
    "                activation=tf.keras.activations.relu,\n",
    "                activation_mode=experiment_config['activation_mode'],\n",
    "            )\n",
    "            \n",
    "            # First do some pretraining for warming up the estimates if needed\n",
    "            if experiment_config.get(\"pre_train_epochs\"):\n",
    "                print(\"\\tModel pre-training...\")\n",
    "                model.fit(\n",
    "                    x=x_train,\n",
    "                    y=y_train,\n",
    "                    epochs=experiment_config[\"pre_train_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "                fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "                similarity_ratio = oracle.concept_similarity_matrix(\n",
    "                    concept_representations=list(map(\n",
    "                        lambda x: cw_model(x).numpy(),\n",
    "                        concept_groups\n",
    "                    )),\n",
    "                    compute_ratios=True,\n",
    "                )\n",
    "                im, cbar = heatmap(\n",
    "                    similarity_ratio,\n",
    "                    [f\"$c_{i}$\" for i in range(len(concept_groups))],\n",
    "                    [f\"$c_{i}$\" for i in range(len(concept_groups))],\n",
    "                    ax=ax,\n",
    "                    cmap=\"magma\",\n",
    "                    cbarlabel=f\"Similarity Ratio\",\n",
    "                    vmin=0,\n",
    "                    vmax=1,\n",
    "                )\n",
    "                texts = annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "                fig.tight_layout()\n",
    "\n",
    "                fig.suptitle(f\"Baseline Concept Axis Separability\", fontsize=25)\n",
    "                fig.subplots_adjust(top=0.85)\n",
    "                plt.show()\n",
    "                print(\"\\t\\tModel pre-training completed\")\n",
    "            \n",
    "            # Set up the dataset in a nice usable form for unrolling the training\n",
    "            # loop\n",
    "            main_dataset_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            main_dataset_loader = main_dataset_loader.shuffle(buffer_size=1000).batch(\n",
    "                experiment_config[\"batch_size\"]\n",
    "            )\n",
    "            \n",
    "            min_size = min(list(map(lambda x: x.shape[0], concept_groups)))\n",
    "            concept_groups = list(map(lambda x: x[:min_size, :], concept_groups))\n",
    "            concept_group_loader = tf.data.Dataset.from_tensor_slices(tuple(concept_groups))\n",
    "            concept_group_loader = concept_group_loader.shuffle(buffer_size=1000).batch(\n",
    "                experiment_config[\"batch_size\"]\n",
    "            )\n",
    "\n",
    "            @tf.function\n",
    "            def _train_step(model, x_batch_train, y_batch_train):\n",
    "                # Update the other model parameters\n",
    "                with tf.GradientTape() as tape:\n",
    "                    logits = model(x_batch_train, training=True)\n",
    "                    loss_value = model.loss(y_batch_train, logits)\n",
    "\n",
    "                grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "                model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "                return loss_value\n",
    "            \n",
    "            total_steps = 0\n",
    "            for epoch in range(experiment_config[\"max_epochs\"]):\n",
    "                for current_step, (x_batch_train, y_batch_train) in enumerate(main_dataset_loader):\n",
    "                    print(\n",
    "                        f'Epoch {epoch + 1} and step {current_step}/{int(np.ceil(x_train.shape[0] / experiment_config[\"batch_size\"]))}         ',\n",
    "                        end=\"\\r\",\n",
    "                    )\n",
    "                    # Need to update the rotation matrix\n",
    "                    if (total_steps + 1) % experiment_config[\"cw_train_freq\"] == 0:\n",
    "                        for _ in range(experiment_config.get(\"cw_train_iterations\", 1)):\n",
    "                            cw_batch_steps = 0\n",
    "                            for concept_groups_batch in concept_group_loader:\n",
    "                                if cw_batch_steps > experiment_config.get(\"cw_train_batch_steps\", float(\"inf\")):\n",
    "                                    break\n",
    "                                model.layers[experiment_config[\"cw_layer\"]].update_rotation_matrix(\n",
    "                                    concept_groups=list(map(lambda x: encoder(x), concept_groups_batch)),\n",
    "                                )\n",
    "                                cw_batch_steps += 1\n",
    "                    if experiment_config.get(\"concept_auc_freq\"):\n",
    "                        if (total_steps % experiment_config[\"concept_auc_freq\"]) == 0:\n",
    "                            concept_aucs = leakage.compute_concept_aucs(\n",
    "                                cw_model=model,\n",
    "                                encoder=encoder,\n",
    "                                cw_layer=experiment_config[\"cw_layer\"],\n",
    "                                x_test=x_test,\n",
    "                                c_test=y_test_concepts,\n",
    "                                num_concepts=experiment_config[\"num_concepts\"],\n",
    "                            )\n",
    "                            print(\n",
    "                                f'Concept AUC at step {total_steps}:',\n",
    "                                concept_aucs\n",
    "                            )\n",
    "                    _train_step(model, x_batch_train, y_batch_train)\n",
    "                    total_steps += 1\n",
    "            \n",
    "            if experiment_config.get(\"post_cw_train_epochs\"):\n",
    "                for post_epoch in range(experiment_config.get(\"post_cw_train_epochs\", 0)):\n",
    "                    cw_batch_steps = 0\n",
    "                    steps_in_batch = len(concept_group_loader)\n",
    "                    for concept_groups_batch in concept_group_loader:\n",
    "                        print(\n",
    "                            f'Post epoch {post_epoch + 1} and step {cw_batch_steps}/{steps_in_batch}         ',\n",
    "                            end=\"\\r\",\n",
    "                        )\n",
    "                        model.layers[experiment_config[\"cw_layer\"]].update_rotation_matrix(\n",
    "                            concept_groups=list(map(lambda x: encoder(x), concept_groups_batch)),\n",
    "                        )\n",
    "                        cw_batch_steps += 1\n",
    "            \n",
    "            print(\"\\tEvaluating model\")\n",
    "            \n",
    "            # finding niches for several values of beta\n",
    "            niche_sizes = []\n",
    "            niche_impurities = []\n",
    "            # And estimate the area under the curve using the trapezoid method\n",
    "            total_area_under_curve_map = defaultdict(float)\n",
    "            prev_value_map = {}\n",
    "            delta_beta = experiment_config.get(\"delta_beta\", 0.05)\n",
    "            if not experiment_config['feature_map']:\n",
    "                c_train_pred = concept_scores(\n",
    "                    model.layers[2],\n",
    "                    encoder(x_train),\n",
    "                    aggregator=experiment_config['aggregator'],\n",
    "                ).numpy()\n",
    "                c_train_pred = c_train_pred[:, :experiment_config[\"num_concepts\"]]\n",
    "                c_test_pred = concept_scores(\n",
    "                    model.layers[2],\n",
    "                    encoder(x_test),\n",
    "                    aggregator=experiment_config['aggregator'],\n",
    "                ).numpy()[:, :experiment_config[\"num_concepts\"]]\n",
    "\n",
    "            else:\n",
    "                c_train_pred = encoder(x_train)\n",
    "                c_train_pred = c_train_pred[:, :, :, :experiment_config[\"num_concepts\"]]\n",
    "                c_test_pred = encoder(x_test)\n",
    "                c_test_pred = c_test_pred[:, :, :, :experiment_config[\"num_concepts\"]]\n",
    "                out_shape = c_train_pred.shape[1] * c_train_pred.shape[2]\n",
    "                c_train_pred = c_train_pred.numpy().reshape(-1, out_shape, c_train_pred.shape[3])\n",
    "                c_test_pred = c_test_pred.numpy().reshape(-1, out_shape, c_test_pred.shape[3])\n",
    "\n",
    "            experiment_variables['niss'].append(\n",
    "                niching.niche_impurity_score(\n",
    "                    c_soft=c_test_pred,\n",
    "                    c_true=y_test_concepts,\n",
    "                    c_soft_train=c_train_pred,\n",
    "                    c_true_train=y_train_concepts,\n",
    "                )\n",
    "            )\n",
    "            print(f'\\t\\tNIS: {experiment_variables[\"niss\"][-1]:.2f}')\n",
    "\n",
    "            os.makedirs(res_dir, exist_ok=True)\n",
    "            dump(experiment_variables, os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            \n",
    "        \n",
    "    return experiment_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(CW)\n",
    "reload(niching)\n",
    "reload(leakage)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "cw_base_experiment_config = dict(\n",
    "    batch_size=128,\n",
    "    max_epochs=300,\n",
    "    pre_train_epochs=0,\n",
    "    post_cw_train_epochs=300,\n",
    "    cw_train_freq=20,\n",
    "    cw_train_batch_steps=20,\n",
    "    learning_rate=1e-3,\n",
    "    encoder_units=[128, 64],\n",
    "    decoder_units=[128, 64],\n",
    "    num_outputs=1,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        \"cw/base\",\n",
    "    ),\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    num_concepts=NUM_CONCEPTS,\n",
    "    latent_dims=0,\n",
    "    covariances=np.arange(0, 1, 0.1),\n",
    "    train_samples=2000,\n",
    "    test_samples=1000,\n",
    "    trials=5,\n",
    "    verbosity=0,\n",
    "    data_concepts=NUM_CONCEPTS,\n",
    "    cw_layer=2,\n",
    "    aggregator='max_pool_mean',\n",
    "    activation_mode='max_pool_mean',\n",
    "    concept_auc_freq=0,\n",
    "    cw_train_iterations=1,\n",
    "    holdout_fraction=0.1,\n",
    "    exclusive_concepts=False,\n",
    "    feature_map=False,\n",
    "    add='',\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "cw_base_results = cw_experiment_loop(\n",
    "    cw_base_experiment_config,\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCD Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_ccd_encoder(\n",
    "    input_shape,\n",
    "    units,\n",
    "    end_activation=\"sigmoid\",\n",
    "    latent_dims=0,\n",
    "    latent_act=None,  # Original paper used \"sigmoid\" but this is troublesome in deep architectures\n",
    "):\n",
    "    encoder_inputs = tf.keras.Input(shape=input_shape)\n",
    "    encoder_compute_graph = encoder_inputs\n",
    "    for i, units in enumerate(units):\n",
    "        encoder_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(encoder_compute_graph)\n",
    "\n",
    "    # And finally map this to the number of concepts we have in our set\n",
    "    encoder_compute_graph = tf.keras.layers.Flatten()(encoder_compute_graph)\n",
    "    encoder_compute_graph = tf.keras.layers.Dense(\n",
    "        latent_dims,\n",
    "        activation=latent_act,\n",
    "        name=\"encoder_bypass_channel\",\n",
    "    )(encoder_compute_graph)\n",
    "\n",
    "    # Now time to collapse all the concepts again back into a single vector\n",
    "    encoder_model = tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        encoder_compute_graph,\n",
    "        name=\"encoder\",\n",
    "    )\n",
    "    return encoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.OCACE.topicModel as CCD\n",
    "\n",
    "############################################################################\n",
    "## Experiment loop\n",
    "############################################################################\n",
    "\n",
    "def ccd_experiment_loop(\n",
    "    experiment_config,\n",
    "    load_from_cache=False,\n",
    "):\n",
    "    experiment_variables = dict(\n",
    "        config = experiment_config,\n",
    "        niss=[],\n",
    "    )\n",
    "    num_concepts = experiment_config[\"num_concepts\"]\n",
    "    res_dir = experiment_config[\"niching_results_dir\"]\n",
    "    if load_from_cache:\n",
    "        if os.path.exists(os.path.join(res_dir, 'results_niching.joblib')):\n",
    "            experiment_variables = load(os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            return experiment_variables\n",
    "    \n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    count = 0\n",
    "    start_ind = 0\n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"niching_results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    )#.mkdir(parents=True, exist_ok=True)\n",
    "    for cov in experiment_config[\"covariances\"][start_ind:]:\n",
    "        print(\"Training with covariance:\", cov)\n",
    "        \n",
    "        channels_axis = (\n",
    "            -1 if experiment_config.get(\"data_format\", \"channels_last\") == \"channels_last\"\n",
    "            else 1\n",
    "        )\n",
    "        if experiment_config[\"num_outputs\"] == 1:\n",
    "            acc_fn = lambda y_true, y_pred: sklearn.metrics.roc_auc_score(\n",
    "                y_true,\n",
    "                y_pred\n",
    "            )\n",
    "        else:\n",
    "            acc_fn = lambda y_true, y_pred: sklearn.metrics.roc_auc_score(\n",
    "                tf.keras.utils.to_categorical(y_true),\n",
    "                scipy.special.softmax(y_pred, axis=-1),\n",
    "                multi_class='ovo',\n",
    "            )\n",
    "        \n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} for covariance {cov:.2f}\")\n",
    "            \n",
    "            count += 1\n",
    "\n",
    "            (x_train, y_train, c_train) = produce_data(\n",
    "                experiment_config[\"train_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            (x_test, y_test, c_test) = produce_data(\n",
    "                experiment_config[\"test_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            y_train = np.squeeze(y_train)\n",
    "            y_test = np.squeeze(y_test)\n",
    "            \n",
    "            # Proceed to do and end-to-end model in case we want to\n",
    "            # do some task-specific pretraining\n",
    "            end_to_end_model, encoder, decoder = construct_end_to_end_model(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                learning_rate=experiment_config[\"learning_rate\"],\n",
    "                encoder=construct_ccd_encoder(\n",
    "                    input_shape=experiment_config[\"input_shape\"],\n",
    "                    units=experiment_config[\"encoder_units\"],\n",
    "                    latent_act=experiment_config.get(\"latent_act\", None),\n",
    "                    latent_dims=experiment_config[\"latent_dims\"],\n",
    "                ),\n",
    "                decoder=construct_decoder(\n",
    "                    units=experiment_config[\"decoder_units\"],\n",
    "                    num_outputs=experiment_config[\"num_outputs\"],\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            print(\"\\tModel pre-training...\")\n",
    "            \n",
    "            early_stopping_monitor = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=experiment_config.get(\n",
    "                    \"early_stop_metric\",\n",
    "                    \"val_loss\",\n",
    "                ),\n",
    "                min_delta=experiment_config[\"min_delta\"],\n",
    "                patience=experiment_config[\"patience\"],\n",
    "                restore_best_weights=True,\n",
    "                verbose=2,\n",
    "                mode=experiment_config.get(\n",
    "                    \"early_stop_mode\",\n",
    "                    \"min\",\n",
    "                ),\n",
    "            )\n",
    "            end_to_end_model.fit(\n",
    "                x=x_train,\n",
    "                y=y_train,\n",
    "                epochs=experiment_config[\"max_epochs\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                callbacks=[\n",
    "                    early_stopping_monitor,\n",
    "                ],\n",
    "                validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                verbose=verbosity,\n",
    "            )\n",
    "            print(\"\\t\\tModel pre-training completed\")\n",
    "            print(\"\\tSerializing model\")\n",
    "            print(\"\\tEvaluating model\")\n",
    "            \n",
    "            test_result = end_to_end_model.evaluate(\n",
    "                x_test,\n",
    "                y_test,\n",
    "                verbose=0,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            # Now extract our concept vectors\n",
    "            if \"top_k\" not in experiment_config:\n",
    "                top_k = ccd_compute_k(y=y_train, batch_size=experiment_config[\"batch_size\"])\n",
    "            else:\n",
    "                top_k = experiment_config[\"top_k\"]\n",
    "            topic_model = CCD.TopicModel(\n",
    "                concepts_to_labels_model=decoder,\n",
    "                n_channels=experiment_config[\"latent_dims\"],\n",
    "                n_concepts=experiment_config[\"num_concepts\"],\n",
    "                threshold=experiment_config.get(\"threshold\", 0.5),\n",
    "                loss_fn=end_to_end_model.loss,\n",
    "                top_k=top_k,\n",
    "                lambda1=experiment_config.get(\"lambda1\", 0.1),\n",
    "                lambda2=experiment_config.get(\"lambda2\", 0.1),\n",
    "                seed=experiment_config.get(\"seed\", None),\n",
    "                eps=experiment_config.get(\"eps\", 1e-5),\n",
    "                data_format=experiment_config.get(\n",
    "                    \"data_format\",\n",
    "                    \"channels_last\"\n",
    "                ),\n",
    "                allow_gradient_flow_to_c2l=experiment_config.get(\n",
    "                    'allow_gradient_flow_to_c2l',\n",
    "                    False,\n",
    "                ),\n",
    "                acc_metric=(\n",
    "                    tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "                    if experiment_config[\"num_outputs\"] > 1 else\n",
    "                    tf.keras.metrics.BinaryAccuracy()\n",
    "                ),\n",
    "            )\n",
    "            topic_model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(\n",
    "                    experiment_config.get(\"learning_rate\", 1e-3),\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Train it for a few epochs\n",
    "            print(\"\\tTopic model training...\")\n",
    "            topic_model.fit(\n",
    "                x=encoder(x_train),\n",
    "                y=y_train,\n",
    "                epochs=experiment_config[\"topic_model_train_epochs\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                verbose=verbosity,\n",
    "            )\n",
    "            print(\"\\t\\tTopic model training completed\")\n",
    "            \n",
    "            enc = OneHotEncoder(sparse=False)\n",
    "            y_train = enc.fit_transform(y_train.reshape(-1, 1))\n",
    "            y_test = enc.fit_transform(y_test.reshape(-1, 1))\n",
    "            \n",
    "            c_train_pred = topic_model.concept_scores(encoder(x_train)).numpy()\n",
    "            c_test_pred = topic_model.concept_scores(encoder(x_test)).numpy()\n",
    "\n",
    "            print(\"\\t\\tComputing niching scores...\")\n",
    "            \n",
    "            experiment_variables['niss'].append(\n",
    "                niching.niche_impurity_score(\n",
    "                    c_soft=c_test_pred,\n",
    "                    c_true=c_test,\n",
    "                    c_soft_train=c_train_pred,\n",
    "                    c_true_train=c_train,\n",
    "                )\n",
    "            )\n",
    "            print(f'\\t\\tNIS: {experiment_variables[\"niss\"][-1]:.2f}')\n",
    "            os.makedirs(res_dir, exist_ok=True)\n",
    "            dump(experiment_variables, os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            \n",
    "    return experiment_variables\n",
    "\n",
    "def ccd_compute_k(y, batch_size):\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    avg_class_ratio = np.mean(counts) / y.shape[0]\n",
    "    return int((avg_class_ratio * batch_size) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(niching)\n",
    "reload(CBM)\n",
    "reload(CCD)\n",
    "reload(niching)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "ccd_base_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=300,\n",
    "    topic_model_train_epochs=50,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "\n",
    "    num_concepts=NUM_CONCEPTS,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    latent_dims=NUM_CONCEPTS,\n",
    "    encoder_units=[128, 64],\n",
    "    decoder_units=[128, 64],\n",
    "    latent_act=None,\n",
    "\n",
    "    threshold=0.0,\n",
    "    lambda1=0.1,\n",
    "    lambda2=0.1,\n",
    "    eps=1e-5,\n",
    "\n",
    "    num_outputs=1,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        \"ccd/base\",\n",
    "    ),\n",
    "    covariances=np.arange(0, 1, 0.1),\n",
    "    train_samples=2000,\n",
    "    test_samples=1000,\n",
    "    verbosity=0,\n",
    "    data_concepts=NUM_CONCEPTS,\n",
    "    cw_layer=2,\n",
    "    aggregator='max_pool_mean',\n",
    "    activation_mode='max_pool_mean',\n",
    "    cw_train_freq=20,\n",
    "    concept_auc_freq=0,\n",
    "    cw_train_iterations=1,\n",
    "    holdout_fraction=0.1,\n",
    "    exclusive_concepts=True,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "ccd_base_results = ccd_experiment_loop(\n",
    "    experiment_config=ccd_base_experiment_config,\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(CBM)\n",
    "reload(CCD)\n",
    "reload(niching)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "ccd_extended_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=300,\n",
    "    topic_model_train_epochs=50,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "\n",
    "    num_concepts=2*NUM_CONCEPTS,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    latent_dims=NUM_CONCEPTS,\n",
    "    encoder_units=[128, 64],\n",
    "    decoder_units=[128, 64],\n",
    "    latent_act=None,\n",
    "\n",
    "    threshold=0.0,\n",
    "    lambda1=0.1,\n",
    "    lambda2=0.1,\n",
    "    eps=1e-5,\n",
    "\n",
    "    num_outputs=1,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        \"ccd/extended\",\n",
    "    ),\n",
    "    covariances=np.arange(0, 1, 0.1),\n",
    "    train_samples=2000,\n",
    "    test_samples=1000,\n",
    "    verbosity=0,\n",
    "    data_concepts=NUM_CONCEPTS,\n",
    "    cw_layer=2,\n",
    "    aggregator='max_pool_mean',\n",
    "    activation_mode='max_pool_mean',\n",
    "    cw_train_freq=20,\n",
    "    concept_auc_freq=0,\n",
    "    cw_train_iterations=1,\n",
    "    holdout_fraction=0.1,\n",
    "    exclusive_concepts=True,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "ccd_extended_results = ccd_experiment_loop(\n",
    "    experiment_config=ccd_extended_experiment_config,\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENN Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.SENN.base_senn as SENN\n",
    "import concepts_xai.methods.SENN.aggregators as aggregators\n",
    "reload(SENN)\n",
    "reload(aggregators)\n",
    "\n",
    "\n",
    "def construct_senn_coefficient_model(units, num_concepts, num_outputs):\n",
    "    decoder_layers = [tf.keras.layers.Flatten()] + [\n",
    "        tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation=tf.nn.relu,\n",
    "            name=f\"coefficient_model_dense_{i+1}\",\n",
    "        ) for i, units in enumerate(units)\n",
    "    ]\n",
    "    return tf.keras.Sequential(decoder_layers + [\n",
    "        tf.keras.layers.Dense(\n",
    "            num_concepts * num_outputs,\n",
    "            activation=None,\n",
    "            name=\"coefficient_model_output\",\n",
    "        ),\n",
    "        tf.keras.layers.Reshape([num_outputs, num_concepts])\n",
    "    ])\n",
    "\n",
    "def construct_senn_encoder(\n",
    "    input_shape,\n",
    "    units,\n",
    "    end_activation=\"sigmoid\",\n",
    "    latent_dims=0,\n",
    "    latent_act=None,  # Original paper used \"sigmoid\" but this is troublesome in deep architectures\n",
    "):\n",
    "    encoder_inputs = tf.keras.Input(shape=input_shape)\n",
    "    encoder_compute_graph = encoder_inputs\n",
    "    for i, units in enumerate(units):\n",
    "        encoder_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(encoder_compute_graph)\n",
    "\n",
    "    mean = tf.keras.layers.Dense(latent_dims, activation=None, name=\"means\")(encoder_compute_graph)\n",
    "    log_var = tf.keras.layers.Dense(latent_dims, activation=None, name=\"log_var\")(encoder_compute_graph)\n",
    "    senn_encoder = tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        mean,\n",
    "        name=\"senn_encoder\",\n",
    "    )\n",
    "    vae_encoder = tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        [mean, log_var],\n",
    "        name=\"vae_encoder\",\n",
    "    )\n",
    "    return senn_encoder, vae_encoder\n",
    "\n",
    "\n",
    "def construct_vae_decoder(\n",
    "    units,\n",
    "    output_shape,\n",
    "    latent_dims,\n",
    "):\n",
    "    \"\"\"CNN decoder architecture used in the 'Challenging Common Assumptions in the Unsupervised Learning\n",
    "       of Disentangled Representations' paper (https://arxiv.org/abs/1811.12359)\n",
    "\n",
    "       Note: model is uncompiled\n",
    "    \"\"\"\n",
    "\n",
    "    latent_inputs = tf.keras.Input(shape=(latent_dims,))\n",
    "    model_out = latent_inputs\n",
    "    for unit in units:\n",
    "        model_out = tf.keras.layers.Dense(\n",
    "            unit,\n",
    "            activation='relu',\n",
    "        )(model_out)\n",
    "    model_out = tf.keras.layers.Dense(\n",
    "        output_shape,\n",
    "        activation=None,\n",
    "    )(model_out)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=latent_inputs,\n",
    "        outputs=[model_out],\n",
    "    )\n",
    "\n",
    "\n",
    "def construct_senn_model(\n",
    "    concept_encoder,\n",
    "    concept_decoder,\n",
    "    coefficient_model,\n",
    "    num_outputs,\n",
    "    regularization_strength=0.1,\n",
    "    learning_rate=1e-3,\n",
    "    sparsity_strength=2e-5,\n",
    "):\n",
    "    def reconstruction_loss_fn(y_true, y_pred):\n",
    "        return tf.reduce_sum(\n",
    "            tf.square(y_true - concept_decoder(y_pred)),\n",
    "            [-1]\n",
    "        )\n",
    "    senn_model = SENN.SelfExplainingNN(\n",
    "        encoder_model=concept_encoder,\n",
    "        coefficient_model=coefficient_model,\n",
    "        aggregator_fn=(\n",
    "            aggregators.multiclass_additive_aggregator if (num_outputs >= 2)\n",
    "            else aggregators.scalar_additive_aggregator\n",
    "        ),\n",
    "        task_loss_fn=(\n",
    "            tf.keras.losses.BinaryCrossentropy(from_logits=True) if (num_outputs < 2)\n",
    "            else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        ),\n",
    "        reconstruction_loss_fn=reconstruction_loss_fn,\n",
    "        regularization_strength=regularization_strength,\n",
    "        sparsity_strength=sparsity_strength,\n",
    "        name=\"SENN\",\n",
    "        metrics=[\n",
    "            tf.keras.metrics.BinaryAccuracy() if (num_outputs < 2)\n",
    "            else tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "        ],\n",
    "    )\n",
    "    senn_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    )\n",
    "    return senn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.evaluation.metrics.purity as purity\n",
    "import scipy\n",
    "\n",
    "############################################################################\n",
    "## Experiment loop\n",
    "############################################################################\n",
    "\n",
    "def construct_trivial_auc_mat(num_concepts):\n",
    "    result = np.ones((num_concepts, num_concepts), dtype=np.float32) * 0.5\n",
    "    return result + np.eye(num_concepts, dtype=np.float32) * 0.5\n",
    "    \n",
    "def get_argmax_concept_explanations(preds, class_theta_scores):\n",
    "    if len(preds.shape) == 1:\n",
    "        # Then we will always pick the same set of concept explanations as there is\n",
    "        # one or two classes only\n",
    "        inds = np.zeros(preds.shape, dtype=np.int32)\n",
    "    else:\n",
    "        inds = np.argmax(preds, axis=-1)\n",
    "    result = np.take_along_axis(\n",
    "        class_theta_scores,\n",
    "        np.expand_dims(np.expand_dims(inds, axis=-1), axis=-1),\n",
    "        axis=1,\n",
    "    )\n",
    "    return np.squeeze(result, axis=1)\n",
    "\n",
    "def senn_experiment_loop(\n",
    "    experiment_config,\n",
    "    load_from_cache=False,\n",
    "):\n",
    "    experiment_variables = dict(\n",
    "        config = experiment_config,\n",
    "        niss=[],\n",
    "    )\n",
    "    num_concepts = experiment_config[\"num_concepts\"]\n",
    "    res_dir = experiment_config['niching_results_dir']\n",
    "    if load_from_cache:\n",
    "        if os.path.exists(os.path.join(res_dir, 'results_niching.joblib')):\n",
    "            experiment_variables = load(os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            return experiment_variables\n",
    "    \n",
    "    start_ind = 0\n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            res_dir,\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    for cov in experiment_config[\"covariances\"][start_ind:]:\n",
    "        print(\"Training with covariance:\", cov)\n",
    "        \n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} for covariance {cov:.2f}\")\n",
    "            \n",
    "            (x_train, y_train, c_train) = produce_data(\n",
    "                experiment_config[\"train_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            (x_test, y_test, c_test) = produce_data(\n",
    "                experiment_config[\"test_samples\"],\n",
    "                cov=cov,\n",
    "                num_concepts=experiment_config[\"data_concepts\"],\n",
    "            )\n",
    "            y_train = np.squeeze(y_train)\n",
    "            y_test = np.squeeze(y_test)\n",
    "        \n",
    "            channels_axis = (\n",
    "                -1 if experiment_config.get(\"data_format\", \"channels_last\") == \"channels_last\"\n",
    "                else 1\n",
    "            )\n",
    "        \n",
    "            # Proceed to do and end-to-end model in case we want to\n",
    "            # do some task-specific pretraining\n",
    "            concept_encoder, vae_encoder = construct_senn_encoder(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                units=experiment_config[\"encoder_units\"],\n",
    "                latent_act=experiment_config.get(\"latent_act\", None),\n",
    "                latent_dims=experiment_config[\"latent_dims\"],\n",
    "            )\n",
    "            concept_decoder = construct_vae_decoder(\n",
    "                units=experiment_config[\"decoder_units\"],\n",
    "                output_shape=experiment_config[\"input_shape\"][-1],\n",
    "                latent_dims=experiment_config[\"latent_dims\"],\n",
    "            )\n",
    "            coefficient_model = construct_senn_coefficient_model(\n",
    "                units=experiment_config[\"coefficient_model_units\"],\n",
    "                num_concepts=experiment_config[\"latent_dims\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "            )\n",
    "            \n",
    "            if experiment_config.get(\"pretrain_autoencoder_epochs\"):\n",
    "                autoencoder = beta_vae.BetaVAE(\n",
    "                    encoder=vae_encoder,\n",
    "                    decoder=concept_decoder,\n",
    "                    loss_fn=vae_losses.bernoulli_fn_wrapper(),\n",
    "                    beta=experiment_config.get(\"beta\", 1),\n",
    "                )\n",
    "\n",
    "                autoencoder.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(\n",
    "                        experiment_config.get(\"learning_rate\", 1e-3)\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "                print(\"\\tAutoencoder pre-training...\")\n",
    "                autoencoder.fit(\n",
    "                    x=x_train,\n",
    "                    epochs=experiment_config[\"pretrain_autoencoder_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "                print(\"\\t\\tAutoencoder training completed\")\n",
    "\n",
    "            # Now time to actually construct and train the CBM\n",
    "            senn_model = construct_senn_model(\n",
    "                concept_encoder=concept_encoder,\n",
    "                concept_decoder=concept_decoder,\n",
    "                coefficient_model=coefficient_model,\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "                regularization_strength=experiment_config.get(\"regularization_strength\", 0.1),\n",
    "                learning_rate=experiment_config.get(\"learning_rate\", 1e-3),\n",
    "                sparsity_strength=experiment_config.get(\"sparsity_strength\", 2e-5),\n",
    "            )\n",
    "\n",
    "            early_stopping_monitor = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=experiment_config.get(\n",
    "                    \"early_stop_metric\",\n",
    "                    \"val_loss\",\n",
    "                ),\n",
    "                min_delta=experiment_config[\"min_delta\"],\n",
    "                patience=experiment_config[\"patience\"],\n",
    "                restore_best_weights=True,\n",
    "                verbose=2,\n",
    "                mode=experiment_config.get(\n",
    "                    \"early_stop_mode\",\n",
    "                    \"max\",\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            print(\"\\tSENN training...\")\n",
    "            senn_model.fit(\n",
    "                x=x_train,\n",
    "                y=y_train,\n",
    "                epochs=experiment_config[\"max_epochs\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                callbacks=[\n",
    "                    early_stopping_monitor,\n",
    "                ],\n",
    "                validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                verbose=verbosity,\n",
    "            )\n",
    "            print(\"\\t\\tSENN training completed\")\n",
    "\n",
    "            print(\"\\tEvaluating model\")\n",
    "            test_result = senn_model.evaluate(\n",
    "                x_test,\n",
    "                y_test,\n",
    "                verbose=0,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            task_acc = (\n",
    "                test_result['sparse_top_k_categorical_accuracy']\n",
    "                if experiment_config['num_outputs'] > 1 else\n",
    "                test_result['binary_accuracy']\n",
    "            )\n",
    "\n",
    "            if experiment_config['num_outputs'] > 1:\n",
    "                # Then lets apply a softmax activation over all the probability\n",
    "                # classes\n",
    "                preds = scipy.special.softmax(\n",
    "                    senn_model.predict(x_test)[0],\n",
    "                    axis=-1\n",
    "                )\n",
    "\n",
    "                # And select just the labels that are in fact being used\n",
    "                one_hot_labels = tf.keras.utils.to_categorical(y_test)\n",
    "                auc = (sklearn.metrics.roc_auc_score(\n",
    "                    one_hot_labels,\n",
    "                    preds,\n",
    "                    multi_class='ovo',\n",
    "                ))\n",
    "            else:\n",
    "                auc = (sklearn.metrics.roc_auc_score(\n",
    "                    y_test,\n",
    "                    senn_model.predict(x_test)[0],\n",
    "                ))\n",
    "\n",
    "            print(\n",
    "                f\"\\t\\tTest auc = {auc:.4f}, \"\n",
    "                f\"task accuracy = {task_acc:.4f}\"\n",
    "            )\n",
    "            \n",
    "            \n",
    "            \n",
    "            x_train_preds, (_, x_train_theta_class_scores) = senn_model(x_train)\n",
    "            c_train_pred = get_argmax_concept_explanations(\n",
    "                x_train_preds.numpy(),\n",
    "                x_train_theta_class_scores.numpy(),\n",
    "            )\n",
    "            \n",
    "            x_test_preds, (_, x_test_theta_class_scores) = senn_model(x_test)\n",
    "            c_test_pred = get_argmax_concept_explanations(\n",
    "                x_test_preds.numpy(),\n",
    "                x_test_theta_class_scores.numpy(),\n",
    "            )\n",
    "            \n",
    "            print(\"\\t\\tComputing niching scores...\")\n",
    "            \n",
    "            experiment_variables['niss'].append(\n",
    "                niching.niche_impurity_score(\n",
    "                    c_soft=c_test_pred,\n",
    "                    c_true=c_test,\n",
    "                    c_soft_train=c_train_pred,\n",
    "                    c_true_train=c_train,\n",
    "                )\n",
    "            )\n",
    "            print(f'\\t\\NIS: {experiment_variables[\"niss\"][-1]:.2f}')\n",
    "            \n",
    "            os.makedirs(res_dir, exist_ok=True)\n",
    "            dump(experiment_variables, os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            \n",
    "    return experiment_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(niching)\n",
    "reload(CBM)\n",
    "reload(SENN)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "senn_base_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=300,\n",
    "    topic_model_train_epochs=50,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    \n",
    "    num_concepts=NUM_CONCEPTS,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    latent_dims=10,\n",
    "    encoder_units=[128, 64],\n",
    "    decoder_units=[128, 64],\n",
    "    latent_act=None,\n",
    "    \n",
    "    coefficient_model_units=[64, 64],\n",
    "    latent_decoder_units=[64, 64],\n",
    "    \n",
    "    predictor_max_epochs=300,\n",
    "    \n",
    "    regularization_strength=0.1,\n",
    "    sparsity_strength=2e-5,\n",
    "    \n",
    "    num_outputs=2,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        \"senn/purity\",\n",
    "    ),\n",
    "    covariances=np.arange(0, 1, 0.1),\n",
    "    train_samples=2000,\n",
    "    test_samples=1000,\n",
    "    verbosity=0,\n",
    "    data_concepts=NUM_CONCEPTS,\n",
    "    cw_layer=2,\n",
    "    aggregator='max_pool_mean',\n",
    "    activation_mode='max_pool_mean',\n",
    "    cw_train_freq=20,\n",
    "    concept_auc_freq=0,\n",
    "    cw_train_iterations=1,\n",
    "    holdout_fraction=0.1,\n",
    "    exclusive_concepts=True,\n",
    ")\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "senn_base_results = senn_experiment_loop(\n",
    "    experiment_config=senn_base_experiment_config,\n",
    "    load_from_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(niching)\n",
    "reload(CBM)\n",
    "reload(SENN)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "senn_extended_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=300,\n",
    "    topic_model_train_epochs=50,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    \n",
    "    num_concepts=NUM_CONCEPTS*2,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    latent_dims=10,\n",
    "    encoder_units=[128, 64],\n",
    "    decoder_units=[128, 64],\n",
    "    latent_act=None,\n",
    "    \n",
    "    coefficient_model_units=[64, 64],\n",
    "    latent_decoder_units=[64, 64],\n",
    "    \n",
    "    predictor_max_epochs=300,\n",
    "    \n",
    "    regularization_strength=0.1,\n",
    "    sparsity_strength=2e-5,\n",
    "    \n",
    "    num_outputs=1,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        \"senn/purity_extended\",\n",
    "    ),\n",
    "    covariances=np.arange(0, 1, 0.1),\n",
    "    train_samples=2000,\n",
    "    test_samples=1000,\n",
    "    verbosity=0,\n",
    "    data_concepts=NUM_CONCEPTS,\n",
    "    cw_layer=2,\n",
    "    aggregator='max_pool_mean',\n",
    "    activation_mode='max_pool_mean',\n",
    "    cw_train_freq=20,\n",
    "    concept_auc_freq=0,\n",
    "    cw_train_iterations=1,\n",
    "    holdout_fraction=0.1,\n",
    "    exclusive_concepts=True,\n",
    ")\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "senn_extended_results = senn_experiment_loop(\n",
    "    experiment_config=senn_extended_experiment_config,\n",
    "    load_from_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
