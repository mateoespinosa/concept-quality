{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dIcmOacShy-p",
    "outputId": "e47e6328-6477-4d52-d914-4dbf25f7fb6b"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xD3cPbsJhy-r"
   },
   "source": [
    "# Integrated Concept Niching Shapes3D Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwclGfwnhy-s"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYPJQZB-hy-t"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib\n",
    "import concepts_xai\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import rc\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "from tensorflow.keras.models import load_model\n",
    "from joblib import dump, load\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import utils\n",
    "import model_utils\n",
    "\n",
    "from concepts_xai.evaluation.metrics.niching import niche_completeness\n",
    "from concepts_xai.evaluation.metrics.niching import niche_completeness_ratio\n",
    "from concepts_xai.evaluation.metrics.niching import niche_purity\n",
    "from concepts_xai.evaluation.metrics.niching import niche_finding\n",
    "import concepts_xai.evaluation.metrics.niching as niching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktrTSqfchy-u"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "## Set seeds up for reproducibility\n",
    "################################################################################\n",
    "\n",
    "utils.reseed(87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "## Global Variables Defining Experiment Flow\n",
    "################################################################################\n",
    "\n",
    "FROM_CACHE = True\n",
    "_LATEX_SYMBOL = \"$\"\n",
    "BASE_DIR = '.'\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"results/shapes3d\")\n",
    "NICHING_RESULTS_DIR = os.path.join(BASE_DIR, \"results_concept_niching_integrated/shapes3d\")\n",
    "DATASETS_DIR = os.path.join(BASE_DIR, \"results/shapes3d\", \"datasets/\")\n",
    "Path(DATASETS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "rc('text', usetex=(_LATEX_SYMBOL == \"$\"))\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold_text(x):\n",
    "    if _LATEX_SYMBOL == \"$\":\n",
    "        return r\"$\\textbf{\" + x + \"}$\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRdZhyY9hy-v"
   },
   "source": [
    "# Graph Dependency Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "shapes3d_train_ds = tfds.load('shapes3d', split='train', shuffle_files=True)\n",
    "\n",
    "def relabel_categorical(x):\n",
    "    if len(x.shape) == 1:\n",
    "        _, unique_inds = np.unique(\n",
    "            x,\n",
    "            return_inverse=True,\n",
    "        )\n",
    "        return unique_inds\n",
    "    \n",
    "    result = x[:, :]\n",
    "    for i in range(x.shape[-1]):\n",
    "        _, unique_inds = np.unique(\n",
    "            x[:, i],\n",
    "            return_inverse=True,\n",
    "        )\n",
    "        result[:, i] = unique_inds\n",
    "    return result\n",
    "\n",
    "def cardinality_encoding(card_group_1, card_group_2):\n",
    "    result_to_encoding = {}\n",
    "    for i in card_group_1:\n",
    "        for j in card_group_2:\n",
    "            result_to_encoding[(i, j)] = len(result_to_encoding)\n",
    "    return result_to_encoding\n",
    "\n",
    "def extract_data(\n",
    "    filter_fn,\n",
    "    sample_map_fn=lambda x: x,\n",
    "    concept_map_fn=lambda x: x,\n",
    "    step=1,\n",
    "    label_fn=lambda ex: ex['label_shape'] * 8 + ex['label_scale'],\n",
    "    dataset_path=None,\n",
    "    force_rerun=False,\n",
    "):\n",
    "    if (not force_rerun) and dataset_path and os.path.exists(dataset_path):\n",
    "        # Them time to load up this dataset!\n",
    "        ds = np.load(dataset_path)\n",
    "        return ds[\"X\"], ds[\"y\"], ds[\"c\"]\n",
    "    num_entries = len(shapes3d_train_ds)\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    c_train = []\n",
    "    for i, ex in enumerate(tfds.as_numpy(\n",
    "        shapes3d_train_ds.shuffle(buffer_size=15000)\n",
    "    )):\n",
    "        if i % step != 0:\n",
    "            continue\n",
    "        concepts = [\n",
    "            ex['label_floor_hue'],\n",
    "            ex['label_wall_hue'],\n",
    "            ex['label_object_hue'],\n",
    "            ex['label_scale'],\n",
    "            ex['label_shape'],\n",
    "            ex['label_orientation'],\n",
    "        ]\n",
    "        if not filter_fn(concepts):\n",
    "            continue\n",
    "        print(i, end=\"\\r\")\n",
    "\n",
    "        x_train.append(sample_map_fn(ex['image']))\n",
    "        y_train.append(label_fn(ex))\n",
    "        c_train.append(concept_map_fn(concepts))\n",
    "    x_train = np.stack(x_train, axis=0) / 255.0\n",
    "    y_train = relabel_categorical(np.stack(y_train, axis=0))\n",
    "    c_train = relabel_categorical(np.stack(c_train, axis=0))\n",
    "    if dataset_path:\n",
    "        # Then serialize it to speed up things next time\n",
    "        np.savez(\n",
    "            dataset_path,\n",
    "            X=x_train,\n",
    "            y=y_train,\n",
    "            c=c_train,\n",
    "        )\n",
    "    return x_train, y_train, c_train\n",
    "\n",
    "############################################################################\n",
    "## Construct a binary concept task in the shapes3D dataset \n",
    "############################################################################\n",
    "\n",
    "def count_class_balance(y):\n",
    "    one_hot = tf.keras.utils.to_categorical(y)\n",
    "    return np.sum(one_hot, axis=0) / one_hot.shape[0]\n",
    "\n",
    "def multiclass_task_bin_concepts_map_fn(concepts):\n",
    "    return [\n",
    "        int(concepts[0] < 5),\n",
    "        int(concepts[1] < 5),\n",
    "        int(concepts[2] < 5),\n",
    "        int(concepts[3] < 4),\n",
    "        int(concepts[4] < 2),\n",
    "        int(concepts[5] < 7),\n",
    "    ]\n",
    "\n",
    "    \n",
    "def multiclass_task_bin_concepts_label_fn(concept_dict):\n",
    "    concept_vector = multiclass_task_bin_concepts_map_fn([\n",
    "        concept_dict['label_floor_hue'],\n",
    "        concept_dict['label_wall_hue'],\n",
    "        concept_dict['label_object_hue'],\n",
    "        concept_dict['label_scale'],\n",
    "        concept_dict['label_shape'],\n",
    "        concept_dict['label_orientation'],\n",
    "    ])\n",
    "    binary_label_encoding = [\n",
    "        concept_vector[0] or concept_vector[1],\n",
    "        concept_vector[2] or concept_vector[3],\n",
    "        concept_vector[4] or concept_vector[5],\n",
    "    ]\n",
    "    return int(\n",
    "        \"\".join(list(map(str, binary_label_encoding))),\n",
    "        2\n",
    "    )\n",
    "\n",
    "\n",
    "def filter_fn_dep_0(concept):\n",
    "    ranges = [\n",
    "        list(range(0, 10, 2)),\n",
    "        list(range(0, 10, 2)),\n",
    "        list(range(0, 10, 2)),\n",
    "        list(range(0, 8)),\n",
    "        list(range(4)),\n",
    "        list(range(0, 15, 4)),\n",
    "    ]\n",
    "    \n",
    "    concept_vector = multiclass_task_bin_concepts_map_fn(concept)\n",
    "\n",
    "    # First filter as in small dataset to constraint the size of the data a bit\n",
    "    return all([\n",
    "        (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "    ])\n",
    "\n",
    "\n",
    "floor_hue_wall_hue_sets_lower = [\n",
    "    list(np.random.permutation(7))[:5]\n",
    "    for _ in range(10)\n",
    "]\n",
    "\n",
    "floor_hue_wall_hue_sets_upper = [\n",
    "    list(3 + np.random.permutation(7))[:5]\n",
    "    for _ in range(10)\n",
    "]\n",
    "\n",
    "def filter_fn_dep_1(concept):\n",
    "    ranges = [\n",
    "        list(range(0, 10)),\n",
    "        list(range(0, 10, 2)),\n",
    "        list(range(0, 10, 2)),\n",
    "        list(range(0, 8)),\n",
    "        list(range(4)),\n",
    "        list(range(0, 15, 4)),\n",
    "    ]\n",
    "    \n",
    "    concept_vector = multiclass_task_bin_concepts_map_fn(concept)\n",
    "\n",
    "    # First filter as in small dataset to constraint the size of the data a bit\n",
    "    if not all([\n",
    "        (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "    ]):\n",
    "        return False\n",
    "    if concept[0] < 5:\n",
    "        return concept[1] in floor_hue_wall_hue_sets_lower[concept[0]]\n",
    "    else:\n",
    "        return (concept[1] in floor_hue_wall_hue_sets_upper[concept[0]])\n",
    "\n",
    "\n",
    "wall_hue_object_hue_sets_lower = [\n",
    "    list(np.random.permutation(7))[:5]\n",
    "    for _ in range(10)\n",
    "]\n",
    "wall_hue_object_hue_sets_upper = [\n",
    "    list(3 + np.random.permutation(7))[:5]\n",
    "    for _ in range(10)\n",
    "]\n",
    "\n",
    "def filter_fn_dep_2(concept):\n",
    "    ranges = [\n",
    "        list(range(0, 10)),\n",
    "        list(range(0, 10)),\n",
    "        list(range(0, 10, 2)),\n",
    "        list(range(0, 8)),\n",
    "        list(range(4)),\n",
    "        list(range(0, 15, 4)),\n",
    "    ]\n",
    "    \n",
    "    concept_vector = multiclass_task_bin_concepts_map_fn(concept)\n",
    "    \n",
    "    # First filter as in small dataset to constraint the size of the data a bit\n",
    "    if not all([\n",
    "        (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "    ]):\n",
    "        return False\n",
    "    \n",
    "    if (concept[0] < 5):\n",
    "        if concept[1] not in floor_hue_wall_hue_sets_lower[concept[0]]:\n",
    "            return False\n",
    "    else:\n",
    "        if (concept[1] not in floor_hue_wall_hue_sets_upper[concept[0]]):\n",
    "            return False\n",
    "    \n",
    "    if (concept[1] < 5):\n",
    "        if concept[2] not in wall_hue_object_hue_sets_lower[concept[1]]:\n",
    "            return False\n",
    "    else:\n",
    "        if (concept[2] not in wall_hue_object_hue_sets_upper[concept[1]]):\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "\n",
    "\n",
    "object_hue_scale_sets_lower = [\n",
    "    list(np.random.permutation(6))[:4]\n",
    "    for _ in range(10)\n",
    "]\n",
    "\n",
    "object_hue_scale_sets_upper = [\n",
    "    list(2 + np.random.permutation(6))[:4]\n",
    "    for _ in range(10)\n",
    "]\n",
    "\n",
    "def filter_fn_dep_3(concept):\n",
    "    ranges = [\n",
    "        list(range(0, 10)),\n",
    "        list(range(0, 10)),\n",
    "        list(range(0, 10)),\n",
    "        list(range(0, 8)),\n",
    "        list(range(4)),\n",
    "        list(range(0, 15, 4)),\n",
    "    ]\n",
    "    \n",
    "    concept_vector = multiclass_task_bin_concepts_map_fn(concept)\n",
    "\n",
    "    # First filter as in small dataset to constraint the size of the data a bit\n",
    "    if not all([\n",
    "        (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "    ]):\n",
    "        return False\n",
    "    \n",
    "    if (concept[0] < 5):\n",
    "        if concept[1] not in floor_hue_wall_hue_sets_lower[concept[0]]:\n",
    "            return False\n",
    "    else:\n",
    "        if (concept[1] not in floor_hue_wall_hue_sets_upper[concept[0]]):\n",
    "            return False\n",
    "    \n",
    "    if (concept[1] < 5):\n",
    "        if concept[2] not in wall_hue_object_hue_sets_lower[concept[1]]:\n",
    "            return False\n",
    "    else:\n",
    "        if (concept[2] not in wall_hue_object_hue_sets_upper[concept[1]]):\n",
    "            return False\n",
    "    \n",
    "    if (concept[2] < 5):\n",
    "        if concept[3] not in object_hue_scale_sets_lower[concept[2]]:\n",
    "            return False\n",
    "    else:\n",
    "        if (concept[3] not in object_hue_scale_sets_upper[concept[2]]):\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "\n",
    "scale_shape_sets_lower = [\n",
    "    list(np.random.permutation(3))[:2]\n",
    "    for _ in range(8)\n",
    "]\n",
    "\n",
    "scale_shape_sets_upper = [\n",
    "    list(1 + np.random.permutation(3))[:2]\n",
    "    for _ in range(8)\n",
    "]\n",
    "\n",
    "def filter_fn_dep_4(concept):\n",
    "    ranges = [\n",
    "        list(range(0, 10)),\n",
    "        list(range(0, 10)),\n",
    "        list(range(0, 10)),\n",
    "        list(range(0, 8)),\n",
    "        list(range(4)),\n",
    "        list(range(0, 15, 2)),\n",
    "    ]\n",
    "    \n",
    "    concept_vector = multiclass_task_bin_concepts_map_fn(concept)\n",
    "    # First filter as in small dataset to constraint the size of the data a bit\n",
    "    if not all([\n",
    "        (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "    ]):\n",
    "        return False\n",
    "    \n",
    "    if (concept[0] < 5):\n",
    "        if concept[1] not in floor_hue_wall_hue_sets_lower[concept[0]]:\n",
    "            return False\n",
    "    else:\n",
    "        if (concept[1] not in floor_hue_wall_hue_sets_upper[concept[0]]):\n",
    "            return False\n",
    "    \n",
    "    if (concept[1] < 5):\n",
    "        if concept[2] not in wall_hue_object_hue_sets_lower[concept[1]]:\n",
    "            return False\n",
    "    else:\n",
    "        if (concept[2] not in wall_hue_object_hue_sets_upper[concept[1]]):\n",
    "            return False\n",
    "    \n",
    "    if (concept[2] < 5):\n",
    "        if concept[3] not in object_hue_scale_sets_lower[concept[2]]:\n",
    "            return False\n",
    "    else:\n",
    "        if (concept[3] not in object_hue_scale_sets_upper[concept[2]]):\n",
    "            return False\n",
    "    \n",
    "    if (concept[3] < 4):\n",
    "        if concept[4] not in scale_shape_sets_lower[concept[3]]:\n",
    "            return False\n",
    "    else:\n",
    "        if (concept[4] not in scale_shape_sets_upper[concept[3]]):\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "    \n",
    "    \n",
    "shape_rotation_sets_lower = [\n",
    "    list(np.random.permutation(9))[:7]\n",
    "    for _ in range(4)\n",
    "]\n",
    "\n",
    "shape_rotation_sets_upper = [\n",
    "    list(6 + np.random.permutation(9))[:7]\n",
    "    for _ in range(4)\n",
    "]\n",
    "\n",
    "def filter_fn_dep_5(concept):\n",
    "    ranges = [\n",
    "        list(range(0, 10)),\n",
    "        list(range(0, 10)),\n",
    "        list(range(0, 10)),\n",
    "        list(range(0, 8)),\n",
    "        list(range(4)),\n",
    "        list(range(0, 15)),\n",
    "    ]\n",
    "    \n",
    "    concept_vector = multiclass_task_bin_concepts_map_fn(concept)\n",
    "\n",
    "    # First filter as in small dataset to constraint the size of the data a bit\n",
    "    if not all([\n",
    "        (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "    ]):\n",
    "        return False\n",
    "    \n",
    "    if (concept[0] < 5):\n",
    "        if concept[1] not in floor_hue_wall_hue_sets_lower[concept[0]]:\n",
    "            return False\n",
    "    else:\n",
    "        if (concept[1] not in floor_hue_wall_hue_sets_upper[concept[0]]):\n",
    "            return False\n",
    "    \n",
    "    if (concept[1] < 5):\n",
    "        if concept[2] not in wall_hue_object_hue_sets_lower[concept[1]]:\n",
    "            return False\n",
    "    else:\n",
    "        if (concept[2] not in wall_hue_object_hue_sets_upper[concept[1]]):\n",
    "            return False\n",
    "    \n",
    "    if (concept[2] < 5):\n",
    "        if concept[3] not in object_hue_scale_sets_lower[concept[2]]:\n",
    "            return False\n",
    "    else:\n",
    "        if (concept[3] not in object_hue_scale_sets_upper[concept[2]]):\n",
    "            return False\n",
    "    \n",
    "    if (concept[3] < 4):\n",
    "        if concept[4] not in scale_shape_sets_lower[concept[3]]:\n",
    "            return False\n",
    "    else:\n",
    "        if (concept[4] not in scale_shape_sets_upper[concept[3]]):\n",
    "            return False\n",
    "        \n",
    "    if (concept[4] < 2):\n",
    "        if concept[5] not in shape_rotation_sets_lower[concept[4]]:\n",
    "            return False\n",
    "    else:\n",
    "        if (concept[5] not in shape_rotation_sets_upper[concept[4]]):\n",
    "            return False\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_multiclass_task_bin_concepts_label_fn(concept_dict):\n",
    "    concept_vector = multiclass_task_bin_concepts_map_fn([\n",
    "        concept_dict['label_floor_hue'],\n",
    "        concept_dict['label_wall_hue'],\n",
    "        concept_dict['label_object_hue'],\n",
    "        concept_dict['label_scale'],\n",
    "        concept_dict['label_shape'],\n",
    "        concept_dict['label_orientation'],\n",
    "    ])\n",
    "    if concept_vector[4] == 0:\n",
    "        offset = 0\n",
    "        binary_label_encoding = [\n",
    "            concept_vector[0],\n",
    "            concept_vector[1],\n",
    "        ]\n",
    "    else:\n",
    "        offset = 4\n",
    "        binary_label_encoding = [\n",
    "            concept_vector[2],\n",
    "            concept_vector[3],\n",
    "            concept_vector[5],\n",
    "        ]\n",
    "        \n",
    "    return offset + int(\n",
    "        \"\".join(list(map(str, binary_label_encoding))),\n",
    "        2\n",
    "    )\n",
    "\n",
    "balanced_multiclass_task_bin_concepts_dep_0_x_train, balanced_multiclass_task_bin_concepts_dep_0_y_train, balanced_multiclass_task_bin_concepts_dep_0_c_train = extract_data(\n",
    "    filter_fn_dep_0,\n",
    "    dataset_path=os.path.join(DATASETS_DIR, \"balanced_multiclass_task_bin_concepts_dep_0_concepts.npz\"),\n",
    "    concept_map_fn=multiclass_task_bin_concepts_map_fn,\n",
    "    label_fn=balanced_multiclass_task_bin_concepts_label_fn,\n",
    "#     force_rerun=True,\n",
    ")\n",
    "\n",
    "balanced_multiclass_task_bin_concepts_dep_0_x_train, balanced_multiclass_task_bin_concepts_dep_0_x_test, balanced_multiclass_task_bin_concepts_dep_0_y_train, balanced_multiclass_task_bin_concepts_dep_0_y_test, balanced_multiclass_task_bin_concepts_dep_0_c_train, balanced_multiclass_task_bin_concepts_dep_0_c_test = train_test_split(\n",
    "    balanced_multiclass_task_bin_concepts_dep_0_x_train,\n",
    "    balanced_multiclass_task_bin_concepts_dep_0_y_train,\n",
    "    balanced_multiclass_task_bin_concepts_dep_0_c_train,\n",
    "    test_size=0.2,\n",
    ")\n",
    "\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_0_x_train.shape =\", balanced_multiclass_task_bin_concepts_dep_0_x_train.shape)\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_0_x_train label distribution:\", count_class_balance(balanced_multiclass_task_bin_concepts_dep_0_y_train))\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_0_x_train concept distribution:\", np.sum(balanced_multiclass_task_bin_concepts_dep_0_c_train, axis=0)/balanced_multiclass_task_bin_concepts_dep_0_c_train.shape[0])\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_0_x_train exclusive concept distribution:\", np.sum(balanced_multiclass_task_bin_concepts_dep_0_c_train[np.sum(balanced_multiclass_task_bin_concepts_dep_0_c_train, axis=-1) == 1], axis=0)/balanced_multiclass_task_bin_concepts_dep_0_c_train.shape[0])\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_0_x_test.shape =\", balanced_multiclass_task_bin_concepts_dep_0_x_test.shape)\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_0_c_test.shape =\", balanced_multiclass_task_bin_concepts_dep_0_c_test.shape)\n",
    "balanced_multiclass_task_bin_concepts_dep_0_train = (balanced_multiclass_task_bin_concepts_dep_0_x_train, balanced_multiclass_task_bin_concepts_dep_0_y_train, balanced_multiclass_task_bin_concepts_dep_0_c_train)\n",
    "balanced_multiclass_task_bin_concepts_dep_0_test = (balanced_multiclass_task_bin_concepts_dep_0_x_test, balanced_multiclass_task_bin_concepts_dep_0_y_test, balanced_multiclass_task_bin_concepts_dep_0_c_test)\n",
    "\n",
    "dep_0_corr_mat = np.ones(\n",
    "    (\n",
    "        balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_train[1]))\n",
    "    )\n",
    ")\n",
    "for c in range(dep_0_corr_mat.shape[0]):\n",
    "    for l in range(dep_0_corr_mat.shape[1]):\n",
    "        dep_0_corr_mat[c][l] = np.corrcoef(\n",
    "            balanced_multiclass_task_bin_concepts_dep_0_train[2][:, c],\n",
    "            (balanced_multiclass_task_bin_concepts_dep_0_train[1] == l).astype(np.int32),\n",
    "        )[0, 1]\n",
    "fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "im, cbar = utils.heatmap(\n",
    "    np.abs(dep_0_corr_mat),\n",
    "    [f\"$c_{i}$\" for i in range(dep_0_corr_mat.shape[0])],\n",
    "    [f\"$l_{i}$\" for i in range(dep_0_corr_mat.shape[1])],\n",
    "    ax=ax,\n",
    "    cmap=\"magma\",\n",
    "    cbarlabel=f\"Correlation Coef\",\n",
    "    vmin=0, #-1,\n",
    "    vmax=1,\n",
    ")\n",
    "texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.suptitle(f\"3dshapes Concept-Label Absolute Correlations ($\\lambda = 0$)\", fontsize=25)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "plt.show()\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "balanced_multiclass_task_bin_concepts_dep_1_x_train, balanced_multiclass_task_bin_concepts_dep_1_y_train, balanced_multiclass_task_bin_concepts_dep_1_c_train = extract_data(\n",
    "    filter_fn_dep_1,\n",
    "    dataset_path=os.path.join(DATASETS_DIR, \"balanced_multiclass_task_bin_concepts_dep_1_concepts.npz\"),\n",
    "    concept_map_fn=multiclass_task_bin_concepts_map_fn,\n",
    "    label_fn=balanced_multiclass_task_bin_concepts_label_fn,\n",
    "#     force_rerun=True,\n",
    ")\n",
    "\n",
    "balanced_multiclass_task_bin_concepts_dep_1_x_train, balanced_multiclass_task_bin_concepts_dep_1_x_test, balanced_multiclass_task_bin_concepts_dep_1_y_train, balanced_multiclass_task_bin_concepts_dep_1_y_test, balanced_multiclass_task_bin_concepts_dep_1_c_train, balanced_multiclass_task_bin_concepts_dep_1_c_test = train_test_split(\n",
    "    balanced_multiclass_task_bin_concepts_dep_1_x_train,\n",
    "    balanced_multiclass_task_bin_concepts_dep_1_y_train,\n",
    "    balanced_multiclass_task_bin_concepts_dep_1_c_train,\n",
    "    test_size=0.2,\n",
    ")\n",
    "\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_1_x_train.shape =\", balanced_multiclass_task_bin_concepts_dep_1_x_train.shape)\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_1_x_train label distribution:\", count_class_balance(balanced_multiclass_task_bin_concepts_dep_1_y_train))\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_1_x_train concept distribution:\", np.sum(balanced_multiclass_task_bin_concepts_dep_1_c_train, axis=0)/balanced_multiclass_task_bin_concepts_dep_1_c_train.shape[0])\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_1_x_train exclusive concept distribution:\", np.sum(balanced_multiclass_task_bin_concepts_dep_1_c_train[np.sum(balanced_multiclass_task_bin_concepts_dep_1_c_train, axis=-1) == 1], axis=0)/balanced_multiclass_task_bin_concepts_dep_1_c_train.shape[0])\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_1_x_test.shape =\", balanced_multiclass_task_bin_concepts_dep_1_x_test.shape)\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_1_c_test.shape =\", balanced_multiclass_task_bin_concepts_dep_1_c_test.shape)\n",
    "balanced_multiclass_task_bin_concepts_dep_1_train = (balanced_multiclass_task_bin_concepts_dep_1_x_train, balanced_multiclass_task_bin_concepts_dep_1_y_train, balanced_multiclass_task_bin_concepts_dep_1_c_train)\n",
    "balanced_multiclass_task_bin_concepts_dep_1_test = (balanced_multiclass_task_bin_concepts_dep_1_x_test, balanced_multiclass_task_bin_concepts_dep_1_y_test, balanced_multiclass_task_bin_concepts_dep_1_c_test)\n",
    "\n",
    "dep_1_corr_mat = np.ones(\n",
    "    (\n",
    "        balanced_multiclass_task_bin_concepts_dep_1_train[2].shape[-1],\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_1_train[1]))\n",
    "    )\n",
    ")\n",
    "for c in range(dep_1_corr_mat.shape[0]):\n",
    "    for l in range(dep_1_corr_mat.shape[1]):\n",
    "        dep_1_corr_mat[c][l] = np.corrcoef(\n",
    "            balanced_multiclass_task_bin_concepts_dep_1_train[2][:, c],\n",
    "            (balanced_multiclass_task_bin_concepts_dep_1_train[1] == l).astype(np.int32),\n",
    "        )[0, 1]\n",
    "fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "im, cbar = utils.heatmap(\n",
    "    np.abs(dep_1_corr_mat),\n",
    "    [f\"$c_{i}$\" for i in range(dep_1_corr_mat.shape[0])],\n",
    "    [f\"$l_{i}$\" for i in range(dep_1_corr_mat.shape[1])],\n",
    "    ax=ax,\n",
    "    cmap=\"magma\",\n",
    "    cbarlabel=f\"Correlation Coef\",\n",
    "    vmin=0, #-1,\n",
    "    vmax=1,\n",
    ")\n",
    "texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.suptitle(f\"3dshapes Concept-Label Absolute Correlations ($\\lambda = 1$)\", fontsize=25)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "plt.show()\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "balanced_multiclass_task_bin_concepts_dep_2_x_train, balanced_multiclass_task_bin_concepts_dep_2_y_train, balanced_multiclass_task_bin_concepts_dep_2_c_train = extract_data(\n",
    "    filter_fn_dep_2,\n",
    "    dataset_path=os.path.join(DATASETS_DIR, \"balanced_multiclass_task_bin_concepts_dep_2_concepts.npz\"),\n",
    "    concept_map_fn=multiclass_task_bin_concepts_map_fn,\n",
    "    label_fn=balanced_multiclass_task_bin_concepts_label_fn,\n",
    "#     force_rerun=True,\n",
    ")\n",
    "\n",
    "balanced_multiclass_task_bin_concepts_dep_2_x_train, balanced_multiclass_task_bin_concepts_dep_2_x_test, balanced_multiclass_task_bin_concepts_dep_2_y_train, balanced_multiclass_task_bin_concepts_dep_2_y_test, balanced_multiclass_task_bin_concepts_dep_2_c_train, balanced_multiclass_task_bin_concepts_dep_2_c_test = train_test_split(\n",
    "    balanced_multiclass_task_bin_concepts_dep_2_x_train,\n",
    "    balanced_multiclass_task_bin_concepts_dep_2_y_train,\n",
    "    balanced_multiclass_task_bin_concepts_dep_2_c_train,\n",
    "    test_size=0.2,\n",
    ")\n",
    "\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_2_x_train.shape =\", balanced_multiclass_task_bin_concepts_dep_2_x_train.shape)\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_2_x_train label distribution:\", count_class_balance(balanced_multiclass_task_bin_concepts_dep_2_y_train))\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_2_x_train concept distribution:\", np.sum(balanced_multiclass_task_bin_concepts_dep_2_c_train, axis=0)/balanced_multiclass_task_bin_concepts_dep_2_c_train.shape[0])\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_2_x_train exclusive concept distribution:\", np.sum(balanced_multiclass_task_bin_concepts_dep_2_c_train[np.sum(balanced_multiclass_task_bin_concepts_dep_2_c_train, axis=-1) == 1], axis=0)/balanced_multiclass_task_bin_concepts_dep_2_c_train.shape[0])\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_2_x_test.shape =\", balanced_multiclass_task_bin_concepts_dep_2_x_test.shape)\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_2_c_test.shape =\", balanced_multiclass_task_bin_concepts_dep_2_c_test.shape)\n",
    "balanced_multiclass_task_bin_concepts_dep_2_train = (balanced_multiclass_task_bin_concepts_dep_2_x_train, balanced_multiclass_task_bin_concepts_dep_2_y_train, balanced_multiclass_task_bin_concepts_dep_2_c_train)\n",
    "balanced_multiclass_task_bin_concepts_dep_2_test = (balanced_multiclass_task_bin_concepts_dep_2_x_test, balanced_multiclass_task_bin_concepts_dep_2_y_test, balanced_multiclass_task_bin_concepts_dep_2_c_test)\n",
    "\n",
    "dep_2_corr_mat = np.ones(\n",
    "    (\n",
    "        balanced_multiclass_task_bin_concepts_dep_2_train[2].shape[-1],\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_2_train[1]))\n",
    "    )\n",
    ")\n",
    "for c in range(dep_2_corr_mat.shape[0]):\n",
    "    for l in range(dep_2_corr_mat.shape[1]):\n",
    "        dep_2_corr_mat[c][l] = np.corrcoef(\n",
    "            balanced_multiclass_task_bin_concepts_dep_2_train[2][:, c],\n",
    "            (balanced_multiclass_task_bin_concepts_dep_2_train[1] == l).astype(np.int32),\n",
    "        )[0, 1]\n",
    "fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "im, cbar = utils.heatmap(\n",
    "    np.abs(dep_2_corr_mat),\n",
    "    [f\"$c_{i}$\" for i in range(dep_2_corr_mat.shape[0])],\n",
    "    [f\"$l_{i}$\" for i in range(dep_2_corr_mat.shape[1])],\n",
    "    ax=ax,\n",
    "    cmap=\"magma\",\n",
    "    cbarlabel=f\"Correlation Coef\",\n",
    "    vmin=0, #-1,\n",
    "    vmax=1,\n",
    ")\n",
    "texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.suptitle(f\"3dshapes Concept-Label Absolute Correlations ($\\lambda = 2$)\", fontsize=25)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "plt.show()\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "balanced_multiclass_task_bin_concepts_dep_3_x_train, balanced_multiclass_task_bin_concepts_dep_3_y_train, balanced_multiclass_task_bin_concepts_dep_3_c_train = extract_data(\n",
    "    filter_fn_dep_3,\n",
    "    dataset_path=os.path.join(DATASETS_DIR, \"balanced_multiclass_task_bin_concepts_dep_3_concepts.npz\"),\n",
    "    concept_map_fn=multiclass_task_bin_concepts_map_fn,\n",
    "    label_fn=balanced_multiclass_task_bin_concepts_label_fn,\n",
    "#     force_rerun=True,\n",
    ")\n",
    "\n",
    "balanced_multiclass_task_bin_concepts_dep_3_x_train, balanced_multiclass_task_bin_concepts_dep_3_x_test, balanced_multiclass_task_bin_concepts_dep_3_y_train, balanced_multiclass_task_bin_concepts_dep_3_y_test, balanced_multiclass_task_bin_concepts_dep_3_c_train, balanced_multiclass_task_bin_concepts_dep_3_c_test = train_test_split(\n",
    "    balanced_multiclass_task_bin_concepts_dep_3_x_train,\n",
    "    balanced_multiclass_task_bin_concepts_dep_3_y_train,\n",
    "    balanced_multiclass_task_bin_concepts_dep_3_c_train,\n",
    "    test_size=0.2,\n",
    ")\n",
    "\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_3_x_train.shape =\", balanced_multiclass_task_bin_concepts_dep_3_x_train.shape)\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_3_x_train label distribution:\", count_class_balance(balanced_multiclass_task_bin_concepts_dep_3_y_train))\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_3_x_train concept distribution:\", np.sum(balanced_multiclass_task_bin_concepts_dep_3_c_train, axis=0)/balanced_multiclass_task_bin_concepts_dep_3_c_train.shape[0])\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_3_x_train exclusive concept distribution:\", np.sum(balanced_multiclass_task_bin_concepts_dep_3_c_train[np.sum(balanced_multiclass_task_bin_concepts_dep_3_c_train, axis=-1) == 1], axis=0)/balanced_multiclass_task_bin_concepts_dep_3_c_train.shape[0])\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_3_x_test.shape =\", balanced_multiclass_task_bin_concepts_dep_3_x_test.shape)\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_3_c_test.shape =\", balanced_multiclass_task_bin_concepts_dep_3_c_test.shape)\n",
    "balanced_multiclass_task_bin_concepts_dep_3_train = (balanced_multiclass_task_bin_concepts_dep_3_x_train, balanced_multiclass_task_bin_concepts_dep_3_y_train, balanced_multiclass_task_bin_concepts_dep_3_c_train)\n",
    "balanced_multiclass_task_bin_concepts_dep_3_test = (balanced_multiclass_task_bin_concepts_dep_3_x_test, balanced_multiclass_task_bin_concepts_dep_3_y_test, balanced_multiclass_task_bin_concepts_dep_3_c_test)\n",
    "\n",
    "dep_3_corr_mat = np.ones(\n",
    "    (\n",
    "        balanced_multiclass_task_bin_concepts_dep_3_train[2].shape[-1],\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_3_train[1]))\n",
    "    )\n",
    ")\n",
    "for c in range(dep_3_corr_mat.shape[0]):\n",
    "    for l in range(dep_3_corr_mat.shape[1]):\n",
    "        dep_3_corr_mat[c][l] = np.corrcoef(\n",
    "            balanced_multiclass_task_bin_concepts_dep_3_train[2][:, c],\n",
    "            (balanced_multiclass_task_bin_concepts_dep_3_train[1] == l).astype(np.int32),\n",
    "        )[0, 1]\n",
    "fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "im, cbar = utils.heatmap(\n",
    "    np.abs(dep_3_corr_mat),\n",
    "    [f\"$c_{i}$\" for i in range(dep_3_corr_mat.shape[0])],\n",
    "    [f\"$l_{i}$\" for i in range(dep_3_corr_mat.shape[1])],\n",
    "    ax=ax,\n",
    "    cmap=\"magma\",\n",
    "    cbarlabel=f\"Correlation Coef\",\n",
    "    vmin=0, #-1,\n",
    "    vmax=1,\n",
    ")\n",
    "texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.suptitle(f\"3dshapes Concept-Label Absolute Correlations ($\\lambda = 3$)\", fontsize=25)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "plt.show()\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "balanced_multiclass_task_bin_concepts_dep_4_x_train, balanced_multiclass_task_bin_concepts_dep_4_y_train, balanced_multiclass_task_bin_concepts_dep_4_c_train = extract_data(\n",
    "    filter_fn_dep_4,\n",
    "    dataset_path=os.path.join(DATASETS_DIR, \"balanced_multiclass_task_bin_concepts_dep_4_concepts.npz\"),\n",
    "    concept_map_fn=multiclass_task_bin_concepts_map_fn,\n",
    "    label_fn=balanced_multiclass_task_bin_concepts_label_fn,\n",
    "#     force_rerun=True,\n",
    ")\n",
    "\n",
    "balanced_multiclass_task_bin_concepts_dep_4_x_train, balanced_multiclass_task_bin_concepts_dep_4_x_test, balanced_multiclass_task_bin_concepts_dep_4_y_train, balanced_multiclass_task_bin_concepts_dep_4_y_test, balanced_multiclass_task_bin_concepts_dep_4_c_train, balanced_multiclass_task_bin_concepts_dep_4_c_test = train_test_split(\n",
    "    balanced_multiclass_task_bin_concepts_dep_4_x_train,\n",
    "    balanced_multiclass_task_bin_concepts_dep_4_y_train,\n",
    "    balanced_multiclass_task_bin_concepts_dep_4_c_train,\n",
    "    test_size=0.2,\n",
    ")\n",
    "\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_4_x_train.shape =\", balanced_multiclass_task_bin_concepts_dep_4_x_train.shape)\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_4_x_train label distribution:\", count_class_balance(balanced_multiclass_task_bin_concepts_dep_4_y_train))\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_4_x_train concept distribution:\", np.sum(balanced_multiclass_task_bin_concepts_dep_4_c_train, axis=0)/balanced_multiclass_task_bin_concepts_dep_4_c_train.shape[0])\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_4_x_train exclusive concept distribution:\", np.sum(balanced_multiclass_task_bin_concepts_dep_4_c_train[np.sum(balanced_multiclass_task_bin_concepts_dep_4_c_train, axis=-1) == 1], axis=0)/balanced_multiclass_task_bin_concepts_dep_4_c_train.shape[0])\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_4_x_test.shape =\", balanced_multiclass_task_bin_concepts_dep_4_x_test.shape)\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_4_c_test.shape =\", balanced_multiclass_task_bin_concepts_dep_4_c_test.shape)\n",
    "balanced_multiclass_task_bin_concepts_dep_4_train = (balanced_multiclass_task_bin_concepts_dep_4_x_train, balanced_multiclass_task_bin_concepts_dep_4_y_train, balanced_multiclass_task_bin_concepts_dep_4_c_train)\n",
    "balanced_multiclass_task_bin_concepts_dep_4_test = (balanced_multiclass_task_bin_concepts_dep_4_x_test, balanced_multiclass_task_bin_concepts_dep_4_y_test, balanced_multiclass_task_bin_concepts_dep_4_c_test)\n",
    "\n",
    "dep_4_corr_mat = np.ones(\n",
    "    (\n",
    "        balanced_multiclass_task_bin_concepts_dep_4_train[2].shape[-1],\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_4_train[1]))\n",
    "    )\n",
    ")\n",
    "for c in range(dep_4_corr_mat.shape[0]):\n",
    "    for l in range(dep_4_corr_mat.shape[1]):\n",
    "        dep_4_corr_mat[c][l] = np.corrcoef(\n",
    "            balanced_multiclass_task_bin_concepts_dep_4_train[2][:, c],\n",
    "            (balanced_multiclass_task_bin_concepts_dep_4_train[1] == l).astype(np.int32),\n",
    "        )[0, 1]\n",
    "fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "im, cbar = utils.heatmap(\n",
    "    np.abs(dep_4_corr_mat),\n",
    "    [f\"$c_{i}$\" for i in range(dep_4_corr_mat.shape[0])],\n",
    "    [f\"$l_{i}$\" for i in range(dep_4_corr_mat.shape[1])],\n",
    "    ax=ax,\n",
    "    cmap=\"magma\",\n",
    "    cbarlabel=f\"Correlation Coef\",\n",
    "    vmin=0, #-1,\n",
    "    vmax=1,\n",
    ")\n",
    "texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.suptitle(f\"3dshapes Concept-Label Absolute Correlations ($\\lambda = 4$)\", fontsize=25)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "plt.show()\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "balanced_multiclass_task_bin_concepts_dep_5_x_train, balanced_multiclass_task_bin_concepts_dep_5_y_train, balanced_multiclass_task_bin_concepts_dep_5_c_train = extract_data(\n",
    "    filter_fn_dep_5,\n",
    "    dataset_path=os.path.join(DATASETS_DIR, \"balanced_multiclass_task_bin_concepts_dep_5_concepts.npz\"),\n",
    "    concept_map_fn=multiclass_task_bin_concepts_map_fn,\n",
    "    label_fn=balanced_multiclass_task_bin_concepts_label_fn,\n",
    "#     force_rerun=True,\n",
    ")\n",
    "\n",
    "balanced_multiclass_task_bin_concepts_dep_5_x_train, balanced_multiclass_task_bin_concepts_dep_5_x_test, balanced_multiclass_task_bin_concepts_dep_5_y_train, balanced_multiclass_task_bin_concepts_dep_5_y_test, balanced_multiclass_task_bin_concepts_dep_5_c_train, balanced_multiclass_task_bin_concepts_dep_5_c_test = train_test_split(\n",
    "    balanced_multiclass_task_bin_concepts_dep_5_x_train,\n",
    "    balanced_multiclass_task_bin_concepts_dep_5_y_train,\n",
    "    balanced_multiclass_task_bin_concepts_dep_5_c_train,\n",
    "    test_size=0.2,\n",
    ")\n",
    "\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_5_x_train.shape =\", balanced_multiclass_task_bin_concepts_dep_5_x_train.shape)\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_5_x_train label distribution:\", count_class_balance(balanced_multiclass_task_bin_concepts_dep_5_y_train))\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_5_x_train concept distribution:\", np.sum(balanced_multiclass_task_bin_concepts_dep_5_c_train, axis=0)/balanced_multiclass_task_bin_concepts_dep_5_c_train.shape[0])\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_5_x_train exclusive concept distribution:\", np.sum(balanced_multiclass_task_bin_concepts_dep_5_c_train[np.sum(balanced_multiclass_task_bin_concepts_dep_5_c_train, axis=-1) == 1], axis=0)/balanced_multiclass_task_bin_concepts_dep_5_c_train.shape[0])\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_5_x_test.shape =\", balanced_multiclass_task_bin_concepts_dep_5_x_test.shape)\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_5_c_test.shape =\", balanced_multiclass_task_bin_concepts_dep_5_c_test.shape)\n",
    "balanced_multiclass_task_bin_concepts_dep_5_train = (balanced_multiclass_task_bin_concepts_dep_5_x_train, balanced_multiclass_task_bin_concepts_dep_5_y_train, balanced_multiclass_task_bin_concepts_dep_5_c_train)\n",
    "balanced_multiclass_task_bin_concepts_dep_5_test = (balanced_multiclass_task_bin_concepts_dep_5_x_test, balanced_multiclass_task_bin_concepts_dep_5_y_test, balanced_multiclass_task_bin_concepts_dep_5_c_test)\n",
    "\n",
    "dep_5_corr_mat = np.ones(\n",
    "    (\n",
    "        balanced_multiclass_task_bin_concepts_dep_5_train[2].shape[-1],\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_5_train[1]))\n",
    "    )\n",
    ")\n",
    "for c in range(dep_5_corr_mat.shape[0]):\n",
    "    for l in range(dep_5_corr_mat.shape[1]):\n",
    "        dep_5_corr_mat[c][l] = np.corrcoef(\n",
    "            balanced_multiclass_task_bin_concepts_dep_5_train[2][:, c],\n",
    "            (balanced_multiclass_task_bin_concepts_dep_5_train[1] == l).astype(np.int32),\n",
    "        )[0, 1]\n",
    "fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "im, cbar = utils.heatmap(\n",
    "    np.abs(dep_5_corr_mat),\n",
    "    [f\"$c_{i}$\" for i in range(dep_5_corr_mat.shape[0])],\n",
    "    [f\"$l_{i}$\" for i in range(dep_5_corr_mat.shape[1])],\n",
    "    ax=ax,\n",
    "    cmap=\"magma\",\n",
    "    cbarlabel=f\"Correlation Coef\",\n",
    "    vmin=0, #-1,\n",
    "    vmax=1,\n",
    ")\n",
    "texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.suptitle(f\"3dshapes Concept-Label Absolute Correlations ($\\lambda = 5$)\", fontsize=25)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "plt.show()\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_niching_dataset(x, y, c):\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    y = enc.fit_transform(y.reshape(-1, 1))\n",
    "    n_samples_per_class = y.sum(axis=0, dtype=int)\n",
    "    n_samples_to_draw = int(n_samples_per_class.min())\n",
    "    samples_per_class = []\n",
    "    for i in range(y.shape[1]):\n",
    "        samples_per_class.append(np.argwhere(y[:, i]==1)[np.random.choice(n_samples_per_class[i], n_samples_to_draw)].squeeze())\n",
    "    samples_per_class = np.concatenate(samples_per_class)\n",
    "    return x[samples_per_class], y[samples_per_class], c[samples_per_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_0 = balanced_niching_dataset(*balanced_multiclass_task_bin_concepts_dep_0_train)\n",
    "train_1 = balanced_niching_dataset(*balanced_multiclass_task_bin_concepts_dep_1_train)\n",
    "train_2 = balanced_niching_dataset(*balanced_multiclass_task_bin_concepts_dep_2_train)\n",
    "train_3 = balanced_niching_dataset(*balanced_multiclass_task_bin_concepts_dep_3_train)\n",
    "train_4 = balanced_niching_dataset(*balanced_multiclass_task_bin_concepts_dep_4_train)\n",
    "train_5 = balanced_niching_dataset(*balanced_multiclass_task_bin_concepts_dep_5_train)\n",
    "\n",
    "test_0 = balanced_niching_dataset(*balanced_multiclass_task_bin_concepts_dep_0_test)\n",
    "test_1 = balanced_niching_dataset(*balanced_multiclass_task_bin_concepts_dep_1_test)\n",
    "test_2 = balanced_niching_dataset(*balanced_multiclass_task_bin_concepts_dep_2_test)\n",
    "test_3 = balanced_niching_dataset(*balanced_multiclass_task_bin_concepts_dep_3_test)\n",
    "test_4 = balanced_niching_dataset(*balanced_multiclass_task_bin_concepts_dep_4_test)\n",
    "test_5 = balanced_niching_dataset(*balanced_multiclass_task_bin_concepts_dep_5_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wciMdtP9hy-1"
   },
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ey3yb8HIhy-2",
    "outputId": "08bc7f0b-146f-48e5-8f9f-40b608e09cb3"
   },
   "outputs": [],
   "source": [
    "# Construct the encoder model\n",
    "def _extract_concepts(activations, concept_cardinality):\n",
    "    concepts = []\n",
    "    total_seen = 0\n",
    "    if all(np.array(concept_cardinality) <= 1):\n",
    "        # Then nothing to do here as they are all binary concepts\n",
    "        return activations\n",
    "    for num_values in concept_cardinality:\n",
    "        concepts.append(activations[:, total_seen: total_seen + num_values])\n",
    "        total_seen += num_values\n",
    "    return concepts\n",
    "    \n",
    "def construct_encoder(\n",
    "    input_shape,\n",
    "    filter_groups,\n",
    "    units,\n",
    "    concept_cardinality,\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    latent_dims=0,\n",
    "    output_logits=False,\n",
    "):\n",
    "    encoder_inputs = tf.keras.Input(shape=input_shape)\n",
    "    encoder_compute_graph = encoder_inputs\n",
    "    \n",
    "    # Start with our convolutions\n",
    "    num_convs = 0\n",
    "    for filter_group in filter_groups:\n",
    "        for (num_filters, kernel_size) in filter_group:\n",
    "            encoder_compute_graph = tf.keras.layers.Conv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=\"SAME\",\n",
    "                activation=None,\n",
    "                name=f'encoder_conv_{num_convs}',\n",
    "            )(encoder_compute_graph)\n",
    "            num_convs += 1\n",
    "            encoder_compute_graph = tf.keras.layers.BatchNormalization()(\n",
    "                encoder_compute_graph\n",
    "            )\n",
    "            encoder_compute_graph = tf.keras.activations.relu(encoder_compute_graph)\n",
    "        # Then do a max pool here to control the parameter count of the model\n",
    "        # at the end of each group\n",
    "        encoder_compute_graph = tf.keras.layers.MaxPooling2D(\n",
    "            pool_size=max_pool_window,\n",
    "            strides=max_pool_stride,\n",
    "        )(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Flatten this guy\n",
    "    encoder_compute_graph = tf.keras.layers.Flatten()(encoder_compute_graph)\n",
    "    \n",
    "    # Add a dropout if requested\n",
    "    if drop_prob:\n",
    "        encoder_compute_graph = tf.keras.layers.Dropout(drop_prob)(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Finally, include the fully connected bottleneck here\n",
    "    for i, units in enumerate(units):\n",
    "        encoder_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(encoder_compute_graph)\n",
    "    \n",
    "    if latent_dims:\n",
    "        bypass = tf.keras.layers.Dense(\n",
    "            latent_dims,\n",
    "            activation=\"sigmoid\",\n",
    "            name=\"encoder_bypass_channel\",\n",
    "        )(encoder_compute_graph)\n",
    "    else:\n",
    "        bypass = None\n",
    "    \n",
    "    # Map to our output distribution to a flattened\n",
    "    # vector where we will extract distributions over\n",
    "    # all concept values\n",
    "    encoder_compute_graph = tf.keras.layers.Dense(\n",
    "        sum(concept_cardinality),\n",
    "        activation=None,\n",
    "        name=\"encoder_concept_outputs\",\n",
    "    )(encoder_compute_graph)\n",
    "        \n",
    "    # Separate this vector into all of its heads\n",
    "    concept_outputs = _extract_concepts(\n",
    "        encoder_compute_graph,\n",
    "        concept_cardinality,\n",
    "    )\n",
    "    if not output_logits:\n",
    "        if isinstance(concept_outputs, list):\n",
    "            for i, concept_vec in enumerate(concept_outputs):\n",
    "                if concept_vec.shape[-1] == 1:\n",
    "                    # Then this is a binary concept so simply apply sigmoid\n",
    "                    concept_outputs[i] = tf.keras.activations.sigmoid(concept_vec)\n",
    "                else:\n",
    "                    # Else we will apply a softmax layer as we assume that all of these\n",
    "                    # entries represent a multi-modal probability distribution\n",
    "                    concept_outputs[i] = tf.keras.activations.softmax(\n",
    "                        concept_vec,\n",
    "                        axis=-1,\n",
    "                    )\n",
    "        else:\n",
    "            # Else they are allbinary concepts so let's sigmoid them\n",
    "            concept_outputs = tf.keras.activations.sigmoid(concept_outputs)\n",
    "    return tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        [concept_outputs, bypass] if bypass is not None else concept_outputs,\n",
    "        name=\"encoder\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7lu1IS3hy-4"
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## Build concepts-to-labels model\n",
    "############################################################################\n",
    "\n",
    "def construct_decoder(units, num_outputs):\n",
    "    decoder_layers = [\n",
    "        tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation=tf.nn.relu,\n",
    "            name=f\"decoder_dense_{i+1}\",\n",
    "        ) for i, units in enumerate(units)\n",
    "    ]\n",
    "    return tf.keras.Sequential(decoder_layers + [\n",
    "        tf.keras.layers.Dense(\n",
    "            num_outputs if num_outputs > 2 else 1,\n",
    "            activation=None,\n",
    "            name=\"decoder_model_output\",\n",
    "        )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjd9d2sDhy-4",
    "outputId": "f8a8faed-079a-475b-d5be-adad132a272d"
   },
   "outputs": [],
   "source": [
    "# Construct the complete model\n",
    "def construct_end_to_end_model(\n",
    "    input_shape,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    num_outputs,\n",
    "    learning_rate=1e-3,\n",
    "):\n",
    "    model_inputs = tf.keras.Input(shape=input_shape)\n",
    "    latent = encoder(model_inputs)\n",
    "    if isinstance(latent, list):\n",
    "        if len(latent) > 1:\n",
    "            compacted_vector = tf.keras.layers.Concatenate(axis=-1)(\n",
    "                latent\n",
    "            )\n",
    "        else:\n",
    "            compacted_vector = latent[0]\n",
    "    else:\n",
    "        compacted_vector = latent\n",
    "    model_compute_graph = decoder(compacted_vector)\n",
    "    # Now time to collapse all the concepts again back into a single vector\n",
    "    model = tf.keras.Model(\n",
    "        model_inputs,\n",
    "        model_compute_graph,\n",
    "        name=\"complete_model\",\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "        loss=(\n",
    "            tf.keras.losses.BinaryCrossentropy(from_logits=True) if (num_outputs <= 2)\n",
    "            else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        ),\n",
    "        metrics=[\n",
    "            \"binary_accuracy\" if (num_outputs <= 2)\n",
    "            else tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "        ],\n",
    "    )\n",
    "    return model, encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_UzDOFVhy-5"
   },
   "source": [
    "# CBM Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTjkp5B3hy-6",
    "outputId": "35f4b4fe-7295-4747-a9d4-c52f4c75dab3"
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## Build CBM\n",
    "############################################################################\n",
    "import concepts_xai.methods.CBM.CBModel as CBM\n",
    "\n",
    "def construct_cbm(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    num_outputs,\n",
    "    alpha=0.1,\n",
    "    learning_rate=1e-3,\n",
    "    latent_dims=0,\n",
    "    encoder_output_logits=False,\n",
    "):\n",
    "    model_factory = CBM.BypassJointCBM if latent_dims else CBM.JointConceptBottleneckModel\n",
    "    cbm_model = model_factory(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        task_loss=(\n",
    "            tf.keras.losses.BinaryCrossentropy(from_logits=True) if (num_outputs <= 2)\n",
    "            else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        ),\n",
    "        name=\"joint_cbm\",\n",
    "        metrics=[\n",
    "            tf.keras.metrics.BinaryAccuracy() if (num_outputs <= 2)\n",
    "            else tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "        ],\n",
    "        alpha=alpha,\n",
    "        pass_concept_logits=encoder_output_logits,\n",
    "    )\n",
    "\n",
    "    ############################################################################\n",
    "    ## Compile CBM Model\n",
    "    ############################################################################\n",
    "\n",
    "    cbm_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    )\n",
    "    return cbm_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.evaluation.metrics.purity as purity\n",
    "from collections import defaultdict\n",
    "\n",
    "############################################################################\n",
    "## Experiment loop\n",
    "############################################################################\n",
    "\n",
    "def construct_trivial_auc_mat(num_concepts):\n",
    "    result = np.ones((num_concepts, num_concepts), dtype=np.float32) * 0.5\n",
    "    return result + np.eye(num_concepts, dtype=np.float32) * 0.5\n",
    "    \n",
    "def cbm_experiment_loop(\n",
    "    experiment_config,\n",
    "    datasets,\n",
    "    load_from_cache=False,\n",
    "    oracle_matrix_cache=None,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    experiment_variables = dict(\n",
    "        config = experiment_config,\n",
    "        niss=[],\n",
    "    )\n",
    "    res_dir = experiment_config['niching_results_dir']\n",
    "    if load_from_cache:\n",
    "        if os.path.exists(os.path.join(res_dir, 'results_niching.joblib')):\n",
    "            experiment_variables = load(os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    count = 0\n",
    "    start_ind = 0\n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    for (ds_name, (x_train, y_train, c_train), (x_test, y_test, c_test)) in datasets[start_ind:]:\n",
    "        print(\"Training with dataset:\", ds_name)\n",
    "        task_accs = []\n",
    "        concept_accs = []\n",
    "        aucs = []\n",
    "        purity_mats = []\n",
    "        oracle_mats = []\n",
    "        purities = []\n",
    "        non_oracle_purities = []\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} of dataset {ds_name}\")\n",
    "            \n",
    "            # Proceed to do and end-to-end model in case we want to\n",
    "            # do some task-specific pretraining\n",
    "            end_to_end_model, encoder, decoder = construct_end_to_end_model(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "                learning_rate=experiment_config[\"learning_rate\"],\n",
    "                encoder=construct_encoder(\n",
    "                    input_shape=experiment_config[\"input_shape\"],\n",
    "                    filter_groups=experiment_config[\"encoder_filter_groups\"],\n",
    "                    units=experiment_config[\"encoder_units\"],\n",
    "                    concept_cardinality=experiment_config[\"concept_cardinality\"],\n",
    "                    drop_prob=experiment_config.get(\"drop_prob\", 0.5),\n",
    "                    max_pool_window=experiment_config.get(\"max_pool_window\", (2, 2)),\n",
    "                    max_pool_stride=experiment_config.get(\"max_pool_stride\", (2, 2)),\n",
    "                    latent_dims=experiment_config.get(\"latent_dims\", 0),\n",
    "                    output_logits=experiment_config.get(\"encoder_output_logits\", False),\n",
    "                ),\n",
    "                decoder=construct_decoder(\n",
    "                    units=experiment_config[\"decoder_units\"],\n",
    "                    num_outputs=experiment_config[\"num_outputs\"],\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            # Now time to actually construct and train the CBM\n",
    "            cbm_model = construct_cbm(\n",
    "                encoder=load_model(\n",
    "                    os.path.join(\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                        f\"models/encoder_{ds_name}_trial_{trial}\"\n",
    "                    )\n",
    "                ),\n",
    "                decoder=load_model(\n",
    "                    os.path.join(\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                        f\"models/decoder_{ds_name}_trial_{trial}\"\n",
    "                    )\n",
    "                ),\n",
    "                alpha=experiment_config[\"alpha\"],\n",
    "                learning_rate=experiment_config[\"learning_rate\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "                latent_dims=experiment_config.get(\"latent_dims\", 0),\n",
    "                encoder_output_logits=experiment_config.get(\"encoder_output_logits\", False),\n",
    "            )\n",
    "\n",
    "            print(\"\\t\\tCBM training completed\")\n",
    "\n",
    "            print(\"\\t\\tEncode...\")\n",
    "            concept_train_list = cbm_model.encoder(x_train)\n",
    "            concept_test_list = cbm_model.encoder(x_test)\n",
    "            \n",
    "            c_train_pred = []\n",
    "            for concept in concept_train_list:\n",
    "                c_train_pred.append(concept[:, 1])\n",
    "            c_train_pred = tf.stack(c_train_pred, axis=1).numpy()\n",
    "            \n",
    "            print(\"\\t\\tListed...\")\n",
    "            \n",
    "            c_test_pred = []\n",
    "            for concept in concept_test_list:\n",
    "                c_test_pred.append(concept[:, 1])\n",
    "            c_test_pred = tf.stack(c_test_pred, axis=1).numpy()\n",
    "            \n",
    "            experiment_variables['niss'].append(\n",
    "                niching.niche_impurity_score(\n",
    "                    c_soft=c_test_pred,\n",
    "                    c_true=c_test,\n",
    "                    c_soft_train=c_train_pred,\n",
    "                    c_true_train=c_train,\n",
    "                )\n",
    "            )\n",
    "            print(f'\\t\\tNIS: {experiment_variables[\"niss\"][-1]:.2f}')\n",
    "            \n",
    "            os.makedirs(res_dir, exist_ok=True)\n",
    "            dump(experiment_variables, os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            \n",
    "    return experiment_variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(CBM)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "cbm_base_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    pre_train_epochs=0,\n",
    "    warmup_epochs=0,\n",
    "    trials=5,\n",
    "    alpha=10,\n",
    "    learning_rate=1e-3,\n",
    "    encoder_filter_groups=[\n",
    "        [(8, (7, 7))],\n",
    "        [(16, (5, 5))],\n",
    "        [(32, (3, 3))],\n",
    "        [(64, (3, 3))]\n",
    "    ],\n",
    "    encoder_units=[64, 64],\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    pax_pool_stride=2,\n",
    "    decoder_units=[64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    concept_cardinality=[2 for _ in range(balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1])],\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(RESULTS_DIR, \"cbm/dependency_balanced_multiclass_tasks\"),\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        'cbm/base'\n",
    "    ),\n",
    "    input_shape=[64, 64, 3],\n",
    "    num_concepts=multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    latent_dims=0,\n",
    "    holdout_fraction=0.1,\n",
    "    verbosity=0,\n",
    "    early_stop_metric=\"val_concept_accuracy\",\n",
    "    early_stop_mode=\"max\",\n",
    "    encoder_output_logits=False,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(cbm_base_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "dependency_multiclass_figure_dir = os.path.join(cbm_base_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(dependency_multiclass_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "# cbm_model = cbm_experiment_loop(\n",
    "cbm_base_results = cbm_experiment_loop(\n",
    "    cbm_base_experiment_config,\n",
    "    load_from_cache=FROM_CACHE,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0\", train_0, test_0),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1\", train_1, test_1),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2\", train_2, test_2),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3\", train_3, test_3),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4\", train_4, test_4),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_5\", train_5, test_5),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(CBM)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "cbm_from_logits_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    pre_train_epochs=0,\n",
    "    warmup_epochs=0,\n",
    "    trials=5,\n",
    "    alpha=10,\n",
    "    learning_rate=1e-3,\n",
    "    encoder_filter_groups=[\n",
    "        [(8, (7, 7))],\n",
    "        [(16, (5, 5))],\n",
    "        [(32, (3, 3))],\n",
    "        [(64, (3, 3))]\n",
    "    ],\n",
    "    encoder_units=[64, 64],\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    pax_pool_stride=2,\n",
    "    decoder_units=[64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    concept_cardinality=[2 for _ in range(balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1])],\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(RESULTS_DIR, \"cbm/dependency_balanced_multiclass_from_logits_tasks\"),\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        'cbm/from_logits'\n",
    "    ),\n",
    "    input_shape=[64, 64, 3],\n",
    "    num_concepts=multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    latent_dims=0,\n",
    "    holdout_fraction=0.1,\n",
    "    verbosity=0,\n",
    "    early_stop_metric=\"val_concept_accuracy\",\n",
    "    early_stop_mode=\"max\",\n",
    "    encoder_output_logits=True,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(cbm_from_logits_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "dependency_multiclass_figure_dir = os.path.join(cbm_from_logits_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(dependency_multiclass_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "# cbm_model = cbm_experiment_loop(\n",
    "cbm_from_logits_results = cbm_experiment_loop(\n",
    "    cbm_from_logits_experiment_config,\n",
    "    load_from_cache=False, #FROM_CACHE,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0\", train_0, test_0),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1\", train_1, test_1),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2\", train_2, test_2),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3\", train_3, test_3),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4\", train_4, test_4),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_5\", train_5, test_5),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CW Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.CW.CWLayer as CW\n",
    "\n",
    "def conv_predictor_model_fn(\n",
    "    input_concept_classes=1,\n",
    "    output_concept_classes=2,\n",
    "):\n",
    "    estimator = tf.keras.models.Sequential([\n",
    "         tf.keras.layers.Conv2D(\n",
    "            filters=16,\n",
    "            kernel_size=(3,3),\n",
    "            padding=\"SAME\",\n",
    "            activation=\"relu\",\n",
    "            data_format=\"channels_last\",\n",
    "        ),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=(3,3),\n",
    "            padding=\"SAME\",\n",
    "            activation=\"relu\",\n",
    "            data_format=\"channels_last\",\n",
    "        ),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(\n",
    "            output_concept_classes if output_concept_classes > 2 else 1,\n",
    "            # We will merge the activation into the loss for numerical\n",
    "            # stability\n",
    "            activation=None,\n",
    "        ),\n",
    "    ])\n",
    "    estimator.compile(\n",
    "        # Use ADAM optimizer by default\n",
    "        optimizer='adam',\n",
    "        # Note: we assume labels come without a one-hot-encoding in the\n",
    "        #       case when the concepts are categorical.\n",
    "        loss=(\n",
    "            tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                from_logits=True\n",
    "            ) if output_concept_classes > 2 else\n",
    "            tf.keras.losses.BinaryCrossentropy(\n",
    "                from_logits=True,\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    return estimator\n",
    "\n",
    "\n",
    "\n",
    "def construct_cw_model(\n",
    "    input_shape,\n",
    "    num_outputs,\n",
    "    filter_groups,\n",
    "    units,\n",
    "    activation_mode,\n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    T=5,\n",
    "    eps=1e-5,\n",
    "    momentum=0.9,\n",
    "    c1=1e-4,\n",
    "    c2=0.9,\n",
    "    max_tau_iterations=500,\n",
    "    initial_tau=1000.0,\n",
    "    initial_beta=1e8,\n",
    "    initial_alpha=0,\n",
    "):\n",
    "    model_inputs = tf.keras.Input(shape=input_shape)\n",
    "    model_compute_graph = model_inputs\n",
    "    \n",
    "    # Start with our convolutions\n",
    "    num_convs = 0\n",
    "    cw_inputs = []\n",
    "    cw_ouputs = []\n",
    "    for filter_group in filter_groups:\n",
    "        # Add a default \"no CW layer\" to each filter group\n",
    "        # if they have not specified this feature\n",
    "        filter_group = list(map(\n",
    "            lambda x: x if len(x) == 3 else (x[0], x[1], False),\n",
    "            filter_group\n",
    "        ))\n",
    "        for (num_filters, kernel_size, cw_layer) in filter_group:\n",
    "            model_compute_graph = tf.keras.layers.Conv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=\"SAME\",\n",
    "                activation=None,\n",
    "                name=f'encoder_conv_{num_convs}',\n",
    "            )(model_compute_graph)\n",
    "            num_convs += 1\n",
    "            if cw_layer:\n",
    "                cw_inputs.append(model_compute_graph)\n",
    "                model_compute_graph = CW.ConceptWhiteningLayer(\n",
    "                    data_format=\"channels_last\",\n",
    "                    activation_mode=activation_mode,\n",
    "                    T=T,\n",
    "                    eps=eps,\n",
    "                    momentum=momentum,\n",
    "                    c1=c1,\n",
    "                    c2=c2,\n",
    "                    max_tau_iterations=max_tau_iterations,\n",
    "                    initial_tau=initial_tau,\n",
    "                    initial_beta=initial_beta,\n",
    "                    initial_alpha=initial_alpha,\n",
    "                )(\n",
    "                    model_compute_graph\n",
    "                )\n",
    "                cw_ouputs.append(model_compute_graph)\n",
    "            else:\n",
    "                model_compute_graph = tf.keras.layers.BatchNormalization(\n",
    "                    axis=-1,\n",
    "                )(model_compute_graph)\n",
    "            model_compute_graph = tf.keras.activations.relu(model_compute_graph)\n",
    "        # Then do a max pool here to control the parameter count of the model\n",
    "        # at the end of each group\n",
    "        model_compute_graph = tf.keras.layers.MaxPooling2D(\n",
    "            pool_size=max_pool_window,\n",
    "            strides=max_pool_stride,\n",
    "        )(\n",
    "            model_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Flatten this guy\n",
    "    model_compute_graph = tf.keras.layers.Flatten()(model_compute_graph)\n",
    "    \n",
    "    # Add a dropout if requested\n",
    "    if drop_prob:\n",
    "        model_compute_graph = tf.keras.layers.Dropout(drop_prob)(\n",
    "            model_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Finally, include the fully connected bottleneck here\n",
    "    for i, units in enumerate(units):\n",
    "        model_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(model_compute_graph)\n",
    "    \n",
    "    # Map to our output distribution to a flattened\n",
    "    # vector where we will extract distributions over\n",
    "    # all concept values\n",
    "    model_compute_graph = tf.keras.layers.Dense(\n",
    "        num_outputs,\n",
    "        activation=None,\n",
    "        name=\"logits\",\n",
    "    )(model_compute_graph)\n",
    "    \n",
    "  \n",
    "    cw_model = tf.keras.Model(\n",
    "        model_inputs,\n",
    "        model_compute_graph,\n",
    "        name=\"cw_model\",\n",
    "    )\n",
    "    cw_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "        loss=(\n",
    "            tf.keras.losses.BinaryCrossentropy(from_logits=True) if (num_outputs <= 2)\n",
    "            else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        ),\n",
    "        metrics=[\n",
    "            \"binary_accuracy\" if (num_outputs <= 2)\n",
    "            else tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    encoder = tf.keras.Model(\n",
    "        model_inputs,\n",
    "        cw_inputs,\n",
    "        name=\"encoder_model\",\n",
    "    )\n",
    "\n",
    "    cw_output_model = tf.keras.Model(\n",
    "        model_inputs,\n",
    "        cw_ouputs,\n",
    "        name=\"cw_output_model\",\n",
    "    )\n",
    "    return cw_model, encoder, cw_output_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "def channels_corr_mat(outputs):\n",
    "    if len(outputs.shape) == 2:\n",
    "        outputs = np.expand_dims(\n",
    "            np.expand_dims(outputs, axis=1),\n",
    "            axis=1,\n",
    "        )\n",
    "    # Change (N, H, W, C) to (C, N, H, W)\n",
    "    outputs = np.transpose(outputs, [3, 0, 1, 2])\n",
    "    # Change (C, N, H, W) to (C, NxHxW)\n",
    "    cnhw_shape = outputs.shape\n",
    "    outputs = np.transpose(np.reshape(outputs, [cnhw_shape[0], -1]))\n",
    "    outputs -= np.mean(outputs, axis=0, keepdims=True)\n",
    "    outputs = outputs / np.std(outputs, axis=0, keepdims=True)\n",
    "    return np.dot(outputs.transpose(), outputs) / outputs.shape[0]\n",
    "\n",
    "def conv_predictor_model_fn(\n",
    "    input_concept_classes=1,\n",
    "    output_concept_classes=2,\n",
    "):\n",
    "    estimator = tf.keras.models.Sequential([\n",
    "         tf.keras.layers.Conv2D(\n",
    "            filters=16,\n",
    "            kernel_size=(3,3),\n",
    "            padding=\"SAME\",\n",
    "            activation=\"relu\",\n",
    "            data_format=\"channels_last\",\n",
    "        ),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=(3,3),\n",
    "            padding=\"SAME\",\n",
    "            activation=\"relu\",\n",
    "            data_format=\"channels_last\",\n",
    "        ),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(\n",
    "            output_concept_classes if output_concept_classes > 2 else 1,\n",
    "            # We will merge the activation into the loss for numerical\n",
    "            # stability\n",
    "            activation=None,\n",
    "        ),\n",
    "    ])\n",
    "    estimator.compile(\n",
    "        # Use ADAM optimizer by default\n",
    "        optimizer='adam',\n",
    "        # Note: we assume labels come without a one-hot-encoding in the\n",
    "        #       case when the concepts are categorical.\n",
    "        loss=(\n",
    "            tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                from_logits=True\n",
    "            ) if output_concept_classes > 2 else\n",
    "            tf.keras.losses.BinaryCrossentropy(\n",
    "                from_logits=True,\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    return estimator\n",
    "\n",
    "\n",
    "\n",
    "def concept_scores(\n",
    "    cw_layer,\n",
    "    inputs,\n",
    "    aggregator='max_pool_mean',\n",
    "    concept_indices=None,\n",
    "):\n",
    "    outputs = cw_layer(inputs, training=False)\n",
    "    if len(tf.shape(outputs)) == 2:\n",
    "        # Then the scores are already computed by our forward pass\n",
    "        scores = outputs\n",
    "    else:\n",
    "        # Else, we need to do some aggregation\n",
    "        if aggregator == 'mean':\n",
    "            # Compute the mean over all channels\n",
    "            scores = tf.math.reduce_mean(outputs, axis=[2, 3])\n",
    "        elif aggregator == 'max_pool_mean':\n",
    "            # First downsample using a max pool and then continue with\n",
    "            # a mean\n",
    "            window_size = min(\n",
    "                2,\n",
    "                outputs.shape[-1],\n",
    "                outputs.shape[-2],\n",
    "            )\n",
    "            scores = tf.nn.max_pool(\n",
    "                outputs,\n",
    "                ksize=window_size,\n",
    "                strides=window_size,\n",
    "                padding=\"SAME\",\n",
    "                data_format=\"NCHW\",\n",
    "            )\n",
    "            scores = tf.math.reduce_mean(scores, axis=[2, 3])\n",
    "        elif aggregator == 'max':\n",
    "            # Simply select the maximum value across a given channel\n",
    "            scores = tf.math.reduce_max(outputs, axis=[2, 3])\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported aggregator {aggregator}.')\n",
    "\n",
    "    if concept_indices is not None:\n",
    "        return scores[:, concept_indices]\n",
    "    return scores\n",
    "\n",
    "\n",
    "\n",
    "def cw_experiment_loop(\n",
    "    experiment_config,\n",
    "    datasets,\n",
    "    load_from_cache=False,\n",
    "    oracle_matrix_cache=None,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    \n",
    "    experiment_variables = dict(\n",
    "        config = experiment_config,\n",
    "        niss=[],\n",
    "    )\n",
    "    res_dir = experiment_config['niching_results_dir']\n",
    "    if load_from_cache:\n",
    "        if os.path.exists(os.path.join(res_dir, 'results_niching.joblib')):\n",
    "            experiment_variables = load(os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            return experiment_variables\n",
    "        \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    count = 0\n",
    "    start_ind = 0\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    )\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    for (ds_name, (x_train, y_train, c_train), (x_test, y_test, c_test)) in datasets[start_ind:]:\n",
    "        print(\"Training with dataset:\", ds_name)\n",
    "        \n",
    "        if not experiment_config.get(\"exclusive_concepts\", False):\n",
    "            concept_groups = [\n",
    "                x_train[c_train[:, i] == 1, :, :, :]\n",
    "                for i in range(c_train.shape[-1])\n",
    "            ]\n",
    "        else:\n",
    "            concept_groups = [\n",
    "                x_train[np.logical_and(c_train[:, i] == 1, np.sum(c_train, axis=-1) == 1), :, :, :]\n",
    "                for i in range(c_train.shape[-1])\n",
    "            ]\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} of {ds_name}\")\n",
    "            if not experiment_config.get(\"exclusive_test_concepts\", False):\n",
    "                test_concept_groups = [\n",
    "                    x_train[c_train[:, i] == 1, :, :, :]\n",
    "                    for i in range(c_test.shape[-1])\n",
    "                ]\n",
    "            else:\n",
    "                test_concept_groups = [\n",
    "                    x_test[np.logical_and(c_test[:, i] == 1, np.sum(c_test, axis=-1) == 1), :, :, :]\n",
    "                    for i in range(c_test.shape[-1])\n",
    "                ]\n",
    "            print(\"Sizes of test_concept_groups:\", list(map(lambda x: x.shape, test_concept_groups)))\n",
    "            for i, group in enumerate(test_concept_groups):\n",
    "                max_test_size = experiment_config.get(\"max_concept_group_size\", group.shape[-1])\n",
    "                test_concept_groups[i] = (\n",
    "                    group[np.random.choice(group.shape[0], max_test_size), : :, :]\n",
    "                    if group.shape[0] > max_test_size else group\n",
    "                )\n",
    "\n",
    "\n",
    "            # Construct our CW model\n",
    "            model, encoder, cw_model = construct_cw_model(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "                filter_groups=experiment_config[\"filter_groups\"],\n",
    "                units=experiment_config[\"units\"],\n",
    "                drop_prob=experiment_config.get(\"drop_prob\", 0),\n",
    "                max_pool_window=experiment_config.get(\"max_pool_window\", (2, 2)),\n",
    "                max_pool_stride=experiment_config.get(\"max_pool_stride\", (2, 2)),\n",
    "                T=experiment_config.get(\"T\", 5),\n",
    "                eps=experiment_config.get(\"eps\", 1e-5),\n",
    "                momentum=experiment_config.get(\"momentum\", 0.9),\n",
    "                activation_mode=experiment_config[\"activation_mode\"],\n",
    "                c1=experiment_config.get(\"c1\", 1e-4),\n",
    "                c2=experiment_config.get(\"c2\", 0.9),\n",
    "                max_tau_iterations=experiment_config.get(\"max_tau_iterations\", 500),\n",
    "                initial_tau=experiment_config.get(\"initial_tau\", 1000),\n",
    "                initial_beta=experiment_config.get(\"initial_beta\", 1e8),\n",
    "                initial_alpha=experiment_config.get(\"initial_alpha\", 0),\n",
    "            )\n",
    "            \n",
    "            print(\"\\t\\tCW training completed\")\n",
    "            model = load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/end_to_end_model_{ds_name}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # finding niches for several values of beta\n",
    "            niche_sizes = []\n",
    "            niche_impurities = []\n",
    "            # And estimate the area under the curve using the trapezoid method\n",
    "            total_area_under_curve_map = defaultdict(float)\n",
    "            prev_value_map = {}\n",
    "            delta_beta = experiment_config.get(\"delta_beta\", 0.05)\n",
    "            if not experiment_config['feature_map']:\n",
    "                encoder = tf.keras.Model(\n",
    "                    inputs=model.get_layer(model.layers[0].name).input,\n",
    "                    outputs=model.get_layer(model.layers[13].name).output,\n",
    "                )\n",
    "                c_train_pred = concept_scores(\n",
    "                    model.layers[14],\n",
    "                    encoder(x_train),\n",
    "                    aggregator=experiment_config['aggregator'],\n",
    "                ).numpy()\n",
    "                c_train_pred = c_train_pred[:, :experiment_config[\"num_concepts\"]]\n",
    "                c_test_pred = concept_scores(\n",
    "                    model.layers[14],\n",
    "                    encoder(x_test),\n",
    "                    aggregator=experiment_config['aggregator'],\n",
    "                ).numpy()[:, :experiment_config[\"num_concepts\"]]\n",
    "\n",
    "            else:\n",
    "                encoder = tf.keras.Model(\n",
    "                    inputs=model.get_layer(model.layers[0].name).input,\n",
    "                    outputs=model.get_layer(model.layers[14].name).output,\n",
    "                )\n",
    "\n",
    "                c_train_pred = encoder(x_train)\n",
    "                c_train_pred = c_train_pred[:, :, :, :experiment_config[\"num_concepts\"]]\n",
    "                c_test_pred = encoder(x_test)\n",
    "                c_test_pred = c_test_pred[:, :, :, :experiment_config[\"num_concepts\"]]\n",
    "                out_shape = c_train_pred.shape[1]*c_train_pred.shape[2]\n",
    "                c_train_pred = c_train_pred.numpy().reshape(-1, out_shape, c_train_pred.shape[3])\n",
    "                c_test_pred = c_test_pred.numpy().reshape(-1, out_shape, c_test_pred.shape[3])\n",
    "                \n",
    "            experiment_variables['niss'].append(\n",
    "                niching.niche_impurity_score(\n",
    "                    c_soft=c_test_pred,\n",
    "                    c_true=c_test,\n",
    "                    c_soft_train=c_train_pred,\n",
    "                    c_true_train=c_train,\n",
    "                )\n",
    "            )\n",
    "            print(f'\\t\\tNIS: {experiment_variables[\"niss\"][-1]:.2f}')\n",
    "            \n",
    "            os.makedirs(res_dir, exist_ok=True)\n",
    "            dump(experiment_variables, os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            \n",
    "        \n",
    "    return experiment_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(CW)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "cw_features_experiment_config = dict(\n",
    "    batch_size=128,\n",
    "    max_epochs=50,\n",
    "    pre_train_epochs=50,\n",
    "    post_train_epochs=0,\n",
    "    cw_train_freq=20,\n",
    "    cw_train_batch_steps=20,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    filter_groups=[\n",
    "        [(8, (7, 7), False)],\n",
    "        [(16, (5, 5), False)],\n",
    "        [(32, (3, 3), False)],\n",
    "        [(64, (3, 3), True)],\n",
    "    ],\n",
    "    units=[64, 64, 64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(RESULTS_DIR, \"cw/balanced_multiclass_purity\"),\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_train[0].shape[1:],\n",
    "    num_concepts=balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    cw_layer=14,\n",
    "    aggregator='max_pool_mean',\n",
    "    activation_mode='max_pool_mean',\n",
    "    cw_train_iterations=1,\n",
    "    concept_auc_freq=0,\n",
    "    holdout_fraction=0.1,\n",
    "    heatmap_display=True,\n",
    "    T=5,\n",
    "    eps=1e-5,\n",
    "    momentum=0.9,\n",
    "    initial_tau=1000,\n",
    "    initial_beta=1e8,\n",
    "    initial_alpha=0,\n",
    "    drop_prob=0,\n",
    "    exclusive_concepts=False,\n",
    "    exclusive_true_concepts=False,\n",
    "    feature_map=True,\n",
    "    add='',\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "cw_features_experiment_config['niching_results_dir'] = os.path.join(\n",
    "    NICHING_RESULTS_DIR,\n",
    "    f'cw/base_feature_{cw_features_experiment_config[\"feature_map\"]}{cw_features_experiment_config[\"add\"]}',\n",
    ")\n",
    "\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "cw_features_results = cw_experiment_loop(\n",
    "    cw_features_experiment_config,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0\", train_0, test_0),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1\", train_1, test_1),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2\", train_2, test_2),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3\", train_3, test_3),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4\", train_4, test_4),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_5\", train_5, test_5),\n",
    "    ],\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(CW)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "cw_base_experiment_config = dict(\n",
    "    batch_size=128,\n",
    "    max_epochs=50,\n",
    "    pre_train_epochs=50,\n",
    "    post_train_epochs=0,\n",
    "    cw_train_freq=20,\n",
    "    cw_train_batch_steps=20,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    filter_groups=[\n",
    "        [(8, (7, 7), False)],\n",
    "        [(16, (5, 5), False)],\n",
    "        [(32, (3, 3), False)],\n",
    "        [(64, (3, 3), True)],\n",
    "    ],\n",
    "    units=[64, 64, 64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(RESULTS_DIR, \"cw/balanced_multiclass_purity\"),\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_train[0].shape[1:],\n",
    "    num_concepts=balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    cw_layer=14,\n",
    "    aggregator='max_pool_mean',\n",
    "    activation_mode='max_pool_mean',\n",
    "    cw_train_iterations=1,\n",
    "    concept_auc_freq=0,\n",
    "    holdout_fraction=0.1,\n",
    "    heatmap_display=True,\n",
    "    T=5,\n",
    "    eps=1e-5,\n",
    "    momentum=0.9,\n",
    "    initial_tau=1000,\n",
    "    initial_beta=1e8,\n",
    "    initial_alpha=0,\n",
    "    drop_prob=0,\n",
    "    exclusive_concepts=False,\n",
    "    exclusive_true_concepts=False,\n",
    "    feature_map=False,\n",
    "    add='',\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "cw_base_experiment_config['niching_results_dir'] = os.path.join(\n",
    "    NICHING_RESULTS_DIR,\n",
    "    f'cw/base_feature_{cw_base_experiment_config[\"feature_map\"]}{cw_base_experiment_config[\"add\"]}',\n",
    ")\n",
    "\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "cw_base_results = cw_experiment_loop(\n",
    "    cw_base_experiment_config,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0\", train_0, test_0),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1\", train_1, test_1),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2\", train_2, test_2),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3\", train_3, test_3),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4\", train_4, test_4),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_5\", train_5, test_5),\n",
    "    ],\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(CW)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "cw_mean_experiment_config = dict(\n",
    "    batch_size=128,\n",
    "    max_epochs=50,\n",
    "    pre_train_epochs=50,\n",
    "    post_train_epochs=0,\n",
    "    cw_train_freq=20,\n",
    "    cw_train_batch_steps=20,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    filter_groups=[\n",
    "        [(8, (7, 7), False)],\n",
    "        [(16, (5, 5), False)],\n",
    "        [(32, (3, 3), False)],\n",
    "        [(64, (3, 3), True)],\n",
    "    ],\n",
    "    units=[64, 64, 64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(RESULTS_DIR, \"cw/balanced_multiclass_purity\"),\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_train[0].shape[1:],\n",
    "    num_concepts=balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    cw_layer=14,\n",
    "    aggregator='mean',\n",
    "    activation_mode='mean',\n",
    "    cw_train_iterations=1,\n",
    "    concept_auc_freq=0,\n",
    "    holdout_fraction=0.1,\n",
    "    heatmap_display=True,\n",
    "    T=5,\n",
    "    eps=1e-5,\n",
    "    momentum=0.9,\n",
    "    initial_tau=1000,\n",
    "    initial_beta=1e8,\n",
    "    initial_alpha=0,\n",
    "    drop_prob=0,\n",
    "    exclusive_concepts=False,\n",
    "    exclusive_true_concepts=False,\n",
    "    feature_map=False,\n",
    "    add='_mean',\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "cw_mean_experiment_config['niching_results_dir'] = os.path.join(\n",
    "    NICHING_RESULTS_DIR,\n",
    "    f'cw/base_feature_{cw_mean_experiment_config[\"feature_map\"]}{cw_mean_experiment_config[\"add\"]}',\n",
    ")\n",
    "\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "cw_mean_results = cw_experiment_loop(\n",
    "    cw_mean_experiment_config,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0\", train_0, test_0),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1\", train_1, test_1),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2\", train_2, test_2),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3\", train_3, test_3),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4\", train_4, test_4),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_5\", train_5, test_5),\n",
    "    ],\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada-ML-VAE Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weakly Labelled Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwtrain_0 = [np.concatenate((train_0[0], train_0[0]), axis=1), train_0[2], train_0[1]]\n",
    "cwtrain_1 = [np.concatenate((train_1[0], train_1[0]), axis=1), train_1[2], train_1[1]]\n",
    "cwtrain_2 = [np.concatenate((train_2[0], train_2[0]), axis=1), train_2[2], train_2[1]]\n",
    "cwtrain_3 = [np.concatenate((train_3[0], train_3[0]), axis=1), train_3[2], train_3[1]]\n",
    "cwtrain_4 = [np.concatenate((train_4[0], train_4[0]), axis=1), train_4[2], train_4[1]]\n",
    "cwtrain_5 = [np.concatenate((train_5[0], train_5[0]), axis=1), train_5[2], train_5[1]]\n",
    "\n",
    "cwtest_0 = [np.concatenate((test_0[0], test_0[0]), axis=1), test_0[2], test_0[1]]\n",
    "cwtest_1 = [np.concatenate((test_1[0], test_1[0]), axis=1), test_1[2], test_1[1]]\n",
    "cwtest_2 = [np.concatenate((test_2[0], test_2[0]), axis=1), test_2[2], test_2[1]]\n",
    "cwtest_3 = [np.concatenate((test_3[0], test_3[0]), axis=1), test_3[2], test_3[1]]\n",
    "cwtest_4 = [np.concatenate((test_4[0], test_4[0]), axis=1), test_4[2], test_4[1]]\n",
    "cwtest_5 = [np.concatenate((test_5[0], test_5[0]), axis=1), test_5[2], test_5[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.VAE.weak_vae as weak_vae\n",
    "import concepts_xai.methods.VAE.baseVAE as base_vae\n",
    "import concepts_xai.methods.VAE.losses as vae_losses\n",
    "reload(vae_losses)\n",
    "reload(weak_vae)\n",
    "\n",
    "def construct_vae_encoder(\n",
    "    input_shape,\n",
    "    filter_groups,\n",
    "    units,\n",
    "    latent_dims,\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    include_norm=False,\n",
    "    include_pool=False,\n",
    "):\n",
    "    encoder_inputs = tf.keras.Input(shape=input_shape)\n",
    "    encoder_compute_graph = encoder_inputs\n",
    "    \n",
    "    # Start with our convolutions\n",
    "    num_convs = 0\n",
    "    for filter_group in filter_groups:\n",
    "        for filter_args in filter_group:\n",
    "            if len(filter_args) == 2:\n",
    "                filter_args = (*filter_args, 1)\n",
    "            (num_filters, kernel_size, stride) = filter_args\n",
    "            encoder_compute_graph = tf.keras.layers.Conv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                strides=stride,\n",
    "                padding=\"SAME\",\n",
    "                activation=None if include_norm else \"relu\",\n",
    "                name=f'encoder_conv_{num_convs}',\n",
    "            )(encoder_compute_graph)\n",
    "            num_convs += 1\n",
    "            if include_norm:\n",
    "                encoder_compute_graph = tf.keras.layers.BatchNormalization()(\n",
    "                    encoder_compute_graph\n",
    "                )\n",
    "                encoder_compute_graph = tf.keras.activations.relu(encoder_compute_graph)\n",
    "        if include_pool:\n",
    "            # Then do a max pool here to control the parameter count of the model\n",
    "            # at the end of each group\n",
    "            encoder_compute_graph = tf.keras.layers.MaxPooling2D(\n",
    "                pool_size=max_pool_window,\n",
    "                strides=max_pool_stride,\n",
    "            )(\n",
    "                encoder_compute_graph\n",
    "            )\n",
    "    \n",
    "    # Flatten this guy\n",
    "    encoder_compute_graph = tf.keras.layers.Flatten()(encoder_compute_graph)\n",
    "    \n",
    "    # Add a dropout if requested\n",
    "    if drop_prob:\n",
    "        encoder_compute_graph = tf.keras.layers.Dropout(drop_prob)(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Finally, include the fully connected bottleneck here\n",
    "    for i, units in enumerate(units):\n",
    "        encoder_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(encoder_compute_graph)\n",
    "\n",
    "    mean = tf.keras.layers.Dense(latent_dims, activation=None, name=\"means\")(encoder_compute_graph)\n",
    "    log_var = tf.keras.layers.Dense(latent_dims, activation=None, name=\"log_var\")(encoder_compute_graph)\n",
    "    return tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        [mean, log_var],\n",
    "        name=\"encoder\",\n",
    "    )\n",
    "\n",
    "def construct_vae_decoder(\n",
    "    units,\n",
    "    output_shape,\n",
    "    latent_dims,\n",
    "):\n",
    "    \"\"\"CNN decoder architecture used in the 'Challenging Common Assumptions in the Unsupervised Learning\n",
    "       of Disentangled Representations' paper (https://arxiv.org/abs/1811.12359)\n",
    "\n",
    "       Note: model is uncompiled\n",
    "    \"\"\"\n",
    "\n",
    "    latent_inputs = tf.keras.Input(shape=(latent_dims,))\n",
    "    model_out = latent_inputs\n",
    "    for unit in units:\n",
    "        model_out = tf.keras.layers.Dense(\n",
    "            unit,\n",
    "            activation='relu',\n",
    "        )(model_out)\n",
    "    model_out = tf.keras.layers.Reshape([4, 4, 32])(model_out)\n",
    "\n",
    "    model_out = tf.keras.layers.Conv2DTranspose(\n",
    "        filters=64,\n",
    "        kernel_size=4,\n",
    "        strides=2,\n",
    "        activation='relu',\n",
    "        padding=\"same\"\n",
    "    )(model_out)\n",
    "\n",
    "    model_out = tf.keras.layers.Conv2DTranspose(\n",
    "        filters=32,\n",
    "        kernel_size=4,\n",
    "        strides=2,\n",
    "        activation='relu',\n",
    "        padding=\"same\"\n",
    "    )(model_out)\n",
    "\n",
    "    model_out = tf.keras.layers.Conv2DTranspose(\n",
    "        filters=32,\n",
    "        kernel_size=4,\n",
    "        strides=2,\n",
    "        activation='relu',\n",
    "        padding=\"same\",\n",
    "    )(model_out)\n",
    "\n",
    "    model_out = tf.keras.layers.Conv2DTranspose(\n",
    "        filters=output_shape[-1],\n",
    "        kernel_size=4,\n",
    "        strides=2,\n",
    "        padding=\"same\",\n",
    "        activation=None,\n",
    "    )(model_out)\n",
    "    model_out = tf.keras.layers.Reshape(output_shape)(model_out)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=latent_inputs,\n",
    "        outputs=[model_out],\n",
    "    )\n",
    "\n",
    "def construct_wvae(\n",
    "    input_shape,\n",
    "    latent_dims,\n",
    "    filter_groups,\n",
    "    encoder_units,\n",
    "    decoder_units,\n",
    "    drop_prob=0.5,\n",
    "    include_pool=False,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    learning_rate=1e-3,\n",
    "    beta=1.0,\n",
    "    vae_model=weak_vae.MLVaeArgmax,\n",
    "):\n",
    "    wvae_encoder = construct_vae_encoder(\n",
    "        input_shape=input_shape,\n",
    "        filter_groups=filter_groups,\n",
    "        units=encoder_units,\n",
    "        drop_prob=drop_prob,\n",
    "        include_pool=include_pool,\n",
    "        max_pool_window=max_pool_window,\n",
    "        max_pool_stride=max_pool_stride,\n",
    "        latent_dims=latent_dims,\n",
    "    )\n",
    "    wvae_decoder = construct_vae_decoder(\n",
    "        output_shape=input_shape,\n",
    "        units=decoder_units,\n",
    "        latent_dims=latent_dims,\n",
    "    )\n",
    "\n",
    "    wvae_model = vae_model(\n",
    "        encoder=wvae_encoder,\n",
    "        decoder=wvae_decoder,\n",
    "        loss_fn=vae_losses.bernoulli_fn_wrapper(),\n",
    "        beta=beta,\n",
    "    )\n",
    "    wvae_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    )\n",
    "    return wvae_model\n",
    "\n",
    "def construct_pretrained_wvae(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    learning_rate=1e-3,\n",
    "    beta=1.0,\n",
    "    vae_model=weak_vae.MLVaeArgmax,\n",
    "):\n",
    "    wvae_model = vae_model(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        loss_fn=vae_losses.l2_loss_wrapper(),\n",
    "        beta=beta,\n",
    "    )\n",
    "    wvae_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    )\n",
    "    return wvae_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import concepts_xai.evaluation.metrics.purity as purity\n",
    "import concepts_xai.methods.VAE.betaVAE as beta_vae\n",
    "\n",
    "############################################################################\n",
    "## Experiment loop\n",
    "############################################################################\n",
    "\n",
    "def construct_trivial_auc_mat(num_concepts):\n",
    "    result = np.ones((num_concepts, num_concepts), dtype=np.float32) * 0.5\n",
    "    return result + np.eye(num_concepts, dtype=np.float32) * 0.5\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def wvae_experiment_loop(\n",
    "    experiment_config,\n",
    "    datasets,\n",
    "    wvae,\n",
    "    latent='',\n",
    "    load_from_cache=False,\n",
    "    include_all_losses=True,\n",
    "    vae_model=weak_vae.MLVaeArgmax,\n",
    "    oracle_matrix_cache=None,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    if vae_model != beta_vae.BetaVAE:\n",
    "        prefix = \"\"\n",
    "        split_fn = lambda x: x[:, :x.shape[1]//2, ...] if len(x.shape) > 1 else x[:x.shape[0]//2]\n",
    "    else:\n",
    "        prefix = \"balanced_\"\n",
    "        split_fn = lambda x: x\n",
    "    experiment_variables = dict(\n",
    "        config = experiment_config,\n",
    "        niss=[],\n",
    "    )\n",
    "    res_dir = experiment_config['niching_results_dir']\n",
    "    if load_from_cache:\n",
    "        if os.path.exists(os.path.join(res_dir, 'results_niching.joblib')):\n",
    "            experiment_variables = load(os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    count = 0\n",
    "    start_ind = 0\n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    for (ds_name, th, (x_train, c_train, y_train), (x_test, c_test, y_test)) in datasets[start_ind:]:\n",
    "        latent_dim = experiment_config['latent_dim']\n",
    "        print(\"Training with latent dimensions\", latent_dim, \"in dataset\", ds_name)\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']}\")\n",
    "            \n",
    "            # Time to actually construct and train the WVAE\n",
    "            wvae_model = construct_wvae(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                latent_dims=latent_dim,\n",
    "                filter_groups=experiment_config[\"filter_groups\"],\n",
    "                encoder_units=experiment_config[\"encoder_units\"],\n",
    "                decoder_units=experiment_config[\"decoder_units\"],\n",
    "                drop_prob=experiment_config['drop_prob'],\n",
    "                max_pool_window=experiment_config['max_pool_window'],\n",
    "                max_pool_stride=experiment_config['max_pool_stride'],\n",
    "                beta=experiment_config['beta'],\n",
    "                vae_model=vae_model,\n",
    "            )\n",
    "            \n",
    "                \n",
    "            print(\"\\t\\tWVAE training completed\")\n",
    "            \n",
    "            wvae_model.encoder = load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/{prefix}{ds_name}_encoder_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            print(f\"\\t\\tComputing purity score...\")\n",
    "            c_train_pred = wvae_model.sample_from_latent_distribution(\n",
    "                *wvae_model.encoder(split_fn(x_train))\n",
    "            ).numpy()\n",
    "            c_test_pred = wvae_model.sample_from_latent_distribution(\n",
    "                *wvae_model.encoder(split_fn(x_test))\n",
    "            ).numpy()\n",
    "            experiment_variables['niss'].append(\n",
    "                niching.niche_impurity_score(\n",
    "                    c_soft=c_test_pred,\n",
    "                    c_true=c_test,\n",
    "                    c_soft_train=c_train_pred,\n",
    "                    c_true_train=c_train,\n",
    "                )\n",
    "            )\n",
    "            print(f'\\t\\tNIS: {experiment_variables[\"niss\"][-1]:.2f}')\n",
    "            \n",
    "            os.makedirs(res_dir, exist_ok=True)\n",
    "            dump(experiment_variables, os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            \n",
    "    return experiment_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(weak_vae)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "ada_mlvae_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=multiclass_task_bin_concepts_dep_0_train[0].shape[1:],\n",
    "    latent_dim=multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[256, 512],\n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    beta=1,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"ada_ml_vae/multilabel_purity_latent_{multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}\"\n",
    "    ),\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        f'ada_ml_vae/graph_dependency_multiclass_tasks_purity_latent_{multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}',\n",
    "    ),\n",
    "    num_concepts=multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "ada_mlvae_experiment_config['niching_results_dir'] = os.path.join(\n",
    "    NICHING_RESULTS_DIR,\n",
    "    f'ada_ml_vae/graph_dependency_multiclass_tasks_purity_latent_{multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}',\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(ada_mlvae_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "ada_mlvae_figure_dir = os.path.join(ada_mlvae_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(ada_mlvae_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "ada_mlvae_results = wvae_experiment_loop(\n",
    "    ada_mlvae_experiment_config,\n",
    "    wvae=\"ada_ml_vae\",\n",
    "    latent=f\"_latent_{multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}\",\n",
    "    include_all_losses=True,\n",
    "    vae_model=weak_vae.MLVaeArgmax,\n",
    "    datasets=[\n",
    "        (\"multiclass_task_bin_concepts_dep_0\", 0.2, cwtrain_0, cwtest_0),\n",
    "        (\"multiclass_task_bin_concepts_dep_1\", 0.2, cwtrain_1, cwtest_1),\n",
    "        (\"multiclass_task_bin_concepts_dep_2\", 0.2, cwtrain_2, cwtest_2),\n",
    "        (\"multiclass_task_bin_concepts_dep_3\", 0.2, cwtrain_3, cwtest_3),\n",
    "        (\"multiclass_task_bin_concepts_dep_4\", 0.2, cwtrain_4, cwtest_4),\n",
    "        (\"multiclass_task_bin_concepts_dep_5\", 0.2, cwtrain_5, cwtest_5),\n",
    "    ],\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(weak_vae)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "ada_mlvae_extended_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=multiclass_task_bin_concepts_dep_0_train[0].shape[1:],\n",
    "    latent_dim=(2 * multiclass_task_bin_concepts_dep_0_train[2].shape[-1]),\n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[256, 512],\n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    beta=1,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"ada_ml_vae/multilabel_extended_purity_latent_{2 * multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}\"\n",
    "    ),\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        f'ada_ml_vae/graph_dependency_multiclass_tasks_purity_latent_{2 * multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}',\n",
    "    ),\n",
    "    num_concepts=multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "ada_mlvae_extended_experiment_config['niching_results_dir'] = os.path.join(\n",
    "    NICHING_RESULTS_DIR,\n",
    "    f'ada_ml_vae/graph_dependency_multiclass_tasks_purity_latent_{2 * multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}',\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(ada_mlvae_extended_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "ada_mlvae_extended_figure_dir = os.path.join(ada_mlvae_extended_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(ada_mlvae_extended_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "ada_mlvae_extended_results = wvae_experiment_loop(\n",
    "    ada_mlvae_extended_experiment_config,\n",
    "    wvae=\"ada_ml_vae\",\n",
    "    latent=f\"_latent_{2 * multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}\",\n",
    "    include_all_losses=True,\n",
    "    vae_model=weak_vae.MLVaeArgmax,\n",
    "    datasets=[\n",
    "        (\"multiclass_task_bin_concepts_dep_0\", 0.2, cwtrain_0, cwtest_0),\n",
    "        (\"multiclass_task_bin_concepts_dep_1\", 0.2, cwtrain_1, cwtest_1),\n",
    "        (\"multiclass_task_bin_concepts_dep_2\", 0.2, cwtrain_2, cwtest_2),\n",
    "        (\"multiclass_task_bin_concepts_dep_3\", 0.2, cwtrain_3, cwtest_3),\n",
    "        (\"multiclass_task_bin_concepts_dep_4\", 0.2, cwtrain_4, cwtest_4),\n",
    "        (\"multiclass_task_bin_concepts_dep_5\", 0.2, cwtrain_5, cwtest_5),\n",
    "    ],\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada-GVAE Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Concepts Task (Independent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(weak_vae)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "ada_gvae_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=multiclass_task_bin_concepts_dep_0_train[0].shape[1:],\n",
    "    latent_dim=multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[256, 512],\n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    beta=1,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"ada_g_vae/multilabel_purity_latent_{multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}\"\n",
    "    ),\n",
    "    num_concepts=multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "ada_gvae_experiment_config['niching_results_dir'] = os.path.join(\n",
    "    NICHING_RESULTS_DIR,\n",
    "    f'ada_g_vae/graph_dependency_multiclass_tasks_purity_latent_{multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}',\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(ada_gvae_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "ada_gvae_figure_dir = os.path.join(ada_gvae_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(ada_gvae_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "ada_gvae_results = wvae_experiment_loop(\n",
    "    ada_gvae_experiment_config,\n",
    "    wvae=\"ada_g_vae\",\n",
    "    latent=f\"_latent_{multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}\",\n",
    "    include_all_losses=True,\n",
    "    vae_model=weak_vae.GroupVAEArgmax,\n",
    "    datasets=[\n",
    "        (\"multiclass_task_bin_concepts_dep_0\", 0.2, cwtrain_0, cwtest_0),\n",
    "        (\"multiclass_task_bin_concepts_dep_1\", 0.2, cwtrain_1, cwtest_1),\n",
    "        (\"multiclass_task_bin_concepts_dep_2\", 0.2, cwtrain_2, cwtest_2),\n",
    "        (\"multiclass_task_bin_concepts_dep_3\", 0.2, cwtrain_3, cwtest_3),\n",
    "        (\"multiclass_task_bin_concepts_dep_4\", 0.2, cwtrain_4, cwtest_4),\n",
    "        (\"multiclass_task_bin_concepts_dep_5\", 0.2, cwtrain_5, cwtest_5),\n",
    "    ],\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(weak_vae)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "ada_gvae_extended_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=multiclass_task_bin_concepts_dep_0_train[0].shape[1:],\n",
    "    latent_dim=multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[256, 512],\n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    beta=1,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"ada_g_vae/multilabel_extended_purity_latent_{2*multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}\"\n",
    "    ),\n",
    "    num_concepts=2*multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "ada_gvae_extended_experiment_config['niching_results_dir'] = os.path.join(\n",
    "    NICHING_RESULTS_DIR,\n",
    "    f'ada_g_vae/graph_dependency_multiclass_tasks_purity_latent_{2 * multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}',\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(ada_gvae_extended_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "ada_gvae_extended_figure_dir = os.path.join(ada_gvae_extended_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(ada_gvae_extended_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "ada_gvae_extended_results = wvae_experiment_loop(\n",
    "    ada_gvae_extended_experiment_config,\n",
    "    wvae=\"ada_g_vae\",\n",
    "    latent=f\"_latent_{2*multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}\",\n",
    "    include_all_losses=True,\n",
    "    vae_model=weak_vae.GroupVAEArgmax,\n",
    "    datasets=[\n",
    "        (\"multiclass_task_bin_concepts_dep_0\", 0.2, cwtrain_0, cwtest_0),\n",
    "        (\"multiclass_task_bin_concepts_dep_1\", 0.2, cwtrain_1, cwtest_1),\n",
    "        (\"multiclass_task_bin_concepts_dep_2\", 0.2, cwtrain_2, cwtest_2),\n",
    "        (\"multiclass_task_bin_concepts_dep_3\", 0.2, cwtrain_3, cwtest_3),\n",
    "        (\"multiclass_task_bin_concepts_dep_4\", 0.2, cwtrain_4, cwtest_4),\n",
    "        (\"multiclass_task_bin_concepts_dep_5\", 0.2, cwtrain_5, cwtest_5),\n",
    "    ],\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta-VAE Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.VAE.betaVAE as beta_vae\n",
    "\n",
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(beta_vae)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "beta_vae_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_train[0].shape[1:],\n",
    "    latent_dim=balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    beta=10,\n",
    "    \n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[256, 512],\n",
    "    \n",
    "    latent_decoder_units=[64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    predictor_max_epochs=100,\n",
    "    \n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"beta_vae/balanced_multilabel_purity_latent_{balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}_beta_{10}\"\n",
    "    ),\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        f\"beta_vae/purity_latent_{balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}_beta_{10}\"\n",
    "    ),\n",
    "    num_concepts=balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(beta_vae_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "beta_vae_figure_dir = os.path.join(beta_vae_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(beta_vae_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "beta_vae_results = wvae_experiment_loop(\n",
    "    beta_vae_experiment_config,\n",
    "    wvae=\"beta_vae\",\n",
    "    latent=f\"_latent_{beta_vae_experiment_config['latent_dim']}\",\n",
    "    include_all_losses=True,\n",
    "    vae_model=beta_vae.BetaVAE,\n",
    "    datasets=[\n",
    "        (\"multiclass_task_bin_concepts_dep_0\", 0.2, train_0, test_0),\n",
    "        (\"multiclass_task_bin_concepts_dep_1\", 0.2, train_1, test_1),\n",
    "        (\"multiclass_task_bin_concepts_dep_2\", 0.2, train_2, test_2),\n",
    "        (\"multiclass_task_bin_concepts_dep_3\", 0.2, train_3, test_3),\n",
    "        (\"multiclass_task_bin_concepts_dep_4\", 0.2, train_4, test_4),\n",
    "        (\"multiclass_task_bin_concepts_dep_5\", 0.2, train_5, test_5),\n",
    "    ],\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.VAE.betaVAE as beta_vae\n",
    "\n",
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(beta_vae)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "beta_vae_extended_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_train[0].shape[1:],\n",
    "    latent_dim=2*balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    beta=10,\n",
    "    \n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[256, 512],\n",
    "    \n",
    "    latent_decoder_units=[64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    predictor_max_epochs=100,\n",
    "    \n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"beta_vae/balanced_multilabel_purity_latent_{2*balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}_beta_{10}\"\n",
    "    ),\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        f\"beta_vae/purity_latent_{2*balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}_beta_{10}\"\n",
    "    ),\n",
    "    num_concepts=balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(beta_vae_extended_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "beta_vae_extended_figure_dir = os.path.join(beta_vae_extended_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(beta_vae_extended_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "beta_vae_extended_results = wvae_experiment_loop(\n",
    "    beta_vae_extended_experiment_config,\n",
    "    wvae=\"beta_vae\",\n",
    "    latent=f\"_latent_{beta_vae_extended_experiment_config['latent_dim']}\",\n",
    "    include_all_losses=True,\n",
    "    vae_model=beta_vae.BetaVAE,\n",
    "    datasets=[\n",
    "        (\"multiclass_task_bin_concepts_dep_0\", 0.2, train_0, test_0),\n",
    "        (\"multiclass_task_bin_concepts_dep_1\", 0.2, train_1, test_1),\n",
    "        (\"multiclass_task_bin_concepts_dep_2\", 0.2, train_2, test_2),\n",
    "        (\"multiclass_task_bin_concepts_dep_3\", 0.2, train_3, test_3),\n",
    "        (\"multiclass_task_bin_concepts_dep_4\", 0.2, train_4, test_4),\n",
    "        (\"multiclass_task_bin_concepts_dep_5\", 0.2, train_5, test_5),\n",
    "    ],\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.VAE.betaVAE as beta_vae\n",
    "\n",
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(beta_vae)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "vae_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_train[0].shape[1:],\n",
    "    latent_dim=balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    beta=1,\n",
    "    \n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[256, 512],\n",
    "    \n",
    "    latent_decoder_units=[64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    predictor_max_epochs=100,\n",
    "    \n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"beta_vae/balanced_multilabel_purity_latent_{balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}_beta_{1}\"\n",
    "    ),\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        f\"beta_vae/purity_latent_{balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}_beta_{1}\"\n",
    "    ),\n",
    "    num_concepts=balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(vae_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "vae_figure_dir = os.path.join(vae_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(vae_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "vae_results = wvae_experiment_loop(\n",
    "    vae_experiment_config,\n",
    "    wvae=\"beta_vae\",\n",
    "    latent=f\"_latent_{vae_experiment_config['latent_dim']}\",\n",
    "    include_all_losses=True,\n",
    "    vae_model=beta_vae.BetaVAE,\n",
    "    datasets=[\n",
    "        (\"multiclass_task_bin_concepts_dep_0\", 0.2, train_0, test_0),\n",
    "        (\"multiclass_task_bin_concepts_dep_1\", 0.2, train_1, test_1),\n",
    "        (\"multiclass_task_bin_concepts_dep_2\", 0.2, train_2, test_2),\n",
    "        (\"multiclass_task_bin_concepts_dep_3\", 0.2, train_3, test_3),\n",
    "        (\"multiclass_task_bin_concepts_dep_4\", 0.2, train_4, test_4),\n",
    "        (\"multiclass_task_bin_concepts_dep_5\", 0.2, train_5, test_5),\n",
    "    ],\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.VAE.betaVAE as beta_vae\n",
    "\n",
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(beta_vae)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "vae_extended_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_train[0].shape[1:],\n",
    "    latent_dim=2*balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    beta=1,\n",
    "    \n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[256, 512],\n",
    "    \n",
    "    latent_decoder_units=[64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    predictor_max_epochs=100,\n",
    "    \n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"beta_vae/balanced_multilabel_purity_latent_{2*balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}_beta_{1}\"\n",
    "    ),\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        f\"beta_vae/purity_latent_{2*balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}_beta_{1}\"\n",
    "    ),\n",
    "    num_concepts=balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(vae_extended_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "vae_figure_dir = os.path.join(vae_extended_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(vae_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "vae_extended_results = wvae_experiment_loop(\n",
    "    vae_extended_experiment_config,\n",
    "    wvae=\"beta_vae\",\n",
    "    latent=f\"_latent_{vae_extended_experiment_config['latent_dim']}\",\n",
    "    include_all_losses=True,\n",
    "    vae_model=beta_vae.BetaVAE,\n",
    "    datasets=[\n",
    "        (\"multiclass_task_bin_concepts_dep_0\", 0.2, train_0, test_0),\n",
    "        (\"multiclass_task_bin_concepts_dep_1\", 0.2, train_1, test_1),\n",
    "        (\"multiclass_task_bin_concepts_dep_2\", 0.2, train_2, test_2),\n",
    "        (\"multiclass_task_bin_concepts_dep_3\", 0.2, train_3, test_3),\n",
    "        (\"multiclass_task_bin_concepts_dep_4\", 0.2, train_4, test_4),\n",
    "        (\"multiclass_task_bin_concepts_dep_5\", 0.2, train_5, test_5),\n",
    "    ],\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCD Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_ccd_encoder(\n",
    "    input_shape,\n",
    "    filter_groups,\n",
    "    units,\n",
    "    latent_dims,\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    latent_act=None,  # Leaving sigmoid as used in original paper\n",
    "):\n",
    "    encoder_inputs = tf.keras.Input(shape=input_shape)\n",
    "    encoder_compute_graph = encoder_inputs\n",
    "    \n",
    "    # Start with our convolutions\n",
    "    num_convs = 0\n",
    "    for filter_group in filter_groups:\n",
    "        for (num_filters, kernel_size) in filter_group:\n",
    "            encoder_compute_graph = tf.keras.layers.Conv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=\"SAME\",\n",
    "                activation=None,\n",
    "                name=f'encoder_conv_{num_convs}',\n",
    "            )(encoder_compute_graph)\n",
    "            num_convs += 1\n",
    "            encoder_compute_graph = tf.keras.layers.BatchNormalization()(\n",
    "                encoder_compute_graph\n",
    "            )\n",
    "            encoder_compute_graph = tf.keras.activations.relu(encoder_compute_graph)\n",
    "        # Then do a max pool here to control the parameter count of the model\n",
    "        # at the end of each group\n",
    "        encoder_compute_graph = tf.keras.layers.MaxPooling2D(\n",
    "            pool_size=max_pool_window,\n",
    "            strides=max_pool_stride,\n",
    "        )(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Flatten this guy\n",
    "    encoder_compute_graph = tf.keras.layers.Flatten()(encoder_compute_graph)\n",
    "    \n",
    "    # Add a dropout if requested\n",
    "    if drop_prob:\n",
    "        encoder_compute_graph = tf.keras.layers.Dropout(drop_prob)(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Finally, include the fully connected bottleneck here\n",
    "    for i, units in enumerate(units):\n",
    "        encoder_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(encoder_compute_graph)\n",
    "    \n",
    "    # TIme to generate the latent code here\n",
    "    encoder_compute_graph = tf.keras.layers.Dense(\n",
    "        latent_dims,\n",
    "        activation=latent_act,\n",
    "        name=\"encoder_bypass_channel\",\n",
    "    )(encoder_compute_graph)\n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        encoder_compute_graph,\n",
    "        name=\"encoder\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_ccd_encoder(\n",
    "    input_shape,\n",
    "    filter_groups,\n",
    "    units,\n",
    "    latent_dims,\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    latent_act=None,  #\"sigmoid\",  # Leaving sigmoid as used in original paper\n",
    "):\n",
    "    encoder_inputs = tf.keras.Input(shape=input_shape)\n",
    "    encoder_compute_graph = encoder_inputs\n",
    "    \n",
    "    # Start with our convolutions\n",
    "    num_convs = 0\n",
    "    for filter_group in filter_groups:\n",
    "        for (num_filters, kernel_size) in filter_group:\n",
    "            encoder_compute_graph = tf.keras.layers.Conv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=\"SAME\",\n",
    "                activation=None,\n",
    "                name=f'encoder_conv_{num_convs}',\n",
    "            )(encoder_compute_graph)\n",
    "            num_convs += 1\n",
    "            encoder_compute_graph = tf.keras.layers.BatchNormalization()(\n",
    "                encoder_compute_graph\n",
    "            )\n",
    "            encoder_compute_graph = tf.keras.activations.relu(encoder_compute_graph)\n",
    "        # Then do a max pool here to control the parameter count of the model\n",
    "        # at the end of each group\n",
    "        encoder_compute_graph = tf.keras.layers.MaxPooling2D(\n",
    "            pool_size=max_pool_window,\n",
    "            strides=max_pool_stride,\n",
    "        )(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Flatten this guy\n",
    "    encoder_compute_graph = tf.keras.layers.Flatten()(encoder_compute_graph)\n",
    "    \n",
    "    # Add a dropout if requested\n",
    "    if drop_prob:\n",
    "        encoder_compute_graph = tf.keras.layers.Dropout(drop_prob)(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Finally, include the fully connected bottleneck here\n",
    "    for i, units in enumerate(units):\n",
    "        encoder_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(encoder_compute_graph)\n",
    "    \n",
    "    # TIme to generate the latent code here\n",
    "    encoder_compute_graph = tf.keras.layers.Dense(\n",
    "        latent_dims,\n",
    "        activation=latent_act,\n",
    "        name=\"encoder_bypass_channel\",\n",
    "    )(encoder_compute_graph)\n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        encoder_compute_graph,\n",
    "        name=\"encoder\",\n",
    "    )\n",
    "\n",
    "def construct_ccd_decoder(units, num_outputs):\n",
    "    decoder_layers = [\n",
    "        tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation=tf.nn.relu,\n",
    "            name=f\"decoder_dense_{i+1}\",\n",
    "        ) for i, units in enumerate(units)\n",
    "    ]\n",
    "    return tf.keras.Sequential(\n",
    "        [tf.keras.layers.Flatten()] +\n",
    "        decoder_layers + [\n",
    "        tf.keras.layers.Dense(\n",
    "            num_outputs if num_outputs > 2 else 1,\n",
    "            activation=None,\n",
    "            name=\"decoder_model_output\",\n",
    "        )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.OCACE.topicModel as CCD\n",
    "import concepts_xai.evaluation.metrics.purity as purity\n",
    "import concepts_xai.evaluation.metrics.completeness as completeness\n",
    "\n",
    "############################################################################\n",
    "## Experiment loop\n",
    "############################################################################\n",
    "\n",
    "def ccd_experiment_loop(\n",
    "    experiment_config,\n",
    "    datasets,\n",
    "    load_from_cache=False,\n",
    "    oracle_matrix_cache=None,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    experiment_variables = dict(\n",
    "        config = experiment_config,\n",
    "        niss=[],\n",
    "    )\n",
    "    num_concepts = experiment_config[\"num_concepts\"]\n",
    "    res_dir = experiment_config[\"niching_results_dir\"]\n",
    "    if load_from_cache:\n",
    "        if os.path.exists(os.path.join(res_dir, 'results_niching.joblib')):\n",
    "            experiment_variables = load(os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    count = 0\n",
    "    start_ind = 0\n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    for (ds_name, (x_train, y_train, c_train), (x_test, y_test, c_test)) in datasets[start_ind:]:\n",
    "        print(\"Training with concepts\", num_concepts, \"in dataset\", ds_name)\n",
    "        \n",
    "        channels_axis = (\n",
    "            -1 if experiment_config.get(\"data_format\", \"channels_last\") == \"channels_last\"\n",
    "            else 1\n",
    "        )\n",
    "        \n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} for {num_concepts} concepts\")\n",
    "            print(\"x_train.shape =\", x_train.shape)\n",
    "            print(\"y_train.shape =\", y_train.shape)\n",
    "            print(\"c_train.shape =\", c_train.shape)\n",
    "            # Proceed to do and end-to-end model in case we want to\n",
    "            # do some task-specific pretraining\n",
    "            end_to_end_model, encoder, decoder = construct_end_to_end_model(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "                learning_rate=experiment_config[\"learning_rate\"],\n",
    "                encoder=construct_ccd_encoder(\n",
    "                    input_shape=experiment_config[\"input_shape\"],\n",
    "                    filter_groups=experiment_config[\"encoder_filter_groups\"],\n",
    "                    latent_dims=experiment_config[\"latent_dims\"],\n",
    "                    units=experiment_config[\"encoder_units\"],\n",
    "                    drop_prob=experiment_config.get(\"drop_prob\", 0.5),\n",
    "                    max_pool_window=experiment_config.get(\"max_pool_window\", (2, 2)),\n",
    "                    max_pool_stride=experiment_config.get(\"max_pool_stride\", (2, 2)),\n",
    "                    latent_act=experiment_config.get(\"latent_act\", None),\n",
    "                ),\n",
    "                decoder=construct_decoder(\n",
    "                    units=experiment_config[\"decoder_units\"],\n",
    "                    num_outputs=experiment_config[\"num_outputs\"],\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            print(\"\\t\\tModel pre-training completed\")\n",
    "            print(\"\\tSerializing model\")\n",
    "            \n",
    "            encoder = load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/{ds_name}_encoder_num_concepts_{num_concepts}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            decoder = load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/{ds_name}_decoder_num_concepts_{num_concepts}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            topic_vector = np.load(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/{ds_name}_topic_vector_num_concepts_{num_concepts}_trial_{trial}.npy\"\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            # Now extract our concept vectors\n",
    "            topic_model = CCD.TopicModel(\n",
    "                concepts_to_labels_model=decoder,\n",
    "                n_channels=experiment_config[\"latent_dims\"],\n",
    "                n_concepts=num_concepts,\n",
    "                threshold=experiment_config.get(\"threshold\", 0.5),\n",
    "                loss_fn=end_to_end_model.loss,\n",
    "                top_k=experiment_config.get(\"top_k\", 32),\n",
    "                lambda1=experiment_config.get(\"lambda1\", 0.1),\n",
    "                lambda2=experiment_config.get(\"lambda2\", 0.1),\n",
    "                seed=experiment_config.get(\"seed\", None),\n",
    "                eps=experiment_config.get(\"eps\", 1e-5),\n",
    "                data_format=experiment_config.get(\n",
    "                    \"data_format\",\n",
    "                    \"channels_last\"\n",
    "                ),\n",
    "                allow_gradient_flow_to_c2l=experiment_config.get(\n",
    "                    'allow_gradient_flow_to_c2l',\n",
    "                    False,\n",
    "                ),\n",
    "                acc_metric=(\n",
    "                    tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "                    if experiment_config[\"num_outputs\"] > 1 else\n",
    "                    tf.keras.metrics.BinaryAccuracy()\n",
    "                ),\n",
    "                g_model=load_model(\n",
    "                    os.path.join(\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                        f\"models/{ds_name}_topic_g_model_num_concepts_{num_concepts}_trial_{trial}\"\n",
    "                    )\n",
    "                ),\n",
    "                initial_topic_vector=topic_vector,\n",
    "            )\n",
    "            \n",
    "            print(f\"\\t\\tComputing purity score...\")\n",
    "            topic_model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(\n",
    "                    experiment_config.get(\"learning_rate\", 1e-3),\n",
    "                )\n",
    "            )\n",
    "            c_train_pred = topic_model.concept_scores(encoder(x_train)).numpy()\n",
    "            c_test_pred = topic_model.concept_scores(encoder(x_test)).numpy()\n",
    "            \n",
    "            \n",
    "            print(\"\\t\\tComputing niching scores...\")\n",
    "            # finding niches\n",
    "            print(\"Topic model evaluation:\", sklearn.metrics.accuracy_score(\n",
    "                np.argmax(y_test, axis=-1),\n",
    "                np.argmax(topic_model(encoder(x_test))[0], axis=-1),\n",
    "            ))\n",
    "            experiment_variables['niss'].append(\n",
    "                niching.niche_impurity_score(\n",
    "                    c_soft=c_test_pred,\n",
    "                    c_true=c_test,\n",
    "                    c_soft_train=c_train_pred,\n",
    "                    c_true_train=c_train,\n",
    "                )\n",
    "            )\n",
    "            print(f'\\t\\tNIS: {experiment_variables[\"niss\"][-1]:.2f}')\n",
    "            \n",
    "            \n",
    "            os.makedirs(res_dir, exist_ok=True)\n",
    "            dump(experiment_variables, os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            \n",
    "    return experiment_variables\n",
    "            \n",
    "\n",
    "def ccd_compute_k(y, batch_size):\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    avg_class_ratio = np.mean(counts) / y.shape[0]\n",
    "    return int((avg_class_ratio * batch_size) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(completeness)\n",
    "reload(CBM)\n",
    "reload(CCD)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "ccd_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    topic_model_train_epochs=50,\n",
    "    num_concepts=train_0[2].shape[-1],\n",
    "    latent_dims=10,\n",
    "    threshold=0.0,\n",
    "    top_k=ccd_compute_k(y=train_0[1], batch_size=32),\n",
    "    lambda1=0.1,\n",
    "    lambda2=0.1,\n",
    "    seed=42,\n",
    "    eps=1e-5,\n",
    "    learning_rate=1e-3,\n",
    "    encoder_filter_groups=[\n",
    "        [(8, (7, 7))],\n",
    "        [(16, (5, 5))],\n",
    "        [(32, (3, 3))],\n",
    "        [(64, (3, 3))],\n",
    "    ],\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[64, 64],\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    pax_pool_stride=2,\n",
    "    num_outputs=train_0[1].shape[-1],\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"ccd/balanced_multiclass_thresh_0_num_concepts_{train_0[2].shape[-1]}\"\n",
    "    ),\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        f'ccd/balanced_multiclass_num_concepts_{train_0[2].shape[-1]}',\n",
    "    ),\n",
    "    input_shape=multiclass_task_bin_concepts_dep_0_train[0].shape[1:],\n",
    "    holdout_fraction=0.1,\n",
    "    trials=5,\n",
    "    verbosity=0,\n",
    "    early_stop_metric=\"val_loss\",\n",
    "    early_stop_mode=\"min\",\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(ccd_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "ccd_figure_dir = os.path.join(ccd_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(ccd_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "num_concepts=multiclass_task_bin_concepts_dep_0_train[2].shape[-1]\n",
    "ccd_results = ccd_experiment_loop(\n",
    "    experiment_config=ccd_experiment_config,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0\", train_0, test_0),\n",
    "        (\"multiclass_task_bin_concepts_dep_1\", train_1, test_1),\n",
    "        (\"multiclass_task_bin_concepts_dep_2\", train_2, test_2),\n",
    "        (\"multiclass_task_bin_concepts_dep_3\", train_3, test_3),\n",
    "        (\"multiclass_task_bin_concepts_dep_4\", train_4, test_4),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0\", train_5, test_5),\n",
    "    ],\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(completeness)\n",
    "reload(CBM)\n",
    "reload(CCD)\n",
    "reload(niching)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "ccd_extended_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    topic_model_train_epochs=50,\n",
    "    num_concepts=2*multiclass_task_bin_concepts_dep_0_train[2].shape[-1],\n",
    "    latent_dims=10,\n",
    "    threshold=0.0,\n",
    "    top_k=ccd_compute_k(y=multiclass_task_bin_concepts_dep_0_train[1], batch_size=32),\n",
    "    lambda1=0.1,\n",
    "    lambda2=0.1,\n",
    "    seed=42,\n",
    "    eps=1e-5,\n",
    "    learning_rate=1e-3,\n",
    "    encoder_filter_groups=[\n",
    "        [(8, (7, 7))],\n",
    "        [(16, (5, 5))],\n",
    "        [(32, (3, 3))],\n",
    "        [(64, (3, 3))],\n",
    "    ],\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[64, 64],\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    pax_pool_stride=2,\n",
    "    num_outputs=(\n",
    "        len(set(multiclass_task_bin_concepts_dep_0_train[1])) if len(set(multiclass_task_bin_concepts_dep_0_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"ccd/balanced_multiclass_thresh_0_num_concepts_{2*multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}\"\n",
    "    ),\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        f'ccd/balanced_multiclass_num_concepts_{2*multiclass_task_bin_concepts_dep_0_train[2].shape[-1]}',\n",
    "    ),\n",
    "    input_shape=multiclass_task_bin_concepts_dep_0_train[0].shape[1:],\n",
    "    holdout_fraction=0.1,\n",
    "    trials=5,\n",
    "    verbosity=0,\n",
    "    early_stop_metric=\"val_loss\",\n",
    "    early_stop_mode=\"min\",\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(ccd_extended_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "ccd_extended_figure_dir = os.path.join(ccd_extended_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(ccd_extended_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "ccd_extended_results = ccd_experiment_loop(\n",
    "    experiment_config=ccd_extended_experiment_config,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0\", train_0, test_0),\n",
    "        (\"multiclass_task_bin_concepts_dep_1\", train_1, test_1),\n",
    "        (\"multiclass_task_bin_concepts_dep_2\", train_2, test_2),\n",
    "        (\"multiclass_task_bin_concepts_dep_3\", train_3, test_3),\n",
    "        (\"multiclass_task_bin_concepts_dep_4\", train_4, test_4),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_5\", train_5, test_5),\n",
    "    ],\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENN Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.SENN.base_senn as SENN\n",
    "import concepts_xai.methods.SENN.aggregators as aggregators\n",
    "reload(SENN)\n",
    "reload(aggregators)\n",
    "\n",
    "\n",
    "def construct_senn_coefficient_model(units, num_concepts, num_outputs):\n",
    "    decoder_layers = [tf.keras.layers.Flatten()] + [\n",
    "        tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation=tf.nn.relu,\n",
    "            name=f\"coefficient_model_dense_{i+1}\",\n",
    "        ) for i, units in enumerate(units)\n",
    "    ]\n",
    "    return tf.keras.Sequential(decoder_layers + [\n",
    "        tf.keras.layers.Dense(\n",
    "            num_concepts * num_outputs,\n",
    "            activation=None,\n",
    "            name=\"coefficient_model_output\",\n",
    "        ),\n",
    "        tf.keras.layers.Reshape([num_outputs, num_concepts])\n",
    "    ])\n",
    "\n",
    "def construct_senn_encoder(\n",
    "    input_shape,\n",
    "    filter_groups,\n",
    "    units,\n",
    "    latent_dims,\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    include_norm=False,\n",
    "    include_pool=False,\n",
    "):\n",
    "    encoder_inputs = tf.keras.Input(shape=input_shape)\n",
    "    encoder_compute_graph = encoder_inputs\n",
    "    \n",
    "    # Start with our convolutions\n",
    "    num_convs = 0\n",
    "    for filter_group in filter_groups:\n",
    "        for filter_args in filter_group:\n",
    "            if len(filter_args) == 2:\n",
    "                filter_args = (*filter_args, 1)\n",
    "            (num_filters, kernel_size, stride) = filter_args\n",
    "            encoder_compute_graph = tf.keras.layers.Conv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                strides=stride,\n",
    "                padding=\"SAME\",\n",
    "                activation=None if include_norm else \"relu\",\n",
    "                name=f'encoder_conv_{num_convs}',\n",
    "            )(encoder_compute_graph)\n",
    "            num_convs += 1\n",
    "            if include_norm:\n",
    "                encoder_compute_graph = tf.keras.layers.BatchNormalization()(\n",
    "                    encoder_compute_graph\n",
    "                )\n",
    "                encoder_compute_graph = tf.keras.activations.relu(encoder_compute_graph)\n",
    "        if include_pool:\n",
    "            # Then do a max pool here to control the parameter count of the model\n",
    "            # at the end of each group\n",
    "            encoder_compute_graph = tf.keras.layers.MaxPooling2D(\n",
    "                pool_size=max_pool_window,\n",
    "                strides=max_pool_stride,\n",
    "            )(\n",
    "                encoder_compute_graph\n",
    "            )\n",
    "    \n",
    "    # Flatten this guy\n",
    "    encoder_compute_graph = tf.keras.layers.Flatten()(encoder_compute_graph)\n",
    "    \n",
    "    # Add a dropout if requested\n",
    "    if drop_prob:\n",
    "        encoder_compute_graph = tf.keras.layers.Dropout(drop_prob)(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Finally, include the fully connected bottleneck here\n",
    "    for i, units in enumerate(units):\n",
    "        encoder_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(encoder_compute_graph)\n",
    "\n",
    "    mean = tf.keras.layers.Dense(latent_dims, activation=None, name=\"means\")(encoder_compute_graph)\n",
    "    log_var = tf.keras.layers.Dense(latent_dims, activation=None, name=\"log_var\")(encoder_compute_graph)\n",
    "    senn_encoder = tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        mean,\n",
    "        name=\"senn_encoder\",\n",
    "    )\n",
    "    vae_encoder = tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        [mean, log_var],\n",
    "        name=\"vae_encoder\",\n",
    "    )\n",
    "    return senn_encoder, vae_encoder\n",
    "\n",
    "def construct_senn_model(\n",
    "    concept_encoder,\n",
    "    concept_decoder,\n",
    "    coefficient_model,\n",
    "    num_outputs,\n",
    "    regularization_strength=0.1,\n",
    "    learning_rate=1e-3,\n",
    "    sparsity_strength=2e-5,\n",
    "):\n",
    "    def reconstruction_loss_fn(y_true, y_pred):\n",
    "        return vae_losses.bernoulli_fn_wrapper()(y_true, concept_decoder(y_pred))\n",
    "    senn_model = SENN.SelfExplainingNN(\n",
    "        encoder_model=concept_encoder,\n",
    "        coefficient_model=coefficient_model,\n",
    "        aggregator_fn=(\n",
    "            aggregators.multiclass_additive_aggregator if (num_outputs > 2)\n",
    "            else aggregators.scalar_additive_aggregator\n",
    "        ),\n",
    "        task_loss_fn=(\n",
    "            tf.keras.losses.BinaryCrossentropy(from_logits=True) if (num_outputs <= 2)\n",
    "            else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        ),\n",
    "        reconstruction_loss_fn=reconstruction_loss_fn,\n",
    "        regularization_strength=regularization_strength,\n",
    "        sparsity_strength=sparsity_strength,\n",
    "        name=\"SENN\",\n",
    "        metrics=[\n",
    "            tf.keras.metrics.BinaryAccuracy() if (num_outputs <= 2)\n",
    "            else tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "        ],\n",
    "    )\n",
    "    senn_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    )\n",
    "    return senn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.evaluation.metrics.purity as purity\n",
    "\n",
    "############################################################################\n",
    "## Experiment loop\n",
    "############################################################################\n",
    "\n",
    "def construct_trivial_auc_mat(num_concepts):\n",
    "    result = np.ones((num_concepts, num_concepts), dtype=np.float32) * 0.5\n",
    "    return result + np.eye(num_concepts, dtype=np.float32) * 0.5\n",
    "    \n",
    "def get_argmax_concept_explanations(preds, class_theta_scores):\n",
    "    inds = np.argmax(preds, axis=-1)\n",
    "    result = np.take_along_axis(\n",
    "        class_theta_scores,\n",
    "        np.expand_dims(np.expand_dims(inds, axis=-1), axis=-1),\n",
    "        axis=1,\n",
    "    )\n",
    "    return np.squeeze(result, axis=1)\n",
    "\n",
    "def senn_experiment_loop(\n",
    "    experiment_config,\n",
    "    datasets,\n",
    "    load_from_cache=False,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    experiment_variables = dict(\n",
    "        config = experiment_config,\n",
    "        niss=[],\n",
    "    )\n",
    "    num_concepts = experiment_config[\"num_concepts\"]\n",
    "    res_dir = experiment_config['niching_results_dir']\n",
    "    if load_from_cache:\n",
    "        if os.path.exists(os.path.join(res_dir, 'results_niching.joblib')):\n",
    "            experiment_variables = load(os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    count = 0\n",
    "    start_ind = 0\n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    for (ds_name, (x_train, y_train, c_train), (x_test, y_test, c_test)) in datasets[start_ind:]:\n",
    "        print(\"Training with concepts\", num_concepts, \"in dataset\", ds_name)\n",
    "        \n",
    "        channels_axis = (\n",
    "            -1 if experiment_config.get(\"data_format\", \"channels_last\") == \"channels_last\"\n",
    "            else 1\n",
    "        )\n",
    "        \n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} for {num_concepts} concepts\")\n",
    "            concept_encoder = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/concept_encoder_{ds_name}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            concept_decoder = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/concept_decoder_{ds_name}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            coefficient_model = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/coefficient_model_{ds_name}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            senn_model = construct_senn_model(\n",
    "                concept_encoder=concept_encoder,\n",
    "                concept_decoder=concept_decoder,\n",
    "                coefficient_model=coefficient_model,\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "                regularization_strength=experiment_config.get(\"regularization_strength\", 0.1),\n",
    "                learning_rate=experiment_config.get(\"learning_rate\", 1e-3),\n",
    "                sparsity_strength=experiment_config.get(\"sparsity_strength\", 2e-5),\n",
    "            )\n",
    "            \n",
    "            x_train_preds, (_, x_train_theta_class_scores) = senn_model(x_train)\n",
    "            c_train_pred = get_argmax_concept_explanations(\n",
    "                x_train_preds.numpy(),\n",
    "                x_train_theta_class_scores.numpy(),\n",
    "            )\n",
    "            \n",
    "            x_test_preds, (_, x_test_theta_class_scores) = senn_model(x_test)\n",
    "            c_test_pred = get_argmax_concept_explanations(\n",
    "                x_test_preds.numpy(),\n",
    "                x_test_theta_class_scores.numpy(),\n",
    "            )\n",
    "            \n",
    "            print(\"\\t\\tComputing niching scores...\")\n",
    "            experiment_variables['niss'].append(\n",
    "                niching.niche_impurity_score(\n",
    "                    c_soft=c_test_pred,\n",
    "                    c_true=c_test,\n",
    "                    c_soft_train=c_train_pred,\n",
    "                    c_true_train=c_train,\n",
    "                )\n",
    "            )\n",
    "            print(f'\\t\\tNIS: {experiment_variables[\"niss\"][-1]:.2f}')\n",
    "            \n",
    "            os.makedirs(res_dir, exist_ok=True)\n",
    "            dump(experiment_variables, os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            \n",
    "    return experiment_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(SENN)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "senn_experiment_config = dict(\n",
    "    max_epochs=50,\n",
    "    pretrain_autoencoder_epochs=50,\n",
    "    predictor_max_epochs=100,\n",
    "    batch_size=32,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    \n",
    "    num_concepts=(balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1]),\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_train[0].shape[1:],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    \n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    encoder_units=[64, 64],\n",
    "    include_pool=True,\n",
    "    decoder_units=[256, 512],\n",
    "    coefficient_model_units=[64, 64],\n",
    "    latent_decoder_units=[64, 64],\n",
    "    drop_prob=0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    \n",
    "    \n",
    "    regularization_strength=0.1,\n",
    "    sparsity_strength=2e-5,\n",
    "    \n",
    "    concept_cardinality=[2 for _ in range(balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1])],\n",
    "    results_dir=os.path.join(RESULTS_DIR, \"senn/dependency_multiclass\"),\n",
    "    niching_results_dir=os.path.join(NICHING_RESULTS_DIR, \"senn/dependency_multiclass\"),\n",
    "    verbosity=0,\n",
    "\n",
    "    holdout_fraction=0.1,\n",
    "    patience=float(\"inf\"),\n",
    "    early_stop_metric=\"val_loss\",\n",
    "    early_stop_mode=\"min\",\n",
    "    min_delta=1e-5,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(senn_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "senn_figure_dir = os.path.join(senn_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(senn_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "senn_results = senn_experiment_loop(\n",
    "    senn_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0\", train_0, test_0),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1\", train_1, test_1),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2\", train_2, test_2),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3\", train_3, test_3),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4\", train_4, test_4),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_5\", train_5, test_5),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(SENN)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "senn_extended_experiment_config = dict(\n",
    "    max_epochs=50,\n",
    "    pretrain_autoencoder_epochs=50,\n",
    "    predictor_max_epochs=100,\n",
    "    batch_size=32,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    \n",
    "    num_concepts=(2*balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1]),\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_train[0].shape[1:],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    \n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "#     encoder_units=[128, 64, 64, 64, 32],\n",
    "    encoder_units=[64, 64],\n",
    "    include_pool=True,\n",
    "    decoder_units=[256, 512],\n",
    "    coefficient_model_units=[64, 64],\n",
    "    latent_decoder_units=[64, 64],\n",
    "    drop_prob=0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    \n",
    "    \n",
    "    regularization_strength=0.1,\n",
    "    sparsity_strength=2e-5,\n",
    "    \n",
    "    concept_cardinality=[2 for _ in range(balanced_multiclass_task_bin_concepts_dep_0_train[2].shape[-1])],\n",
    "    results_dir=os.path.join(RESULTS_DIR, \"senn/dependency_multiclass_extended\"),\n",
    "    niching_results_dir=os.path.join(NICHING_RESULTS_DIR, \"senn/dependency_multiclass_extended\"),\n",
    "    verbosity=0,\n",
    "\n",
    "    holdout_fraction=0.1,\n",
    "    patience=float(\"inf\"),\n",
    "    early_stop_metric=\"val_loss\",\n",
    "    early_stop_mode=\"min\",\n",
    "    min_delta=1e-5,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(senn_extended_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "senn_extended_figure_dir = os.path.join(senn_extended_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(senn_extended_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "senn_extended_results = senn_experiment_loop(\n",
    "    senn_extended_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0\", train_0, test_0),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1\", train_1, test_1),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2\", train_2, test_2),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3\", train_3, test_3),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4\", train_4, test_4),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_5\", train_5, test_5),\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "purity_dsprites.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
