{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dIcmOacShy-p",
    "outputId": "e47e6328-6477-4d52-d914-4dbf25f7fb6b"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xD3cPbsJhy-r"
   },
   "source": [
    "# Purity dSprites Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwclGfwnhy-s"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYPJQZB-hy-t"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib\n",
    "import concepts_xai\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import rc\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "from tensorflow.keras.models import load_model\n",
    "from joblib import dump, load\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import utils\n",
    "import model_utils\n",
    "\n",
    "\n",
    "import concepts_xai.evaluation.metrics.niching as niching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktrTSqfchy-u"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "## Set seeds up for reproducibility\n",
    "################################################################################\n",
    "utils.reseed(87)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "## Global Variables Defining Experiment Flow\n",
    "################################################################################\n",
    "\n",
    "FROM_CACHE = True\n",
    "_LATEX_SYMBOL = \"$\"\n",
    "BASE_DIR = '.'\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"results/dsprites\")\n",
    "DATASETS_DIR = os.path.join(BASE_DIR, \"results/dsprites\", \"datasets/\")\n",
    "NICHING_RESULTS_DIR = os.path.join(BASE_DIR, \"results_concept_niching_integrated/dsprites\")\n",
    "Path(DATASETS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(NICHING_RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "rc('text', usetex=(_LATEX_SYMBOL == \"$\"))\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold_text(x):\n",
    "    if _LATEX_SYMBOL == \"$\":\n",
    "        return r\"$\\textbf{\" + x + \"}$\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRdZhyY9hy-v"
   },
   "source": [
    "# Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.datasets.dSprites as dsprites\n",
    "import concepts_xai.datasets.latentFactorData as latentFactorData\n",
    "\n",
    "def generate_dsprites_dataset(\n",
    "    label_fn,\n",
    "    filter_fn=None,\n",
    "    dataset_path=None,\n",
    "    concept_map_fn=lambda x: x,\n",
    "    sample_map_fn=lambda x: x,\n",
    "    dsprites_path=\"dsprites/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz\",\n",
    "    force_reload=False,\n",
    "):\n",
    "    if (not force_reload) and dataset_path and os.path.exists(dataset_path):\n",
    "        # Them time to load up this dataset!\n",
    "        ds = np.load(dataset_path)\n",
    "        return (\n",
    "            (ds[\"x_train\"], ds[\"y_train\"], ds[\"c_train\"]),\n",
    "            (ds[\"x_test\"], ds[\"y_test\"], ds[\"c_test\"])\n",
    "        )\n",
    "    \n",
    "    def _task_fn(x_data, c_data):\n",
    "        return latentFactorData.get_task_data(\n",
    "            x_data=x_data,\n",
    "            c_data=c_data,\n",
    "            label_fn=label_fn,\n",
    "            filter_fn=filter_fn,\n",
    "        )\n",
    "\n",
    "    loaded_dataset = dsprites.dSprites(\n",
    "        dataset_path=dsprites_path,\n",
    "        train_size=0.8,\n",
    "        random_state=42,\n",
    "        task=_task_fn,\n",
    "    )\n",
    "    _, _, _ = loaded_dataset.load_data()\n",
    "\n",
    "    x_train = sample_map_fn(loaded_dataset.x_train)\n",
    "    y_train = loaded_dataset.y_train\n",
    "    c_train = concept_map_fn(loaded_dataset.c_train)\n",
    "    \n",
    "    x_test = sample_map_fn(loaded_dataset.x_test)\n",
    "    y_test = loaded_dataset.y_test\n",
    "    c_test = concept_map_fn(loaded_dataset.c_test)\n",
    "    \n",
    "    if dataset_path:\n",
    "        # Then serialize it to speed up things next time\n",
    "        np.savez(\n",
    "            dataset_path,\n",
    "            x_train=x_train,\n",
    "            y_train=y_train,\n",
    "            c_train=c_train,\n",
    "            x_test=x_test,\n",
    "            y_test=y_test,\n",
    "            c_test=c_test,\n",
    "        )\n",
    "    return (x_train, y_train, c_train), (x_test, y_test, c_test),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_class_balance(y):\n",
    "    one_hot = tf.keras.utils.to_categorical(y)\n",
    "    return np.sum(one_hot, axis=0) / one_hot.shape[0]\n",
    "\n",
    "def multiclass_binary_concepts_map_fn(concepts):\n",
    "    new_concepts = np.zeros((concepts.shape[0], 5))\n",
    "    # We will have 5 concepts:\n",
    "    # (0) \"is it ellipse or square?\"\n",
    "    new_concepts[:, 0] = (concepts[:, 0] < 2).astype(np.int)\n",
    "\n",
    "    # (1) \"is_size < 3?\"\n",
    "    num_sizes = len(set(concepts[:, 1]))\n",
    "    new_concepts[:, 1] = (concepts[:, 1] < num_sizes/2).astype(np.int)\n",
    "\n",
    "    # (2) \"is rotation < PI/2?\"\n",
    "    num_rots = len(set(concepts[:, 2]))\n",
    "    new_concepts[:, 2] = (concepts[:, 2] < num_rots/2).astype(np.int)\n",
    "\n",
    "    # (3) \"is x <= 16?\"\n",
    "    num_x_coords = len(set(concepts[:, 3]))\n",
    "    new_concepts[:, 3] = (concepts[:, 3] < num_x_coords // 2).astype(np.int)\n",
    "\n",
    "    # (4) \"is y <= 16?\"\n",
    "    num_y_coords = len(set(concepts[:, 4]))\n",
    "    new_concepts[:, 4] = (concepts[:, 4] < num_y_coords // 2).astype(np.int)\n",
    "    \n",
    "    return new_concepts\n",
    "\n",
    "def _get_concept_vector(c_data):\n",
    "    return np.array([\n",
    "        # First check if it is an ellipse or a square\n",
    "        int(c_data[0] < 2),\n",
    "        # Now check that it is \"small\"\n",
    "        int(c_data[1] < 3),\n",
    "        # And it has not been rotated more than PI/2 radians\n",
    "        int(c_data[2] < 20),\n",
    "        # Finally, check whether it is in not in the the upper-left quadrant\n",
    "        int(c_data[3] < 15),\n",
    "        int(c_data[4] < 15),\n",
    "    ])\n",
    "\n",
    "def multiclass_task_label_fn(c_data):\n",
    "    # Our task will be a binary task where we are interested in determining\n",
    "    # whether an image is a \"small\" ellipse not in the upper-left\n",
    "    # quadrant that has been rotated less than 3*PI/2 radians\n",
    "    concept_vector = _get_concept_vector(c_data)\n",
    "    binary_label_encoding = [\n",
    "        concept_vector[0] or concept_vector[1],\n",
    "        concept_vector[2] or concept_vector[3],\n",
    "        concept_vector[4],\n",
    "    ]\n",
    "    return int(\n",
    "        \"\".join(list(map(str, binary_label_encoding))),\n",
    "        2\n",
    "    )\n",
    "\n",
    "def dep_0_filter_fn(concept):\n",
    "    ranges = [\n",
    "        list(range(3)),\n",
    "        list(range(0, 6, 2)),\n",
    "        list(range(0, 40, 4)),\n",
    "        list(range(0, 32, 2)),\n",
    "        list(range(0, 32, 2)),\n",
    "    ]\n",
    "    return all([\n",
    "        (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "    ])\n",
    "\n",
    "\n",
    "scale_shape_sets_lower = [\n",
    "    list(np.random.permutation(4))[:3] for i in range(3)\n",
    "]\n",
    "\n",
    "scale_shape_sets_upper = [\n",
    "    list(2 + np.random.permutation(4))[:3] for i in range(3)\n",
    "]\n",
    "def dep_1_filter_fn(concept):\n",
    "    ranges = [\n",
    "        list(range(3)),\n",
    "        list(range(6)),\n",
    "        list(range(0, 40, 4)),\n",
    "        list(range(0, 32, 2)),\n",
    "        list(range(0, 32, 2)),\n",
    "    ]\n",
    "    \n",
    "    concept_vector = _get_concept_vector(concept)\n",
    "\n",
    "    # First filter as in small dataset to constraint the size of the data a bit\n",
    "    if not all([\n",
    "        (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "    ]):\n",
    "        return False\n",
    "    if concept_vector[0]:\n",
    "        if concept[1] not in scale_shape_sets_lower[concept[0]]:\n",
    "            return False\n",
    "    else:\n",
    "        if concept[0] not in scale_shape_sets_upper[concept[0]]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "rotation_scale_sets_lower = [\n",
    "    list(np.random.permutation(30))[:20] for i in range(6)\n",
    "]\n",
    "\n",
    "rotation_scale_sets_upper = [\n",
    "    list(10 + np.random.permutation(30))[:20] for i in range(6)\n",
    "]\n",
    "def dep_2_filter_fn(concept):\n",
    "    ranges = [\n",
    "        list(range(3)),\n",
    "        list(range(6)),\n",
    "        list(range(0, 40, 2)),\n",
    "        list(range(0, 32, 2)),\n",
    "        list(range(0, 32, 2)),\n",
    "    ]\n",
    "    \n",
    "    concept_vector = _get_concept_vector(concept)\n",
    "    # First filter as in small dataset to constraint the size of the data a bit\n",
    "    if not all([\n",
    "        (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "    ]):\n",
    "        return False\n",
    "    if concept_vector[0]:\n",
    "        if concept[1] not in scale_shape_sets_lower[concept[0]]:\n",
    "            return False\n",
    "    else:\n",
    "        if concept[0] not in scale_shape_sets_upper[concept[0]]:\n",
    "            return False\n",
    "    \n",
    "    if concept_vector[1]:\n",
    "        if concept[2] not in rotation_scale_sets_lower[concept[1]]:\n",
    "            return False\n",
    "    else:\n",
    "        if concept[2] not in rotation_scale_sets_upper[concept[1]]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "x_pos_rotation_sets_lower = [\n",
    "    list(np.random.permutation(20))[:16]\n",
    "    for i in range(40)\n",
    "]\n",
    "\n",
    "x_pos_rotation_sets_upper = [\n",
    "    list(12 + np.random.permutation(20))[:16]\n",
    "    for i in range(40)\n",
    "]\n",
    "def dep_3_filter_fn(concept):\n",
    "    ranges = [\n",
    "        list(range(3)),\n",
    "        list(range(6)),\n",
    "        list(range(0, 40, 2)),\n",
    "        list(range(0, 32)),\n",
    "        list(range(0, 32, 2)),\n",
    "    ]\n",
    "    \n",
    "    concept_vector = _get_concept_vector(concept)\n",
    "\n",
    "    # First filter as in small dataset to constraint the size of the data a bit\n",
    "    if not all([\n",
    "        (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "    ]):\n",
    "        return False\n",
    "    if concept_vector[0]:\n",
    "        if concept[1] not in scale_shape_sets_lower[concept[0]]:\n",
    "            return False\n",
    "    else:\n",
    "        if concept[0] not in scale_shape_sets_upper[concept[0]]:\n",
    "            return False\n",
    "    \n",
    "    if concept_vector[1]:\n",
    "        if concept[2] not in rotation_scale_sets_lower[concept[1]]:\n",
    "            return False\n",
    "    else:\n",
    "        if concept[2] not in rotation_scale_sets_upper[concept[1]]:\n",
    "            return False\n",
    "        \n",
    "    if concept_vector[2]:\n",
    "        if concept[3] not in x_pos_rotation_sets_lower[concept[2]]:\n",
    "            return False\n",
    "    else:\n",
    "        if concept[3] not in x_pos_rotation_sets_upper[concept[2]]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "y_pos_x_pos_sets_lower = [\n",
    "    list(np.random.permutation(20))[:16]\n",
    "    for i in range(32)\n",
    "]\n",
    "\n",
    "y_pos_x_pos_sets_upper = [\n",
    "    list(12 + np.random.permutation(20))[:16]\n",
    "    for i in range(32)\n",
    "]\n",
    "def dep_4_filter_fn(concept):\n",
    "    ranges = [\n",
    "        list(range(3)),\n",
    "        list(range(6)),\n",
    "        list(range(0, 40, 2)),\n",
    "        list(range(0, 32)),\n",
    "        list(range(0, 32)),\n",
    "    ]\n",
    "    \n",
    "    concept_vector = _get_concept_vector(concept)\n",
    "\n",
    "    # First filter as in small dataset to constraint the size of the data a bit\n",
    "    if not all([\n",
    "        (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "    ]):\n",
    "        return False\n",
    "    if concept_vector[0]:\n",
    "        if concept[1] not in scale_shape_sets_lower[concept[0]]:\n",
    "            return False\n",
    "    else:\n",
    "        if concept[0] not in scale_shape_sets_upper[concept[0]]:\n",
    "            return False\n",
    "    \n",
    "    if concept_vector[1]:\n",
    "        if concept[2] not in rotation_scale_sets_lower[concept[1]]:\n",
    "            return False\n",
    "    else:\n",
    "        if concept[2] not in rotation_scale_sets_upper[concept[1]]:\n",
    "            return False\n",
    "        \n",
    "    if concept_vector[2]:\n",
    "        if concept[3] not in x_pos_rotation_sets_lower[concept[2]]:\n",
    "            return False\n",
    "    else:\n",
    "        if concept[3] not in x_pos_rotation_sets_upper[concept[2]]:\n",
    "            return False\n",
    "    \n",
    "    if concept_vector[3]:\n",
    "        if concept[4] not in y_pos_x_pos_sets_lower[concept[3]]:\n",
    "            return False\n",
    "    else:\n",
    "        if concept[4] not in y_pos_x_pos_sets_upper[concept[3]]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def balanced_multiclass_task_label_fn(c_data):\n",
    "    # Our task will be a binary task where we are interested in determining\n",
    "    # whether an image is a \"small\" ellipse not in the upper-left\n",
    "    # quadrant that has been rotated less than 3*PI/2 radians\n",
    "    concept_vector = _get_concept_vector(c_data)\n",
    "    threshold = 0\n",
    "    if concept_vector[0] == 1:\n",
    "        binary_label_encoding = [\n",
    "            concept_vector[1],\n",
    "            concept_vector[2],\n",
    "        ]\n",
    "    else:\n",
    "        threshold = 4\n",
    "        binary_label_encoding = [\n",
    "            concept_vector[3],\n",
    "            concept_vector[4],\n",
    "        ]\n",
    "    return threshold + int(\n",
    "        \"\".join(list(map(str, binary_label_encoding))),\n",
    "        2\n",
    "    )\n",
    "\n",
    "balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test = generate_dsprites_dataset(\n",
    "    label_fn=balanced_multiclass_task_label_fn,\n",
    "    filter_fn=dep_0_filter_fn,\n",
    "    dataset_path=os.path.join(DATASETS_DIR, \"balanced_multiclass_task_bin_concepts_dep_0_complete_dataset.npz\"),\n",
    "    dsprites_path=os.path.join(BASE_DIR, \"dsprites/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz\"),\n",
    "    concept_map_fn=multiclass_binary_concepts_map_fn,\n",
    "    force_reload=True,\n",
    ")\n",
    "\n",
    "dep_0_corr_mat = np.ones(\n",
    "    (\n",
    "        balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1]))\n",
    "    )\n",
    ")\n",
    "for c in range(dep_0_corr_mat.shape[0]):\n",
    "    for l in range(dep_0_corr_mat.shape[1]):\n",
    "        dep_0_corr_mat[c][l] = np.corrcoef(\n",
    "            balanced_multiclass_task_bin_concepts_dep_0_complete_train[2][:, c],\n",
    "            (balanced_multiclass_task_bin_concepts_dep_0_complete_train[1] == l).astype(np.int32),\n",
    "        )[0, 1]\n",
    "fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "im, cbar = utils.heatmap(\n",
    "    np.abs(dep_0_corr_mat),\n",
    "    [f\"$c_{i}$\" for i in range(dep_0_corr_mat.shape[0])],\n",
    "    [f\"$l_{i}$\" for i in range(dep_0_corr_mat.shape[1])],\n",
    "    ax=ax,\n",
    "    cmap=\"magma\",\n",
    "    cbarlabel=f\"Correlation Coef\",\n",
    "    vmin=0, #-1,\n",
    "    vmax=1,\n",
    ")\n",
    "texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.suptitle(f\"dSprites Concept-Label Absolute Correlations ($\\lambda = 0$)\", fontsize=25)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "plt.show()\n",
    "\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_0_complete_dataset train size:\", balanced_multiclass_task_bin_concepts_dep_0_complete_train[0].shape[0])\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_0_complete_dataset train concept size:\", balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape)\n",
    "print(\"\\tTrain balance:\", count_class_balance(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1]))\n",
    "print(\"\\tConcept balance:\", np.sum(balanced_multiclass_task_bin_concepts_dep_0_complete_train[2], axis=0)/balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[0])\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test = generate_dsprites_dataset(\n",
    "    label_fn=balanced_multiclass_task_label_fn,\n",
    "    filter_fn=dep_1_filter_fn,\n",
    "    dataset_path=os.path.join(DATASETS_DIR, \"balanced_multiclass_task_bin_concepts_dep_1_complete_dataset.npz\"),\n",
    "    dsprites_path=os.path.join(BASE_DIR, \"dsprites/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz\"),\n",
    "    concept_map_fn=multiclass_binary_concepts_map_fn,\n",
    "    force_reload=True,\n",
    ")\n",
    "\n",
    "dep_1_corr_mat = np.ones(\n",
    "    (\n",
    "        balanced_multiclass_task_bin_concepts_dep_1_complete_train[2].shape[-1],\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_1_complete_train[1]))\n",
    "    )\n",
    ")\n",
    "for c in range(dep_1_corr_mat.shape[0]):\n",
    "    for l in range(dep_1_corr_mat.shape[1]):\n",
    "        dep_1_corr_mat[c][l] = np.corrcoef(\n",
    "            balanced_multiclass_task_bin_concepts_dep_1_complete_train[2][:, c],\n",
    "            (balanced_multiclass_task_bin_concepts_dep_1_complete_train[1] == l).astype(np.int32),\n",
    "        )[0, 1]\n",
    "fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "im, cbar = utils.heatmap(\n",
    "    np.abs(dep_1_corr_mat),\n",
    "    [f\"$c_{i}$\" for i in range(dep_1_corr_mat.shape[0])],\n",
    "    [f\"$l_{i}$\" for i in range(dep_1_corr_mat.shape[1])],\n",
    "    ax=ax,\n",
    "    cmap=\"magma\",\n",
    "    cbarlabel=f\"Correlation Coef\",\n",
    "    vmin=0, #-1,\n",
    "    vmax=1,\n",
    ")\n",
    "texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.suptitle(f\"dSprites Concept-Label Absolute Correlations ($\\lambda = 1$)\", fontsize=25)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_1_complete_dataset train size:\", balanced_multiclass_task_bin_concepts_dep_1_complete_train[0].shape[0])\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_1_complete_dataset train concept size:\", balanced_multiclass_task_bin_concepts_dep_1_complete_train[2].shape)\n",
    "print(\"\\tTrain balance:\", count_class_balance(balanced_multiclass_task_bin_concepts_dep_1_complete_train[1]))\n",
    "print(\"\\tConcept balance:\", np.sum(balanced_multiclass_task_bin_concepts_dep_1_complete_train[2], axis=0)/balanced_multiclass_task_bin_concepts_dep_1_complete_train[2].shape[0])\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test = generate_dsprites_dataset(\n",
    "    label_fn=balanced_multiclass_task_label_fn,\n",
    "    filter_fn=dep_2_filter_fn,\n",
    "    dataset_path=os.path.join(DATASETS_DIR, \"balanced_multiclass_task_bin_concepts_dep_2_complete_dataset.npz\"),\n",
    "    dsprites_path=os.path.join(BASE_DIR, \"dsprites/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz\"),\n",
    "    concept_map_fn=multiclass_binary_concepts_map_fn,\n",
    "    force_reload=True,\n",
    ")\n",
    "\n",
    "dep_2_corr_mat = np.ones(\n",
    "    (\n",
    "        balanced_multiclass_task_bin_concepts_dep_2_complete_train[2].shape[-1],\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_2_complete_train[1]))\n",
    "    )\n",
    ")\n",
    "for c in range(dep_2_corr_mat.shape[0]):\n",
    "    for l in range(dep_2_corr_mat.shape[1]):\n",
    "        dep_2_corr_mat[c][l] = np.corrcoef(\n",
    "            balanced_multiclass_task_bin_concepts_dep_2_complete_train[2][:, c],\n",
    "            (balanced_multiclass_task_bin_concepts_dep_2_complete_train[1] == l).astype(np.int32),\n",
    "        )[0, 1]\n",
    "fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "im, cbar = utils.heatmap(\n",
    "    np.abs(dep_2_corr_mat),\n",
    "    [f\"$c_{i}$\" for i in range(dep_2_corr_mat.shape[0])],\n",
    "    [f\"$l_{i}$\" for i in range(dep_2_corr_mat.shape[1])],\n",
    "    ax=ax,\n",
    "    cmap=\"magma\",\n",
    "    cbarlabel=f\"Correlation Coef\",\n",
    "    vmin=0, #-1,\n",
    "    vmax=1,\n",
    ")\n",
    "texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.suptitle(f\"dSprites Concept-Label Absolute Correlations ($\\lambda = 2$)\", fontsize=25)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_2_complete_dataset train size:\", balanced_multiclass_task_bin_concepts_dep_2_complete_train[0].shape[0])\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_2_complete_dataset train concept size:\", balanced_multiclass_task_bin_concepts_dep_2_complete_train[2].shape)\n",
    "print(\"\\tTrain balance:\", count_class_balance(balanced_multiclass_task_bin_concepts_dep_2_complete_train[1]))\n",
    "print(\"\\tConcept balance:\", np.sum(balanced_multiclass_task_bin_concepts_dep_2_complete_train[2], axis=0)/balanced_multiclass_task_bin_concepts_dep_2_complete_train[2].shape[0])\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test = generate_dsprites_dataset(\n",
    "    label_fn=balanced_multiclass_task_label_fn,\n",
    "    filter_fn=dep_3_filter_fn,\n",
    "    dataset_path=os.path.join(DATASETS_DIR, \"balanced_multiclass_task_bin_concepts_dep_3_complete_dataset.npz\"),\n",
    "    dsprites_path=os.path.join(BASE_DIR, \"dsprites/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz\"),\n",
    "    concept_map_fn=multiclass_binary_concepts_map_fn,\n",
    "    force_reload=True,\n",
    ")\n",
    "\n",
    "dep_3_corr_mat = np.ones(\n",
    "    (\n",
    "        balanced_multiclass_task_bin_concepts_dep_3_complete_train[2].shape[-1],\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_3_complete_train[1]))\n",
    "    )\n",
    ")\n",
    "for c in range(dep_3_corr_mat.shape[0]):\n",
    "    for l in range(dep_3_corr_mat.shape[1]):\n",
    "        dep_3_corr_mat[c][l] = np.corrcoef(\n",
    "            balanced_multiclass_task_bin_concepts_dep_3_complete_train[2][:, c],\n",
    "            (balanced_multiclass_task_bin_concepts_dep_3_complete_train[1] == l).astype(np.int32),\n",
    "        )[0, 1]\n",
    "fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "im, cbar = utils.heatmap(\n",
    "    np.abs(dep_3_corr_mat),\n",
    "    [f\"$c_{i}$\" for i in range(dep_3_corr_mat.shape[0])],\n",
    "    [f\"$l_{i}$\" for i in range(dep_3_corr_mat.shape[1])],\n",
    "    ax=ax,\n",
    "    cmap=\"magma\",\n",
    "    cbarlabel=f\"Correlation Coef\",\n",
    "    vmin=0, #-1,\n",
    "    vmax=1,\n",
    ")\n",
    "texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.suptitle(f\"dSprites Concept-Label Absolute Correlations ($\\lambda = 3$)\", fontsize=25)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "plt.show()\n",
    "\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_3_complete_dataset train size:\", balanced_multiclass_task_bin_concepts_dep_3_complete_train[0].shape[0])\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_3_complete_dataset train concept size:\", balanced_multiclass_task_bin_concepts_dep_3_complete_train[2].shape)\n",
    "print(\"\\tTrain balance:\", count_class_balance(balanced_multiclass_task_bin_concepts_dep_3_complete_train[1]))\n",
    "print(\"\\tConcept balance:\", np.sum(balanced_multiclass_task_bin_concepts_dep_3_complete_train[2], axis=0)/balanced_multiclass_task_bin_concepts_dep_3_complete_train[2].shape[0])\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test = generate_dsprites_dataset(\n",
    "    label_fn=balanced_multiclass_task_label_fn,\n",
    "    filter_fn=dep_4_filter_fn,\n",
    "    dataset_path=os.path.join(DATASETS_DIR, \"balanced_multiclass_task_bin_concepts_dep_4_complete_dataset.npz\"),\n",
    "    dsprites_path=os.path.join(BASE_DIR, \"dsprites/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz\"),\n",
    "    concept_map_fn=multiclass_binary_concepts_map_fn,\n",
    "    force_reload=True,\n",
    ")\n",
    "\n",
    "dep_4_corr_mat = np.ones(\n",
    "    (\n",
    "        balanced_multiclass_task_bin_concepts_dep_4_complete_train[2].shape[-1],\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_4_complete_train[1]))\n",
    "    )\n",
    ")\n",
    "for c in range(dep_4_corr_mat.shape[0]):\n",
    "    for l in range(dep_4_corr_mat.shape[1]):\n",
    "        dep_4_corr_mat[c][l] = np.corrcoef(\n",
    "            balanced_multiclass_task_bin_concepts_dep_4_complete_train[2][:, c],\n",
    "            (balanced_multiclass_task_bin_concepts_dep_4_complete_train[1] == l).astype(np.int32),\n",
    "        )[0, 1]\n",
    "fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "im, cbar = utils.heatmap(\n",
    "    np.abs(dep_4_corr_mat),\n",
    "    [f\"$c_{i}$\" for i in range(dep_4_corr_mat.shape[0])],\n",
    "    [f\"$l_{i}$\" for i in range(dep_4_corr_mat.shape[1])],\n",
    "    ax=ax,\n",
    "    cmap=\"magma\",\n",
    "    cbarlabel=f\"Correlation Coef\",\n",
    "    vmin=0, #-1,\n",
    "    vmax=1,\n",
    ")\n",
    "texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.suptitle(f\"dSprites Concept-Label Absolute Correlations ($\\lambda = 4$)\", fontsize=25)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "plt.show()\n",
    "\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_4_complete_dataset train size:\", balanced_multiclass_task_bin_concepts_dep_4_complete_train[0].shape[0])\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_4_complete_dataset train concept size:\", balanced_multiclass_task_bin_concepts_dep_4_complete_train[2].shape)\n",
    "print(\"\\tTrain balance:\", count_class_balance(balanced_multiclass_task_bin_concepts_dep_4_complete_train[1]))\n",
    "print(\"\\tConcept balance:\", np.sum(balanced_multiclass_task_bin_concepts_dep_4_complete_train[2], axis=0)/balanced_multiclass_task_bin_concepts_dep_4_complete_train[2].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_niching_dataset(x, y, c):\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    y = enc.fit_transform(y.reshape(-1, 1))\n",
    "    n_samples_per_class = y.sum(axis=0, dtype=int)\n",
    "    n_samples_to_draw = int(n_samples_per_class.min())\n",
    "    samples_per_class = []\n",
    "    for i in range(y.shape[1]):\n",
    "        samples_per_class.append(np.argwhere(y[:, i]==1)[np.random.choice(n_samples_per_class[i], n_samples_to_draw)].squeeze())\n",
    "    samples_per_class = np.concatenate(samples_per_class)\n",
    "    return x[samples_per_class], y[samples_per_class], c[samples_per_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_0 = balanced_niching_dataset(*balanced_multiclass_task_bin_concepts_dep_0_complete_train)\n",
    "train_1 = balanced_niching_dataset(*balanced_multiclass_task_bin_concepts_dep_1_complete_train)\n",
    "train_2 = balanced_niching_dataset(*balanced_multiclass_task_bin_concepts_dep_2_complete_train)\n",
    "train_3 = balanced_niching_dataset(*balanced_multiclass_task_bin_concepts_dep_3_complete_train)\n",
    "train_4 = balanced_niching_dataset(*balanced_multiclass_task_bin_concepts_dep_4_complete_train)\n",
    "\n",
    "test_0 = balanced_niching_dataset(*balanced_multiclass_task_bin_concepts_dep_0_complete_test)\n",
    "test_1 = balanced_niching_dataset(*balanced_multiclass_task_bin_concepts_dep_1_complete_test)\n",
    "test_2 = balanced_niching_dataset(*balanced_multiclass_task_bin_concepts_dep_2_complete_test)\n",
    "test_3 = balanced_niching_dataset(*balanced_multiclass_task_bin_concepts_dep_3_complete_test)\n",
    "test_4 = balanced_niching_dataset(*balanced_multiclass_task_bin_concepts_dep_4_complete_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wciMdtP9hy-1"
   },
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ey3yb8HIhy-2",
    "outputId": "08bc7f0b-146f-48e5-8f9f-40b608e09cb3"
   },
   "outputs": [],
   "source": [
    "# Construct the encoder model\n",
    "def _extract_concepts(activations, concept_cardinality):\n",
    "    concepts = []\n",
    "    total_seen = 0\n",
    "    if all(np.array(concept_cardinality) <= 1):\n",
    "        # Then nothing to do here as they are all binary concepts\n",
    "        return activations\n",
    "    for num_values in concept_cardinality:\n",
    "        concepts.append(activations[:, total_seen: total_seen + num_values])\n",
    "        total_seen += num_values\n",
    "    return concepts\n",
    "    \n",
    "def construct_encoder(\n",
    "    input_shape,\n",
    "    filter_groups,\n",
    "    units,\n",
    "    concept_cardinality,\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    latent_dims=0,\n",
    "    output_logits=False,\n",
    "):\n",
    "    encoder_inputs = tf.keras.Input(shape=input_shape)\n",
    "    encoder_compute_graph = encoder_inputs\n",
    "    \n",
    "    # Start with our convolutions\n",
    "    num_convs = 0\n",
    "    for filter_group in filter_groups:\n",
    "        for (num_filters, kernel_size) in filter_group:\n",
    "            encoder_compute_graph = tf.keras.layers.Conv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=\"SAME\",\n",
    "                activation=None,\n",
    "                name=f'encoder_conv_{num_convs}',\n",
    "            )(encoder_compute_graph)\n",
    "            num_convs += 1\n",
    "            encoder_compute_graph = tf.keras.layers.BatchNormalization()(\n",
    "                encoder_compute_graph\n",
    "            )\n",
    "            encoder_compute_graph = tf.keras.activations.relu(encoder_compute_graph)\n",
    "        # Then do a max pool here to control the parameter count of the model\n",
    "        # at the end of each group\n",
    "        encoder_compute_graph = tf.keras.layers.MaxPooling2D(\n",
    "            pool_size=max_pool_window,\n",
    "            strides=max_pool_stride,\n",
    "        )(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Flatten this guy\n",
    "    encoder_compute_graph = tf.keras.layers.Flatten()(encoder_compute_graph)\n",
    "    \n",
    "    # Add a dropout if requested\n",
    "    if drop_prob:\n",
    "        encoder_compute_graph = tf.keras.layers.Dropout(drop_prob)(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Finally, include the fully connected bottleneck here\n",
    "    for i, units in enumerate(units):\n",
    "        encoder_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(encoder_compute_graph)\n",
    "    \n",
    "    if latent_dims:\n",
    "        bypass = tf.keras.layers.Dense(\n",
    "            latent_dims,\n",
    "            activation=\"sigmoid\",\n",
    "            name=\"encoder_bypass_channel\",\n",
    "        )(encoder_compute_graph)\n",
    "    else:\n",
    "        bypass = None\n",
    "    \n",
    "    # Map to our output distribution to a flattened\n",
    "    # vector where we will extract distributions over\n",
    "    # all concept values\n",
    "    encoder_compute_graph = tf.keras.layers.Dense(\n",
    "        sum(concept_cardinality),\n",
    "        activation=None,\n",
    "        name=\"encoder_concept_outputs\",\n",
    "    )(encoder_compute_graph)\n",
    "        \n",
    "    # Separate this vector into all of its heads\n",
    "    concept_outputs = _extract_concepts(\n",
    "        encoder_compute_graph,\n",
    "        concept_cardinality,\n",
    "    )\n",
    "    if not output_logits:\n",
    "        if isinstance(concept_outputs, list):\n",
    "            for i, concept_vec in enumerate(concept_outputs):\n",
    "                if concept_vec.shape[-1] == 1:\n",
    "                    # Then this is a binary concept so simply apply sigmoid\n",
    "                    concept_outputs[i] = tf.keras.activations.sigmoid(concept_vec)\n",
    "                else:\n",
    "                    # Else we will apply a softmax layer as we assume that all of these\n",
    "                    # entries represent a multi-modal probability distribution\n",
    "                    concept_outputs[i] = tf.keras.activations.softmax(\n",
    "                        concept_vec,\n",
    "                        axis=-1,\n",
    "                    )\n",
    "        else:\n",
    "            # Else they are allbinary concepts so let's sigmoid them\n",
    "            concept_outputs = tf.keras.activations.sigmoid(concept_outputs)\n",
    "    return tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        [concept_outputs, bypass] if bypass is not None else concept_outputs,\n",
    "        name=\"encoder\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7lu1IS3hy-4"
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## Build concepts-to-labels model\n",
    "############################################################################\n",
    "\n",
    "def construct_decoder(units, num_outputs):\n",
    "    decoder_layers = [\n",
    "        tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation=tf.nn.relu,\n",
    "            name=f\"decoder_dense_{i+1}\",\n",
    "        ) for i, units in enumerate(units)\n",
    "    ]\n",
    "    return tf.keras.Sequential(decoder_layers + [\n",
    "        tf.keras.layers.Dense(\n",
    "            num_outputs if num_outputs > 2 else 1,\n",
    "            activation=None,\n",
    "            name=\"decoder_model_output\",\n",
    "        )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjd9d2sDhy-4",
    "outputId": "f8a8faed-079a-475b-d5be-adad132a272d"
   },
   "outputs": [],
   "source": [
    "# Construct the complete model\n",
    "def construct_end_to_end_model(\n",
    "    input_shape,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    num_outputs,\n",
    "    learning_rate=1e-3,\n",
    "):\n",
    "    model_inputs = tf.keras.Input(shape=input_shape)\n",
    "    latent = encoder(model_inputs)\n",
    "    if isinstance(latent, list):\n",
    "        if len(latent) > 1:\n",
    "            compacted_vector = tf.keras.layers.Concatenate(axis=-1)(\n",
    "                latent\n",
    "            )\n",
    "        else:\n",
    "            compacted_vector = latent[0]\n",
    "    else:\n",
    "        compacted_vector = latent\n",
    "    model_compute_graph = decoder(compacted_vector)\n",
    "    # Now time to collapse all the concepts again back into a single vector\n",
    "    model = tf.keras.Model(\n",
    "        model_inputs,\n",
    "        model_compute_graph,\n",
    "        name=\"complete_model\",\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "        loss=(\n",
    "            tf.keras.losses.BinaryCrossentropy(from_logits=True) if (num_outputs <= 2)\n",
    "            else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        ),\n",
    "        metrics=[\n",
    "            \"binary_accuracy\" if (num_outputs <= 2)\n",
    "            else tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "        ],\n",
    "    )\n",
    "    return model, encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_UzDOFVhy-5"
   },
   "source": [
    "# CBM Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTjkp5B3hy-6",
    "outputId": "35f4b4fe-7295-4747-a9d4-c52f4c75dab3"
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## Build CBM\n",
    "############################################################################\n",
    "import concepts_xai.methods.CBM.CBModel as CBM\n",
    "\n",
    "def construct_cbm(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    num_outputs,\n",
    "    alpha=0.1,\n",
    "    learning_rate=1e-3,\n",
    "    latent_dims=0,\n",
    "    encoder_output_logits=False,\n",
    "):\n",
    "    model_factory = CBM.BypassJointCBM if latent_dims else CBM.JointConceptBottleneckModel\n",
    "    cbm_model = model_factory(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        task_loss=(\n",
    "            tf.keras.losses.BinaryCrossentropy(from_logits=True) if (num_outputs <= 2)\n",
    "            else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        ),\n",
    "        name=\"joint_cbm\",\n",
    "        metrics=[\n",
    "            tf.keras.metrics.BinaryAccuracy() if (num_outputs <= 2)\n",
    "            else tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "        ],\n",
    "        alpha=alpha,\n",
    "        pass_concept_logits=encoder_output_logits,\n",
    "    )\n",
    "\n",
    "    ############################################################################\n",
    "    ## Compile CBM Model\n",
    "    ############################################################################\n",
    "\n",
    "    cbm_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    )\n",
    "    return cbm_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.evaluation.metrics.purity as purity\n",
    "from collections import defaultdict\n",
    "\n",
    "############################################################################\n",
    "## Experiment loop\n",
    "############################################################################\n",
    "\n",
    "def construct_trivial_auc_mat(num_concepts):\n",
    "    result = np.ones((num_concepts, num_concepts), dtype=np.float32) * 0.5\n",
    "    return result + np.eye(num_concepts, dtype=np.float32) * 0.5\n",
    "    \n",
    "def cbm_experiment_loop(\n",
    "    experiment_config,\n",
    "    datasets,\n",
    "    load_from_cache=False,\n",
    "    oracle_matrix_cache=None,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    experiment_variables = dict(\n",
    "        config = experiment_config,\n",
    "        niss=[],\n",
    "    )\n",
    "    res_dir = experiment_config['niching_results_dir']\n",
    "    if load_from_cache:\n",
    "        if os.path.exists(os.path.join(res_dir, 'results_niching.joblib')):\n",
    "            experiment_variables = load(os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    count = 0\n",
    "    start_ind = 0\n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    for (ds_name, (x_train, y_train, c_train), (x_test, y_test, c_test)) in datasets[start_ind:]:\n",
    "        print(\"Training with dataset:\", ds_name)\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} of dataset {ds_name}\")\n",
    "            \n",
    "            # Proceed to do and end-to-end model in case we want to\n",
    "            # do some task-specific pretraining\n",
    "            end_to_end_model, encoder, decoder = construct_end_to_end_model(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "                learning_rate=experiment_config[\"learning_rate\"],\n",
    "                encoder=construct_encoder(\n",
    "                    input_shape=experiment_config[\"input_shape\"],\n",
    "                    filter_groups=experiment_config[\"encoder_filter_groups\"],\n",
    "                    units=experiment_config[\"encoder_units\"],\n",
    "                    concept_cardinality=experiment_config[\"concept_cardinality\"],\n",
    "                    drop_prob=experiment_config.get(\"drop_prob\", 0.5),\n",
    "                    max_pool_window=experiment_config.get(\"max_pool_window\", (2, 2)),\n",
    "                    max_pool_stride=experiment_config.get(\"max_pool_stride\", (2, 2)),\n",
    "                    latent_dims=experiment_config.get(\"latent_dims\", 0),\n",
    "                    output_logits=experiment_config.get(\"encoder_output_logits\", False),\n",
    "                ),\n",
    "                decoder=construct_decoder(\n",
    "                    units=experiment_config[\"decoder_units\"],\n",
    "                    num_outputs=experiment_config[\"num_outputs\"],\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            # Now time to actually construct and train the CBM\n",
    "            cbm_model = construct_cbm(\n",
    "                encoder=encoder,\n",
    "                decoder=decoder,\n",
    "                alpha=experiment_config[\"alpha\"],\n",
    "                learning_rate=experiment_config[\"learning_rate\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "                latent_dims=experiment_config.get(\"latent_dims\", 0),\n",
    "                encoder_output_logits=experiment_config.get(\"encoder_output_logits\", False),\n",
    "            )\n",
    "\n",
    "            print(\"\\t\\tCBM training completed\")\n",
    "            print(\"\\tSerializing model\")\n",
    "            cbm_model.encoder = load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/encoder_{ds_name}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            print(\"\\t\\tEncode...\")\n",
    "            concept_train_list = cbm_model.encoder(x_train)\n",
    "            concept_test_list = cbm_model.encoder(x_test)\n",
    "            \n",
    "            c_train_pred = []\n",
    "            for concept in concept_train_list:\n",
    "                c_train_pred.append(concept[:, 1])\n",
    "            c_train_pred = tf.stack(c_train_pred, axis=1).numpy()\n",
    "            \n",
    "            print(\"\\t\\tListed...\")\n",
    "            \n",
    "            c_test_pred = []\n",
    "            for concept in concept_test_list:\n",
    "                c_test_pred.append(concept[:, 1])\n",
    "            c_test_pred = tf.stack(c_test_pred, axis=1).numpy()\n",
    "            \n",
    "            experiment_variables['niss'].append(\n",
    "                niching.niche_impurity_score(\n",
    "                    c_soft=c_test_pred,\n",
    "                    c_true=c_test,\n",
    "                    c_soft_train=c_train_pred,\n",
    "                    c_true_train=c_train,\n",
    "                )\n",
    "            )\n",
    "            print(f'\\t\\tNIS: {experiment_variables[\"niss\"][-1]:.2f}')\n",
    "            \n",
    "            os.makedirs(res_dir, exist_ok=True)\n",
    "            dump(experiment_variables, os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            \n",
    "    return experiment_variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(niching)\n",
    "reload(CBM)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "cbm_base_experiment_config = dict(\n",
    "    batch_size=64,\n",
    "    max_epochs=100,\n",
    "    warmup_epochs=0,\n",
    "    pre_train_epochs=0,\n",
    "    trials=5,\n",
    "    alpha=10,\n",
    "    learning_rate=1e-3,\n",
    "    encoder_filter_groups=[\n",
    "        [(8, (7, 7))],\n",
    "        [(16, (5, 5))],\n",
    "        [(32, (3, 3))],\n",
    "        [(64, (3, 3))]\n",
    "    ],\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[64, 64],\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    pax_pool_stride=2,\n",
    "    num_outputs=(\n",
    "        len(set(multiclass_task_bin_concepts_dep_0_complete_train[1]))\n",
    "        if len(set(multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2 else 1\n",
    "    ),\n",
    "    concept_cardinality=[\n",
    "        2 for _ in range(multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1])\n",
    "    ],\n",
    "    num_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(RESULTS_DIR, \"cbm/graph_dependency_balanced_multiclass_tasks_purity\"),\n",
    "    niching_results_dir=os.path.join(NICHING_RESULTS_DIR, \"cbm/base\"),\n",
    "    input_shape=multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    latent_dims=0,\n",
    "    holdout_fraction=0.1,\n",
    "    verbosity=0,\n",
    "    early_stop_metric=\"val_concept_accuracy\",\n",
    "    early_stop_mode=\"max\",\n",
    "    delta_beta=0.05,\n",
    "    encoder_output_logits=False,\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(cbm_base_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "cbm_base_figure_dir = os.path.join(cbm_base_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(cbm_base_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "cbm_base_results = cbm_experiment_loop(\n",
    "    cbm_base_experiment_config,\n",
    "    load_from_cache=FROM_CACHE,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", train_0, test_0),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", train_1, test_1),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", train_2, test_2),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", train_3, test_3),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", train_4, test_4),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(niching)\n",
    "reload(CBM)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "cbm_from_logits_experiment_config = dict(\n",
    "    batch_size=64,\n",
    "    max_epochs=100,\n",
    "    warmup_epochs=0,\n",
    "    pre_train_epochs=0,\n",
    "    trials=5,\n",
    "    alpha=10,\n",
    "    learning_rate=1e-3,\n",
    "    encoder_filter_groups=[\n",
    "        [(8, (7, 7))],\n",
    "        [(16, (5, 5))],\n",
    "        [(32, (3, 3))],\n",
    "        [(64, (3, 3))]\n",
    "    ],\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[64, 64],\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    pax_pool_stride=2,\n",
    "    num_outputs=(\n",
    "        len(set(multiclass_task_bin_concepts_dep_0_complete_train[1]))\n",
    "        if len(set(multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2 else 1\n",
    "    ),\n",
    "    concept_cardinality=[\n",
    "        2 for _ in range(multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1])\n",
    "    ],\n",
    "    num_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(RESULTS_DIR, \"cbm/graph_dependency_balanced_multiclass_from_logits_tasks_purity\"),\n",
    "    niching_results_dir=os.path.join(NICHING_RESULTS_DIR, \"cbm/from_logits\"),\n",
    "    input_shape=multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    latent_dims=0,\n",
    "    holdout_fraction=0.1,\n",
    "    verbosity=0,\n",
    "    early_stop_metric=\"val_concept_accuracy\",\n",
    "    early_stop_mode=\"max\",\n",
    "    delta_beta=0.05,\n",
    "    encoder_output_logits=True,\n",
    ")\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "cbm_from_logits_results = cbm_experiment_loop(\n",
    "    cbm_from_logits_experiment_config,\n",
    "    load_from_cache=FROM_CACHE,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", train_0, test_0),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", train_1, test_1),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", train_2, test_2),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", train_3, test_3),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", train_4, test_4),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CW Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.CW.CWLayer as CW\n",
    "\n",
    "def conv_predictor_model_fn(\n",
    "    input_concept_classes=1,\n",
    "    output_concept_classes=2,\n",
    "):\n",
    "    estimator = tf.keras.models.Sequential([\n",
    "         tf.keras.layers.Conv2D(\n",
    "            filters=16,\n",
    "            kernel_size=(3,3),\n",
    "            padding=\"SAME\",\n",
    "            activation=\"relu\",\n",
    "            data_format=\"channels_last\",\n",
    "        ),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=(3,3),\n",
    "            padding=\"SAME\",\n",
    "            activation=\"relu\",\n",
    "            data_format=\"channels_last\",\n",
    "        ),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(\n",
    "            output_concept_classes if output_concept_classes > 2 else 1,\n",
    "            # We will merge the activation into the loss for numerical\n",
    "            # stability\n",
    "            activation=None,\n",
    "        ),\n",
    "    ])\n",
    "    estimator.compile(\n",
    "        # Use ADAM optimizer by default\n",
    "        optimizer='adam',\n",
    "        # Note: we assume labels come without a one-hot-encoding in the\n",
    "        #       case when the concepts are categorical.\n",
    "        loss=(\n",
    "            tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                from_logits=True\n",
    "            ) if output_concept_classes > 2 else\n",
    "            tf.keras.losses.BinaryCrossentropy(\n",
    "                from_logits=True,\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    return estimator\n",
    "\n",
    "\n",
    "\n",
    "def construct_cw_model(\n",
    "    input_shape,\n",
    "    num_outputs,\n",
    "    filter_groups,\n",
    "    units,\n",
    "    activation_mode,\n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    T=5,\n",
    "    eps=1e-5,\n",
    "    momentum=0.9,\n",
    "    c1=1e-4,\n",
    "    c2=0.9,\n",
    "    max_tau_iterations=500,\n",
    "    initial_tau=1000.0,\n",
    "    initial_beta=1e8,\n",
    "    initial_alpha=0,\n",
    "):\n",
    "    model_inputs = tf.keras.Input(shape=input_shape)\n",
    "    model_compute_graph = model_inputs\n",
    "    \n",
    "    # Start with our convolutions\n",
    "    num_convs = 0\n",
    "    cw_inputs = []\n",
    "    cw_ouputs = []\n",
    "    for filter_group in filter_groups:\n",
    "        # Add a default \"no CW layer\" to each filter group\n",
    "        # if they have not specified this feature\n",
    "        filter_group = list(map(\n",
    "            lambda x: x if len(x) == 3 else (x[0], x[1], False),\n",
    "            filter_group\n",
    "        ))\n",
    "        for (num_filters, kernel_size, cw_layer) in filter_group:\n",
    "            model_compute_graph = tf.keras.layers.Conv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=\"SAME\",\n",
    "                activation=None,\n",
    "                name=f'encoder_conv_{num_convs}',\n",
    "            )(model_compute_graph)\n",
    "            num_convs += 1\n",
    "            if cw_layer:\n",
    "                cw_inputs.append(model_compute_graph)\n",
    "                model_compute_graph = CW.ConceptWhiteningLayer(\n",
    "                    data_format=\"channels_last\",\n",
    "                    activation_mode=activation_mode,\n",
    "                    T=T,\n",
    "                    eps=eps,\n",
    "                    momentum=momentum,\n",
    "                    c1=c1,\n",
    "                    c2=c2,\n",
    "                    max_tau_iterations=max_tau_iterations,\n",
    "                    initial_tau=initial_tau,\n",
    "                    initial_beta=initial_beta,\n",
    "                    initial_alpha=initial_alpha,\n",
    "                )(\n",
    "                    model_compute_graph\n",
    "                )\n",
    "                cw_ouputs.append(model_compute_graph)\n",
    "            else:\n",
    "                model_compute_graph = tf.keras.layers.BatchNormalization(\n",
    "                    axis=-1,\n",
    "                )(model_compute_graph)\n",
    "            model_compute_graph = tf.keras.activations.relu(model_compute_graph)\n",
    "        # Then do a max pool here to control the parameter count of the model\n",
    "        # at the end of each group\n",
    "        model_compute_graph = tf.keras.layers.MaxPooling2D(\n",
    "            pool_size=max_pool_window,\n",
    "            strides=max_pool_stride,\n",
    "        )(\n",
    "            model_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Flatten this guy\n",
    "    model_compute_graph = tf.keras.layers.Flatten()(model_compute_graph)\n",
    "    \n",
    "    # Add a dropout if requested\n",
    "    if drop_prob:\n",
    "        model_compute_graph = tf.keras.layers.Dropout(drop_prob)(\n",
    "            model_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Finally, include the fully connected bottleneck here\n",
    "    for i, units in enumerate(units):\n",
    "        model_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(model_compute_graph)\n",
    "    \n",
    "    # Map to our output distribution to a flattened\n",
    "    # vector where we will extract distributions over\n",
    "    # all concept values\n",
    "    model_compute_graph = tf.keras.layers.Dense(\n",
    "        num_outputs,\n",
    "        activation=None,\n",
    "        name=\"logits\",\n",
    "    )(model_compute_graph)\n",
    "    \n",
    "  \n",
    "    cw_model = tf.keras.Model(\n",
    "        model_inputs,\n",
    "        model_compute_graph,\n",
    "        name=\"cw_model\",\n",
    "    )\n",
    "    cw_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "        loss=(\n",
    "            tf.keras.losses.BinaryCrossentropy(from_logits=True) if (num_outputs <= 2)\n",
    "            else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        ),\n",
    "        metrics=[\n",
    "            \"binary_accuracy\" if (num_outputs <= 2)\n",
    "            else tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    encoder = tf.keras.Model(\n",
    "        model_inputs,\n",
    "        cw_inputs,\n",
    "        name=\"encoder_model\",\n",
    "    )\n",
    "\n",
    "    cw_output_model = tf.keras.Model(\n",
    "        model_inputs,\n",
    "        cw_ouputs,\n",
    "        name=\"cw_output_model\",\n",
    "    )\n",
    "    return cw_model, encoder, cw_output_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "def channels_corr_mat(outputs):\n",
    "    if len(outputs.shape) == 2:\n",
    "        outputs = np.expand_dims(\n",
    "            np.expand_dims(outputs, axis=1),\n",
    "            axis=1,\n",
    "        )\n",
    "    # Change (N, H, W, C) to (C, N, H, W)\n",
    "    outputs = np.transpose(outputs, [3, 0, 1, 2])\n",
    "    # Change (C, N, H, W) to (C, NxHxW)\n",
    "    cnhw_shape = outputs.shape\n",
    "    outputs = np.transpose(np.reshape(outputs, [cnhw_shape[0], -1]))\n",
    "    outputs -= np.mean(outputs, axis=0, keepdims=True)\n",
    "    outputs = outputs / np.std(outputs, axis=0, keepdims=True)\n",
    "    return np.dot(outputs.transpose(), outputs) / outputs.shape[0]\n",
    "\n",
    "def conv_predictor_model_fn(\n",
    "    input_concept_classes=1,\n",
    "    output_concept_classes=2,\n",
    "):\n",
    "    estimator = tf.keras.models.Sequential([\n",
    "         tf.keras.layers.Conv2D(\n",
    "            filters=16,\n",
    "            kernel_size=(3,3),\n",
    "            padding=\"SAME\",\n",
    "            activation=\"relu\",\n",
    "            data_format=\"channels_last\",\n",
    "        ),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=(3,3),\n",
    "            padding=\"SAME\",\n",
    "            activation=\"relu\",\n",
    "            data_format=\"channels_last\",\n",
    "        ),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(\n",
    "            output_concept_classes if output_concept_classes > 2 else 1,\n",
    "            # We will merge the activation into the loss for numerical\n",
    "            # stability\n",
    "            activation=None,\n",
    "        ),\n",
    "    ])\n",
    "    estimator.compile(\n",
    "        # Use ADAM optimizer by default\n",
    "        optimizer='adam',\n",
    "        # Note: we assume labels come without a one-hot-encoding in the\n",
    "        #       case when the concepts are categorical.\n",
    "        loss=(\n",
    "            tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                from_logits=True\n",
    "            ) if output_concept_classes > 2 else\n",
    "            tf.keras.losses.BinaryCrossentropy(\n",
    "                from_logits=True,\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    return estimator\n",
    "\n",
    "\n",
    "\n",
    "def concept_scores(\n",
    "    cw_layer,\n",
    "    inputs,\n",
    "    aggregator='max_pool_mean',\n",
    "    concept_indices=None,\n",
    "):\n",
    "    outputs = cw_layer(inputs, training=False)\n",
    "    if len(tf.shape(outputs)) == 2:\n",
    "        # Then the scores are already computed by our forward pass\n",
    "        scores = outputs\n",
    "    else:\n",
    "        # Else, we need to do some aggregation\n",
    "        if aggregator == 'mean':\n",
    "            # Compute the mean over all channels\n",
    "            scores = tf.math.reduce_mean(outputs, axis=[2, 3])\n",
    "        elif aggregator == 'max_pool_mean':\n",
    "            # First downsample using a max pool and then continue with\n",
    "            # a mean\n",
    "            window_size = min(\n",
    "                2,\n",
    "                outputs.shape[-1],\n",
    "                outputs.shape[-2],\n",
    "            )\n",
    "            scores = tf.nn.max_pool(\n",
    "                outputs,\n",
    "                ksize=window_size,\n",
    "                strides=window_size,\n",
    "                padding=\"SAME\",\n",
    "                data_format=\"NCHW\",\n",
    "            )\n",
    "            scores = tf.math.reduce_mean(scores, axis=[2, 3])\n",
    "        elif aggregator == 'max':\n",
    "            # Simply select the maximum value across a given channel\n",
    "            scores = tf.math.reduce_max(outputs, axis=[2, 3])\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported aggregator {aggregator}.')\n",
    "\n",
    "    if concept_indices is not None:\n",
    "        return scores[:, concept_indices]\n",
    "    return scores\n",
    "\n",
    "\n",
    "\n",
    "def cw_experiment_loop(\n",
    "    experiment_config,\n",
    "    datasets,\n",
    "    load_from_cache=False,\n",
    "    oracle_matrix_cache=None,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    experiment_variables = dict(\n",
    "        config = experiment_config,\n",
    "        niss=[],\n",
    "    )\n",
    "    res_dir = experiment_config['niching_results_dir']\n",
    "    if load_from_cache:\n",
    "        if os.path.exists(os.path.join(res_dir, 'results_niching.joblib')):\n",
    "            experiment_variables = load(os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            return experiment_variables\n",
    "        \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    count = 0\n",
    "    start_ind = 0\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    )\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    for (ds_name, (x_train, y_train, c_train), (x_test, y_test, c_test)) in datasets[start_ind:]:\n",
    "        print(\"Training with dataset:\", ds_name)\n",
    "        \n",
    "        if not experiment_config.get(\"exclusive_concepts\", False):\n",
    "            concept_groups = [\n",
    "                x_train[c_train[:, i] == 1, :, :, :]\n",
    "                for i in range(c_train.shape[-1])\n",
    "            ]\n",
    "        else:\n",
    "            concept_groups = [\n",
    "                x_train[np.logical_and(c_train[:, i] == 1, np.sum(c_train, axis=-1) == 1), :, :, :]\n",
    "                for i in range(c_train.shape[-1])\n",
    "            ]\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} of {ds_name}\")\n",
    "            if not experiment_config.get(\"exclusive_test_concepts\", False):\n",
    "                test_concept_groups = [\n",
    "                    x_train[c_train[:, i] == 1, :, :, :]\n",
    "                    for i in range(c_test.shape[-1])\n",
    "                ]\n",
    "            else:\n",
    "                test_concept_groups = [\n",
    "                    x_test[np.logical_and(c_test[:, i] == 1, np.sum(c_test, axis=-1) == 1), :, :, :]\n",
    "                    for i in range(c_test.shape[-1])\n",
    "                ]\n",
    "            print(\"Sizes of test_concept_groups:\", list(map(lambda x: x.shape, test_concept_groups)))\n",
    "            for i, group in enumerate(test_concept_groups):\n",
    "                max_test_size = experiment_config.get(\"max_concept_group_size\", group.shape[-1])\n",
    "                test_concept_groups[i] = (\n",
    "                    group[np.random.choice(group.shape[0], max_test_size), : :, :]\n",
    "                    if group.shape[0] > max_test_size else group\n",
    "                )\n",
    "\n",
    "\n",
    "            # Construct our CW model\n",
    "            model, encoder, cw_model = construct_cw_model(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "                filter_groups=experiment_config[\"filter_groups\"],\n",
    "                units=experiment_config[\"units\"],\n",
    "                drop_prob=experiment_config.get(\"drop_prob\", 0),\n",
    "                max_pool_window=experiment_config.get(\"max_pool_window\", (2, 2)),\n",
    "                max_pool_stride=experiment_config.get(\"max_pool_stride\", (2, 2)),\n",
    "                T=experiment_config.get(\"T\", 5),\n",
    "                eps=experiment_config.get(\"eps\", 1e-5),\n",
    "                momentum=experiment_config.get(\"momentum\", 0.9),\n",
    "                activation_mode=experiment_config[\"activation_mode\"],\n",
    "                c1=experiment_config.get(\"c1\", 1e-4),\n",
    "                c2=experiment_config.get(\"c2\", 0.9),\n",
    "                max_tau_iterations=experiment_config.get(\"max_tau_iterations\", 500),\n",
    "                initial_tau=experiment_config.get(\"initial_tau\", 1000),\n",
    "                initial_beta=experiment_config.get(\"initial_beta\", 1e8),\n",
    "                initial_alpha=experiment_config.get(\"initial_alpha\", 0),\n",
    "            )\n",
    "            \n",
    "            print(\"\\t\\tCW training completed\")\n",
    "            model = load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/end_to_end_model_{ds_name}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "             # finding niches for several values of beta\n",
    "            niche_sizes = []\n",
    "            niche_impurities = []\n",
    "            # And estimate the area under the curve using the trapezoid method\n",
    "            total_area_under_curve_map = defaultdict(float)\n",
    "            prev_value_map = {}\n",
    "            delta_beta = experiment_config.get(\"delta_beta\", 0.05)\n",
    "            if not experiment_config['feature_map']:\n",
    "                encoder = tf.keras.Model(\n",
    "                    inputs=model.get_layer(model.layers[0].name).input,\n",
    "                    outputs=model.get_layer(model.layers[13].name).output,\n",
    "                )\n",
    "                c_train_pred = concept_scores(\n",
    "                    model.layers[14],\n",
    "                    encoder(x_train),\n",
    "                    aggregator=experiment_config['aggregator'],\n",
    "                ).numpy()\n",
    "                c_train_pred = c_train_pred[:, :experiment_config[\"num_concepts\"]]\n",
    "                c_test_pred = concept_scores(\n",
    "                    model.layers[14],\n",
    "                    encoder(x_test),\n",
    "                    aggregator=experiment_config['aggregator'],\n",
    "                ).numpy()[:, :experiment_config[\"num_concepts\"]]\n",
    "                \n",
    "            else:\n",
    "                encoder = tf.keras.Model(\n",
    "                    inputs=model.get_layer(model.layers[0].name).input,\n",
    "                    outputs=model.get_layer(model.layers[14].name).output,\n",
    "                )\n",
    "\n",
    "                c_train_pred = encoder(x_train)\n",
    "                c_train_pred = c_train_pred[:, :, :, :experiment_config[\"num_concepts\"]]\n",
    "                c_test_pred = encoder(x_test)\n",
    "                c_test_pred = c_test_pred[:, :, :, :experiment_config[\"num_concepts\"]]\n",
    "                out_shape = c_train_pred.shape[1]*c_train_pred.shape[2]\n",
    "                c_train_pred = c_train_pred.numpy().reshape(-1, out_shape, c_train_pred.shape[3])\n",
    "                c_test_pred = c_test_pred.numpy().reshape(-1, out_shape, c_test_pred.shape[3])\n",
    "            \n",
    "            experiment_variables['niss'].append(\n",
    "                niching.niche_impurity_score(\n",
    "                    c_soft=c_test_pred,\n",
    "                    c_true=c_test,\n",
    "                    c_soft_train=c_train_pred,\n",
    "                    c_true_train=c_train,\n",
    "                )\n",
    "            )\n",
    "            print(f'\\t\\tNIS: {experiment_variables[\"niss\"][-1]:.2f}')\n",
    "            \n",
    "            os.makedirs(res_dir, exist_ok=True)\n",
    "            dump(experiment_variables, os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            \n",
    "    return experiment_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(CW)\n",
    "reload(niching)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "cw_features_experiment_config = dict(\n",
    "    batch_size=256,\n",
    "    max_epochs=50,\n",
    "    pre_train_epochs=50,\n",
    "    post_train_epochs=0,\n",
    "    cw_train_freq=20,\n",
    "    cw_train_batch_steps=20,\n",
    "    cw_train_iterations=1,\n",
    "    exclusive_concepts=False,\n",
    "    exclusive_test_concepts=False,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    filter_groups=[\n",
    "        [(8, (7, 7), False)],\n",
    "        [(16, (5, 5), False)],\n",
    "        [(32, (3, 3), False)],\n",
    "        [(64, (3, 3), True)],\n",
    "    ],\n",
    "    units=[64, 64, 64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(multiclass_task_bin_concepts_dep_0_complete_train[1]))\n",
    "        if len(set(multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2 else 1\n",
    "    ),\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(RESULTS_DIR, \"cw/balanced_multiclass_tasks_purity\"),\n",
    "    input_shape=multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    num_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    cw_layer=14,\n",
    "    aggregator='max_pool_mean',\n",
    "    activation_mode='max_pool_mean',\n",
    "    concept_auc_freq=0,\n",
    "    holdout_fraction=0.1,\n",
    "    heatmap_display=True,\n",
    "    T=5,\n",
    "    eps=1e-5,\n",
    "    momentum=0.9,\n",
    "    initial_tau=1000,\n",
    "    initial_beta=1e8,\n",
    "    initial_alpha=0,\n",
    "    drop_prob=0,    \n",
    "    feature_map=True,\n",
    "    add='',\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "cw_features_experiment_config['niching_results_dir'] = os.path.join(\n",
    "    NICHING_RESULTS_DIR,\n",
    "    f'cw/base_feature_{cw_features_experiment_config[\"feature_map\"]}{cw_features_experiment_config[\"add\"]}',\n",
    ")\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "# model, c_train_pred, y_train, c_test_pred, y_test = cw_experiment_loop(\n",
    "cw_features_results = cw_experiment_loop(\n",
    "    cw_features_experiment_config,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", train_0, test_0),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", train_1, test_1),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", train_2, test_2),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", train_3, test_3),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", train_4, test_4),\n",
    "    ],\n",
    "    load_from_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(CW)\n",
    "reload(niching)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "cw_base_experiment_config = dict(\n",
    "    batch_size=256,\n",
    "    max_epochs=50,\n",
    "    pre_train_epochs=50,\n",
    "    post_train_epochs=0,\n",
    "    cw_train_freq=20,\n",
    "    cw_train_batch_steps=20,\n",
    "    cw_train_iterations=1,\n",
    "    exclusive_concepts=False,\n",
    "    exclusive_test_concepts=False,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    filter_groups=[\n",
    "        [(8, (7, 7), False)],\n",
    "        [(16, (5, 5), False)],\n",
    "        [(32, (3, 3), False)],\n",
    "        [(64, (3, 3), True)],\n",
    "    ],\n",
    "    units=[64, 64, 64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(multiclass_task_bin_concepts_dep_0_complete_train[1]))\n",
    "        if len(set(multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2 else 1\n",
    "    ),\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(RESULTS_DIR, \"cw/balanced_multiclass_tasks_purity_max_pool_mean\"),\n",
    "    input_shape=multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    num_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    cw_layer=14,\n",
    "    aggregator='max_pool_mean',\n",
    "    activation_mode='max_pool_mean',\n",
    "    concept_auc_freq=0,\n",
    "    holdout_fraction=0.1,\n",
    "    heatmap_display=True,\n",
    "    T=5,\n",
    "    eps=1e-5,\n",
    "    momentum=0.9,\n",
    "    initial_tau=1000,\n",
    "    initial_beta=1e8,\n",
    "    initial_alpha=0,\n",
    "    drop_prob=0,    \n",
    "    feature_map=False,\n",
    "    add='',\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "cw_base_experiment_config['niching_results_dir'] = os.path.join(\n",
    "    NICHING_RESULTS_DIR,\n",
    "    f'cw/base_feature_{cw_base_experiment_config[\"feature_map\"]}{cw_base_experiment_config[\"add\"]}',\n",
    ")\n",
    "\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "cw_base_results = cw_experiment_loop(\n",
    "    cw_base_experiment_config,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", train_0, test_0),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", train_1, test_1),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", train_2, test_2),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", train_3, test_3),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", train_4, test_4),\n",
    "    ],\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(CW)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "cw_mean_experiment_config = dict(\n",
    "    batch_size=256,\n",
    "    max_epochs=50,\n",
    "    pre_train_epochs=50,\n",
    "    post_train_epochs=0,\n",
    "    cw_train_freq=20,\n",
    "    cw_train_batch_steps=20,\n",
    "    cw_train_iterations=1,\n",
    "    exclusive_concepts=False,\n",
    "    exclusive_test_concepts=False,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    filter_groups=[\n",
    "        [(8, (7, 7), False)],\n",
    "        [(16, (5, 5), False)],\n",
    "        [(32, (3, 3), False)],\n",
    "        [(64, (3, 3), True)],\n",
    "    ],\n",
    "    units=[64, 64, 64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(multiclass_task_bin_concepts_dep_0_complete_train[1]))\n",
    "        if len(set(multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2 else 1\n",
    "    ),\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(RESULTS_DIR, \"cw/balanced_multiclass_tasks_purity_mean\"),\n",
    "    input_shape=multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    num_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    cw_layer=14,\n",
    "    aggregator='mean',\n",
    "    activation_mode='mean',\n",
    "    concept_auc_freq=0,\n",
    "    holdout_fraction=0.1,\n",
    "    heatmap_display=True,\n",
    "    T=5,\n",
    "    eps=1e-5,\n",
    "    momentum=0.9,\n",
    "    initial_tau=1000,\n",
    "    initial_beta=1e8,\n",
    "    initial_alpha=0,\n",
    "    drop_prob=0,    \n",
    "    feature_map=False,\n",
    "    add='_mean',\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "cw_mean_experiment_config['niching_results_dir'] = os.path.join(\n",
    "    NICHING_RESULTS_DIR,\n",
    "    f'cw/base_feature_{cw_mean_experiment_config[\"feature_map\"]}{cw_mean_experiment_config[\"add\"]}',\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(cw_mean_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "cw_mean_figure_dir = os.path.join(cw_mean_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(cw_mean_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "cw_mean_results = cw_experiment_loop(\n",
    "    cw_mean_experiment_config,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", train_0, test_0),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", train_1, test_1),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", train_2, test_2),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", train_3, test_3),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", train_4, test_4),\n",
    "    ],\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada-ML-VAE Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weakly Labelled Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwtrain_0 = [np.concatenate((train_0[0], train_0[0]), axis=1), train_0[2], train_0[1]]\n",
    "cwtrain_1 = [np.concatenate((train_1[0], train_1[0]), axis=1), train_1[2], train_1[1]]\n",
    "cwtrain_2 = [np.concatenate((train_2[0], train_2[0]), axis=1), train_2[2], train_2[1]]\n",
    "cwtrain_3 = [np.concatenate((train_3[0], train_3[0]), axis=1), train_3[2], train_3[1]]\n",
    "cwtrain_4 = [np.concatenate((train_4[0], train_4[0]), axis=1), train_4[2], train_4[1]]\n",
    "\n",
    "cwtest_0 = [np.concatenate((test_0[0], test_0[0]), axis=1), test_0[2], test_0[1]]\n",
    "cwtest_1 = [np.concatenate((test_1[0], test_1[0]), axis=1), test_1[2], test_1[1]]\n",
    "cwtest_2 = [np.concatenate((test_2[0], test_2[0]), axis=1), test_2[2], test_2[1]]\n",
    "cwtest_3 = [np.concatenate((test_3[0], test_3[0]), axis=1), test_3[2], test_3[1]]\n",
    "cwtest_4 = [np.concatenate((test_4[0], test_4[0]), axis=1), test_4[2], test_4[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.VAE.weak_vae as weak_vae\n",
    "import concepts_xai.methods.VAE.baseVAE as base_vae\n",
    "import concepts_xai.methods.VAE.losses as vae_losses\n",
    "reload(vae_losses)\n",
    "reload(weak_vae)\n",
    "\n",
    "def construct_vae_encoder(\n",
    "    input_shape,\n",
    "    filter_groups,\n",
    "    units,\n",
    "    latent_dims,\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    include_norm=False,\n",
    "    include_pool=False,\n",
    "):\n",
    "    encoder_inputs = tf.keras.Input(shape=input_shape)\n",
    "    encoder_compute_graph = encoder_inputs\n",
    "    \n",
    "    # Start with our convolutions\n",
    "    num_convs = 0\n",
    "    for filter_group in filter_groups:\n",
    "        for filter_args in filter_group:\n",
    "            if len(filter_args) == 2:\n",
    "                filter_args = (*filter_args, 1)\n",
    "            (num_filters, kernel_size, stride) = filter_args\n",
    "            encoder_compute_graph = tf.keras.layers.Conv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                strides=stride,\n",
    "                padding=\"SAME\",\n",
    "                activation=None if include_norm else \"relu\",\n",
    "                name=f'encoder_conv_{num_convs}',\n",
    "            )(encoder_compute_graph)\n",
    "            num_convs += 1\n",
    "            if include_norm:\n",
    "                encoder_compute_graph = tf.keras.layers.BatchNormalization()(\n",
    "                    encoder_compute_graph\n",
    "                )\n",
    "                encoder_compute_graph = tf.keras.activations.relu(encoder_compute_graph)\n",
    "        if include_pool:\n",
    "            # Then do a max pool here to control the parameter count of the model\n",
    "            # at the end of each group\n",
    "            encoder_compute_graph = tf.keras.layers.MaxPooling2D(\n",
    "                pool_size=max_pool_window,\n",
    "                strides=max_pool_stride,\n",
    "            )(\n",
    "                encoder_compute_graph\n",
    "            )\n",
    "    \n",
    "    # Flatten this guy\n",
    "    encoder_compute_graph = tf.keras.layers.Flatten()(encoder_compute_graph)\n",
    "    \n",
    "    # Add a dropout if requested\n",
    "    if drop_prob:\n",
    "        encoder_compute_graph = tf.keras.layers.Dropout(drop_prob)(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Finally, include the fully connected bottleneck here\n",
    "    for i, units in enumerate(units):\n",
    "        encoder_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(encoder_compute_graph)\n",
    "\n",
    "    mean = tf.keras.layers.Dense(latent_dims, activation=None, name=\"means\")(encoder_compute_graph)\n",
    "    log_var = tf.keras.layers.Dense(latent_dims, activation=None, name=\"log_var\")(encoder_compute_graph)\n",
    "    return tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        [mean, log_var],\n",
    "        name=\"encoder\",\n",
    "    )\n",
    "\n",
    "def construct_vae_decoder(\n",
    "    units,\n",
    "    output_shape,\n",
    "    latent_dims,\n",
    "):\n",
    "    \"\"\"CNN decoder architecture used in the 'Challenging Common Assumptions in the Unsupervised Learning\n",
    "       of Disentangled Representations' paper (https://arxiv.org/abs/1811.12359)\n",
    "\n",
    "       Note: model is uncompiled\n",
    "    \"\"\"\n",
    "\n",
    "    latent_inputs = tf.keras.Input(shape=(latent_dims,))\n",
    "    model_out = latent_inputs\n",
    "    for unit in units:\n",
    "        model_out = tf.keras.layers.Dense(\n",
    "            unit,\n",
    "            activation='relu',\n",
    "        )(model_out)\n",
    "    model_out = tf.keras.layers.Reshape([4, 4, 32])(model_out)\n",
    "\n",
    "    model_out = tf.keras.layers.Conv2DTranspose(\n",
    "        filters=64,\n",
    "        kernel_size=4,\n",
    "        strides=2,\n",
    "        activation='relu',\n",
    "        padding=\"same\"\n",
    "    )(model_out)\n",
    "\n",
    "    model_out = tf.keras.layers.Conv2DTranspose(\n",
    "        filters=32,\n",
    "        kernel_size=4,\n",
    "        strides=2,\n",
    "        activation='relu',\n",
    "        padding=\"same\"\n",
    "    )(model_out)\n",
    "\n",
    "    model_out = tf.keras.layers.Conv2DTranspose(\n",
    "        filters=32,\n",
    "        kernel_size=4,\n",
    "        strides=2,\n",
    "        activation='relu',\n",
    "        padding=\"same\",\n",
    "    )(model_out)\n",
    "\n",
    "    model_out = tf.keras.layers.Conv2DTranspose(\n",
    "        filters=output_shape[-1],\n",
    "        kernel_size=4,\n",
    "        strides=2,\n",
    "        padding=\"same\",\n",
    "        activation=None,\n",
    "    )(model_out)\n",
    "    model_out = tf.keras.layers.Reshape(output_shape)(model_out)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=latent_inputs,\n",
    "        outputs=[model_out],\n",
    "    )\n",
    "\n",
    "def construct_wvae(\n",
    "    input_shape,\n",
    "    latent_dims,\n",
    "    filter_groups,\n",
    "    encoder_units,\n",
    "    decoder_units,\n",
    "    drop_prob=0.5,\n",
    "    include_pool=False,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    learning_rate=1e-3,\n",
    "    beta=1.0,\n",
    "    vae_model=weak_vae.MLVaeArgmax,\n",
    "):\n",
    "    wvae_encoder = construct_vae_encoder(\n",
    "        input_shape=input_shape,\n",
    "        filter_groups=filter_groups,\n",
    "        units=encoder_units,\n",
    "        drop_prob=drop_prob,\n",
    "        include_pool=include_pool,\n",
    "        max_pool_window=max_pool_window,\n",
    "        max_pool_stride=max_pool_stride,\n",
    "        latent_dims=latent_dims,\n",
    "    )\n",
    "    wvae_decoder = construct_vae_decoder(\n",
    "        output_shape=input_shape,\n",
    "        units=decoder_units,\n",
    "        latent_dims=latent_dims,\n",
    "    )\n",
    "\n",
    "    wvae_model = vae_model(\n",
    "        encoder=wvae_encoder,\n",
    "        decoder=wvae_decoder,\n",
    "        loss_fn=vae_losses.bernoulli_fn_wrapper(),\n",
    "        beta=beta,\n",
    "    )\n",
    "    wvae_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    )\n",
    "    return wvae_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import concepts_xai.evaluation.metrics.purity as purity\n",
    "import concepts_xai.methods.VAE.betaVAE as beta_vae\n",
    "\n",
    "############################################################################\n",
    "## Experiment loop\n",
    "############################################################################\n",
    "\n",
    "def construct_trivial_auc_mat(num_concepts):\n",
    "    result = np.ones((num_concepts, num_concepts), dtype=np.float32) * 0.5\n",
    "    return result + np.eye(num_concepts, dtype=np.float32) * 0.5\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def wvae_experiment_loop(\n",
    "    experiment_config,\n",
    "    datasets,\n",
    "    wvae,\n",
    "    latent='',\n",
    "    load_from_cache=False,\n",
    "    include_all_losses=True,\n",
    "    vae_model=weak_vae.MLVaeArgmax,\n",
    "    oracle_matrix_cache=None,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    if vae_model != beta_vae.BetaVAE:\n",
    "        prefix = \"\"\n",
    "        split_fn = lambda x: x[:, :x.shape[1]//2, ...] if len(x.shape) > 1 else x[:x.shape[0]//2]\n",
    "    else:\n",
    "        prefix = \"balanced_\"\n",
    "        split_fn = lambda x: x\n",
    "    experiment_variables = dict(\n",
    "        config = experiment_config,\n",
    "        niss=[],\n",
    "    )\n",
    "    res_dir = experiment_config['niching_results_dir']\n",
    "    if load_from_cache:\n",
    "        if os.path.exists(os.path.join(res_dir, 'results_niching.joblib')):\n",
    "            experiment_variables = load(os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    count = 0\n",
    "    start_ind = 0\n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    for (ds_name, (x_train, c_train, y_train), (x_test, c_test, y_test)) in datasets[start_ind:]:\n",
    "        latent_dim = experiment_config['latent_dim']\n",
    "        print(\"Training with latent dimensions\", latent_dim, \"in dataset\", ds_name)\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']}\")\n",
    "            \n",
    "            # Time to actually construct and train the WVAE\n",
    "            wvae_model = construct_wvae(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                latent_dims=latent_dim,\n",
    "                filter_groups=experiment_config[\"filter_groups\"],\n",
    "                encoder_units=experiment_config[\"encoder_units\"],\n",
    "                decoder_units=experiment_config[\"decoder_units\"],\n",
    "                drop_prob=experiment_config['drop_prob'],\n",
    "                max_pool_window=experiment_config['max_pool_window'],\n",
    "                max_pool_stride=experiment_config['max_pool_stride'],\n",
    "                beta=experiment_config['beta'],\n",
    "                vae_model=vae_model,\n",
    "            )\n",
    "            \n",
    "                \n",
    "            print(\"\\t\\tWVAE training completed\")\n",
    "            \n",
    "            wvae_model.encoder = load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/{prefix}{ds_name}_encoder_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            print(f\"\\t\\tComputing purity score...\")\n",
    "            c_train_pred = wvae_model.sample_from_latent_distribution(\n",
    "                *wvae_model.encoder(split_fn(x_train))\n",
    "            ).numpy()\n",
    "            c_test_pred = wvae_model.sample_from_latent_distribution(\n",
    "                *wvae_model.encoder(split_fn(x_test))\n",
    "            ).numpy()\n",
    "            experiment_variables['niss'].append(\n",
    "                niching.niche_impurity_score(\n",
    "                    c_soft=c_test_pred,\n",
    "                    c_true=c_test,\n",
    "                    c_soft_train=c_train_pred,\n",
    "                    c_true_train=c_train,\n",
    "                )\n",
    "            )\n",
    "            print(f'\\t\\tNIS: {experiment_variables[\"niss\"][-1]:.2f}')\n",
    "            \n",
    "            os.makedirs(res_dir, exist_ok=True)\n",
    "            dump(experiment_variables, os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            \n",
    "    return experiment_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(weak_vae)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "ada_mlvae_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    latent_dim=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[256, 512],\n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    beta=1,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"ada_ml_vae/multilabel_purity_latent_{multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}\"\n",
    "    ),\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        f\"ada_ml_vae/multilabel_purity_latent_{multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}\"\n",
    "    ),\n",
    "    num_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "ada_mlvae_results = wvae_experiment_loop(\n",
    "    ada_mlvae_experiment_config,\n",
    "    wvae=\"ada_ml_vae\",\n",
    "    latent=f\"_latent_{multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}\",\n",
    "    include_all_losses=True,\n",
    "    vae_model=weak_vae.MLVaeArgmax,\n",
    "    datasets=[\n",
    "        (\"multiclass_task_bin_concepts_dep_0\", cwtrain_0, cwtest_0),\n",
    "        (\"multiclass_task_bin_concepts_dep_1\", cwtrain_1, cwtest_1),\n",
    "        (\"multiclass_task_bin_concepts_dep_2\", cwtrain_2, cwtest_2),\n",
    "        (\"multiclass_task_bin_concepts_dep_3\", cwtrain_3, cwtest_3),\n",
    "        (\"multiclass_task_bin_concepts_dep_4\", cwtrain_4, cwtest_4),\n",
    "    ],\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(weak_vae)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "ada_mlvae_extended_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    latent_dim=2*multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[256, 512],\n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    beta=1,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"ada_ml_vae/multilabel_purity_latent_{2*multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}\"\n",
    "    ),\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        f\"ada_ml_vae/multilabel_purity_latent_{2*multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}\"\n",
    "    ),\n",
    "    num_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "ada_mlvae_extended_results = wvae_experiment_loop(\n",
    "    ada_mlvae_extended_experiment_config,\n",
    "    wvae=\"ada_ml_vae\",\n",
    "    latent=f\"_latent_{2* multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}\",\n",
    "    include_all_losses=True,\n",
    "    vae_model=weak_vae.MLVaeArgmax,\n",
    "    datasets=[\n",
    "        (\"multiclass_task_bin_concepts_dep_0\", cwtrain_0, cwtest_0),\n",
    "        (\"multiclass_task_bin_concepts_dep_1\", cwtrain_1, cwtest_1),\n",
    "        (\"multiclass_task_bin_concepts_dep_2\", cwtrain_2, cwtest_2),\n",
    "        (\"multiclass_task_bin_concepts_dep_3\", cwtrain_3, cwtest_3),\n",
    "        (\"multiclass_task_bin_concepts_dep_4\", cwtrain_4, cwtest_4),\n",
    "    ],\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada-GVAE Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(weak_vae)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "ada_gvae_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    latent_dim=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[256, 512],\n",
    "    \n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    beta=1,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"ada_g_vae/multilabel_purity_latent_{multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}\"\n",
    "    ),\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        f\"ada_g_vae/multilabel_purity_latent_{multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}\"\n",
    "    ),\n",
    "    num_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "ada_gvae_results = wvae_experiment_loop(\n",
    "    ada_gvae_experiment_config,\n",
    "    wvae=\"ada_g_vae\",\n",
    "    latent=f\"_latent_{multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}\",\n",
    "    load_from_cache=FROM_CACHE,\n",
    "    include_all_losses=True,\n",
    "    vae_model=weak_vae.GroupVAEArgmax,\n",
    "    datasets=[\n",
    "        (\"multiclass_task_bin_concepts_dep_0\", cwtrain_0, cwtest_0),\n",
    "        (\"multiclass_task_bin_concepts_dep_1\", cwtrain_1, cwtest_1),\n",
    "        (\"multiclass_task_bin_concepts_dep_2\", cwtrain_2, cwtest_2),\n",
    "        (\"multiclass_task_bin_concepts_dep_3\", cwtrain_3, cwtest_3),\n",
    "        (\"multiclass_task_bin_concepts_dep_4\", cwtrain_4, cwtest_4),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(weak_vae)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "ada_gvae_extended_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    latent_dim=2*multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[256, 512],\n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    beta=1,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"ada_g_vae/multilabel_purity_latent_{2*multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}\"\n",
    "    ),\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        f\"ada_g_vae/multilabel_purity_latent_{2*multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}\"\n",
    "    ),\n",
    "    num_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "ada_gvae_extended_results = wvae_experiment_loop(\n",
    "    ada_gvae_extended_experiment_config,\n",
    "    wvae=\"ada_g_vae\",\n",
    "    latent=f\"_latent_{2*multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}\",\n",
    "    load_from_cache=FROM_CACHE,\n",
    "    include_all_losses=True,\n",
    "    vae_model=weak_vae.GroupVAEArgmax,\n",
    "    datasets=[\n",
    "        (\"multiclass_task_bin_concepts_dep_0\", cwtrain_0, cwtest_0),\n",
    "        (\"multiclass_task_bin_concepts_dep_1\", cwtrain_1, cwtest_1),\n",
    "        (\"multiclass_task_bin_concepts_dep_2\", cwtrain_2, cwtest_2),\n",
    "        (\"multiclass_task_bin_concepts_dep_3\", cwtrain_3, cwtest_3),\n",
    "        (\"multiclass_task_bin_concepts_dep_4\", cwtrain_4, cwtest_4),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta-VAE Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.VAE.betaVAE as beta_vae\n",
    "\n",
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(beta_vae)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "beta_vae_experiment_config = dict(\n",
    "    batch_size=64,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    latent_dim=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    beta=10,\n",
    "    \n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[256, 512],\n",
    "    \n",
    "    latent_decoder_units=[64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(multiclass_task_bin_concepts_dep_0_complete_train[1])) if len(set(multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    predictor_max_epochs=100,\n",
    "    \n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"beta_vae/balanced_multilabel_purity_latent_{multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}_beta_{10}\"\n",
    "    ),\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        f\"beta_vae/purity_latent_{multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}_beta_{10}\"\n",
    "    ),\n",
    "    num_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "beta_vae_results = wvae_experiment_loop(\n",
    "    beta_vae_experiment_config,\n",
    "    wvae=\"beta_vae\",\n",
    "    latent=f\"_latent_{beta_vae_experiment_config['latent_dim']}\",\n",
    "    include_all_losses=True,\n",
    "    vae_model=beta_vae.BetaVAE,\n",
    "    datasets=[\n",
    "        (\"multiclass_task_bin_concepts_dep_0_complete\", train_0, test_0),\n",
    "        (\"multiclass_task_bin_concepts_dep_1_complete\", train_1, test_1),\n",
    "        (\"multiclass_task_bin_concepts_dep_2_complete\", train_2, test_2),\n",
    "        (\"multiclass_task_bin_concepts_dep_3_complete\", train_3, test_3),\n",
    "        (\"multiclass_task_bin_concepts_dep_4_complete\", train_4, test_4),\n",
    "    ],\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.VAE.betaVAE as beta_vae\n",
    "\n",
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(beta_vae)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "beta_vae_extended_experiment_config = dict(\n",
    "    batch_size=64,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    latent_dim=2*multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    beta=10,\n",
    "    \n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[256, 512],\n",
    "    \n",
    "    latent_decoder_units=[64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(multiclass_task_bin_concepts_dep_0_complete_train[1])) if len(set(multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    predictor_max_epochs=100,\n",
    "    \n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"beta_vae/balanced_multilabel_purity_latent_{2*multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}_beta_{10}\"\n",
    "    ),\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        f\"beta_vae/purity_latent_{2*multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}_beta_{10}\"\n",
    "    ),\n",
    "    num_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "beta_vae_extended_results = wvae_experiment_loop(\n",
    "    beta_vae_extended_experiment_config,\n",
    "    wvae=\"beta_vae\",\n",
    "    latent=f\"_latent_{2*beta_vae_extended_experiment_config['latent_dim']}\",\n",
    "    include_all_losses=True,\n",
    "    vae_model=beta_vae.BetaVAE,\n",
    "    datasets=[\n",
    "        (\"multiclass_task_bin_concepts_dep_0_complete\", train_0, test_0),\n",
    "        (\"multiclass_task_bin_concepts_dep_1_complete\", train_1, test_1),\n",
    "        (\"multiclass_task_bin_concepts_dep_2_complete\", train_2, test_2),\n",
    "        (\"multiclass_task_bin_concepts_dep_3_complete\", train_3, test_3),\n",
    "        (\"multiclass_task_bin_concepts_dep_4_complete\", train_4, test_4),\n",
    "    ],\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.VAE.betaVAE as beta_vae\n",
    "\n",
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(beta_vae)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "vae_experiment_config = dict(\n",
    "    batch_size=64,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    latent_dim=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    beta=1,\n",
    "    \n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[256, 512],\n",
    "    \n",
    "    latent_decoder_units=[64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(multiclass_task_bin_concepts_dep_0_complete_train[1])) if len(set(multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    predictor_max_epochs=100,\n",
    "    \n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"beta_vae/balanced_multilabel_purity_latent_{multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}_beta_{1}\"\n",
    "    ),\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        f\"beta_vae/purity_latent_{multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}_beta_{1}\"\n",
    "    ),\n",
    "    num_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "vae_results = wvae_experiment_loop(\n",
    "    vae_experiment_config,\n",
    "    wvae=\"beta_vae\",\n",
    "    latent=f\"_latent_{vae_experiment_config['latent_dim']}\",\n",
    "    include_all_losses=True,\n",
    "    vae_model=beta_vae.BetaVAE,\n",
    "    datasets=[\n",
    "        (\"multiclass_task_bin_concepts_dep_0_complete\", train_0, test_0),\n",
    "        (\"multiclass_task_bin_concepts_dep_1_complete\", train_1, test_1),\n",
    "        (\"multiclass_task_bin_concepts_dep_2_complete\", train_2, test_2),\n",
    "        (\"multiclass_task_bin_concepts_dep_3_complete\", train_3, test_3),\n",
    "        (\"multiclass_task_bin_concepts_dep_4_complete\", train_4, test_4),\n",
    "    ],\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.VAE.betaVAE as beta_vae\n",
    "\n",
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(beta_vae)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "vae_extended_experiment_config = dict(\n",
    "    batch_size=64,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    latent_dim=2*multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    beta=1,\n",
    "    \n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[256, 512],\n",
    "    \n",
    "    latent_decoder_units=[64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(multiclass_task_bin_concepts_dep_0_complete_train[1])) if len(set(multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    predictor_max_epochs=100,\n",
    "    \n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"beta_vae/balanced_multilabel_purity_latent_{2*multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}_beta_{1}\"\n",
    "    ),\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        f\"beta_vae/purity_latent_{2*multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}_beta_{1}\"\n",
    "    ),\n",
    "    num_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "vae_extended_results = wvae_experiment_loop(\n",
    "    vae_extended_experiment_config,\n",
    "    wvae=\"beta_vae\",\n",
    "    latent=f\"_latent_{2*vae_extended_experiment_config['latent_dim']}\",\n",
    "    include_all_losses=True,\n",
    "    vae_model=beta_vae.BetaVAE,\n",
    "    datasets=[\n",
    "        (\"multiclass_task_bin_concepts_dep_0_complete\", train_0, test_0),\n",
    "        (\"multiclass_task_bin_concepts_dep_1_complete\", train_1, test_1),\n",
    "        (\"multiclass_task_bin_concepts_dep_2_complete\", train_2, test_2),\n",
    "        (\"multiclass_task_bin_concepts_dep_3_complete\", train_3, test_3),\n",
    "        (\"multiclass_task_bin_concepts_dep_4_complete\", train_4, test_4),\n",
    "    ],\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCD Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_ccd_encoder(\n",
    "    input_shape,\n",
    "    filter_groups,\n",
    "    units,\n",
    "    latent_dims,\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    latent_act=None,  # Leaving sigmoid as used in original paper\n",
    "):\n",
    "    encoder_inputs = tf.keras.Input(shape=input_shape)\n",
    "    encoder_compute_graph = encoder_inputs\n",
    "    \n",
    "    # Start with our convolutions\n",
    "    num_convs = 0\n",
    "    for filter_group in filter_groups:\n",
    "        for (num_filters, kernel_size) in filter_group:\n",
    "            encoder_compute_graph = tf.keras.layers.Conv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=\"SAME\",\n",
    "                activation=None,\n",
    "                name=f'encoder_conv_{num_convs}',\n",
    "            )(encoder_compute_graph)\n",
    "            num_convs += 1\n",
    "            encoder_compute_graph = tf.keras.layers.BatchNormalization()(\n",
    "                encoder_compute_graph\n",
    "            )\n",
    "            encoder_compute_graph = tf.keras.activations.relu(encoder_compute_graph)\n",
    "        # Then do a max pool here to control the parameter count of the model\n",
    "        # at the end of each group\n",
    "        encoder_compute_graph = tf.keras.layers.MaxPooling2D(\n",
    "            pool_size=max_pool_window,\n",
    "            strides=max_pool_stride,\n",
    "        )(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Flatten this guy\n",
    "    encoder_compute_graph = tf.keras.layers.Flatten()(encoder_compute_graph)\n",
    "    \n",
    "    # Add a dropout if requested\n",
    "    if drop_prob:\n",
    "        encoder_compute_graph = tf.keras.layers.Dropout(drop_prob)(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Finally, include the fully connected bottleneck here\n",
    "    for i, units in enumerate(units):\n",
    "        encoder_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(encoder_compute_graph)\n",
    "    \n",
    "    # TIme to generate the latent code here\n",
    "    encoder_compute_graph = tf.keras.layers.Dense(\n",
    "        latent_dims,\n",
    "        activation=latent_act,\n",
    "        name=\"encoder_bypass_channel\",\n",
    "    )(encoder_compute_graph)\n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        encoder_compute_graph,\n",
    "        name=\"encoder\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_ccd_encoder(\n",
    "    input_shape,\n",
    "    filter_groups,\n",
    "    units,\n",
    "    latent_dims,\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    latent_act=None,  #\"sigmoid\",  # Leaving sigmoid as used in original paper\n",
    "):\n",
    "    encoder_inputs = tf.keras.Input(shape=input_shape)\n",
    "    encoder_compute_graph = encoder_inputs\n",
    "    \n",
    "    # Start with our convolutions\n",
    "    num_convs = 0\n",
    "    for filter_group in filter_groups:\n",
    "        for (num_filters, kernel_size) in filter_group:\n",
    "            encoder_compute_graph = tf.keras.layers.Conv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=\"SAME\",\n",
    "                activation=None,\n",
    "                name=f'encoder_conv_{num_convs}',\n",
    "            )(encoder_compute_graph)\n",
    "            num_convs += 1\n",
    "            encoder_compute_graph = tf.keras.layers.BatchNormalization()(\n",
    "                encoder_compute_graph\n",
    "            )\n",
    "            encoder_compute_graph = tf.keras.activations.relu(encoder_compute_graph)\n",
    "        # Then do a max pool here to control the parameter count of the model\n",
    "        # at the end of each group\n",
    "        encoder_compute_graph = tf.keras.layers.MaxPooling2D(\n",
    "            pool_size=max_pool_window,\n",
    "            strides=max_pool_stride,\n",
    "        )(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Flatten this guy\n",
    "    encoder_compute_graph = tf.keras.layers.Flatten()(encoder_compute_graph)\n",
    "    \n",
    "    # Add a dropout if requested\n",
    "    if drop_prob:\n",
    "        encoder_compute_graph = tf.keras.layers.Dropout(drop_prob)(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Finally, include the fully connected bottleneck here\n",
    "    for i, units in enumerate(units):\n",
    "        encoder_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(encoder_compute_graph)\n",
    "    \n",
    "    # TIme to generate the latent code here\n",
    "    encoder_compute_graph = tf.keras.layers.Dense(\n",
    "        latent_dims,\n",
    "        activation=latent_act,\n",
    "        name=\"encoder_bypass_channel\",\n",
    "    )(encoder_compute_graph)\n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        encoder_compute_graph,\n",
    "        name=\"encoder\",\n",
    "    )\n",
    "\n",
    "def construct_ccd_decoder(units, num_outputs):\n",
    "    decoder_layers = [\n",
    "        tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation=tf.nn.relu,\n",
    "            name=f\"decoder_dense_{i+1}\",\n",
    "        ) for i, units in enumerate(units)\n",
    "    ]\n",
    "    return tf.keras.Sequential(\n",
    "        [tf.keras.layers.Flatten()] +\n",
    "        decoder_layers + [\n",
    "        tf.keras.layers.Dense(\n",
    "            num_outputs if num_outputs > 2 else 1,\n",
    "            activation=None,\n",
    "            name=\"decoder_model_output\",\n",
    "        )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.OCACE.topicModel as CCD\n",
    "import concepts_xai.evaluation.metrics.purity as purity\n",
    "\n",
    "############################################################################\n",
    "## Experiment loop\n",
    "############################################################################\n",
    "\n",
    "def ccd_experiment_loop(\n",
    "    experiment_config,\n",
    "    datasets,\n",
    "    load_from_cache=False,\n",
    "    oracle_matrix_cache=None,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    experiment_variables = dict(\n",
    "        config = experiment_config,\n",
    "        niss=[],\n",
    "    )\n",
    "    num_concepts = experiment_config[\"num_concepts\"]\n",
    "    res_dir = experiment_config[\"niching_results_dir\"]\n",
    "    if load_from_cache:\n",
    "        if os.path.exists(os.path.join(res_dir, 'results_niching.joblib')):\n",
    "            experiment_variables = load(os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    count = 0\n",
    "    start_ind = 0\n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    for (ds_name, (x_train, y_train, c_train), (x_test, y_test, c_test)) in datasets[start_ind:]:\n",
    "        print(\"Training with concepts\", num_concepts, \"in dataset\", ds_name)\n",
    "        \n",
    "        channels_axis = (\n",
    "            -1 if experiment_config.get(\"data_format\", \"channels_last\") == \"channels_last\"\n",
    "            else 1\n",
    "        )\n",
    "        \n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} for {num_concepts} concepts\")\n",
    "            print(\"x_train.shape =\", x_train.shape)\n",
    "            print(\"y_train.shape =\", y_train.shape)\n",
    "            print(\"c_train.shape =\", c_train.shape)\n",
    "            # Proceed to do and end-to-end model in case we want to\n",
    "            # do some task-specific pretraining\n",
    "            end_to_end_model, encoder, decoder = construct_end_to_end_model(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "                learning_rate=experiment_config[\"learning_rate\"],\n",
    "                encoder=construct_ccd_encoder(\n",
    "                    input_shape=experiment_config[\"input_shape\"],\n",
    "                    filter_groups=experiment_config[\"encoder_filter_groups\"],\n",
    "                    latent_dims=experiment_config[\"latent_dims\"],\n",
    "                    units=experiment_config[\"encoder_units\"],\n",
    "                    drop_prob=experiment_config.get(\"drop_prob\", 0.5),\n",
    "                    max_pool_window=experiment_config.get(\"max_pool_window\", (2, 2)),\n",
    "                    max_pool_stride=experiment_config.get(\"max_pool_stride\", (2, 2)),\n",
    "                    latent_act=experiment_config.get(\"latent_act\", None),\n",
    "                ),\n",
    "                decoder=construct_decoder(\n",
    "                    units=experiment_config[\"decoder_units\"],\n",
    "                    num_outputs=experiment_config[\"num_outputs\"],\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            print(\"\\t\\tModel pre-training completed\")\n",
    "            print(\"\\tSerializing model\")\n",
    "            \n",
    "            encoder = load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/{ds_name}_encoder_num_concepts_{num_concepts}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            decoder = load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/{ds_name}_decoder_num_concepts_{num_concepts}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            topic_vector = np.load(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/{ds_name}_topic_vector_num_concepts_{num_concepts}_trial_{trial}.npy\"\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            # Now extract our concept vectors\n",
    "            topic_model = CCD.TopicModel(\n",
    "                concepts_to_labels_model=decoder,\n",
    "                n_channels=experiment_config[\"latent_dims\"],\n",
    "                n_concepts=num_concepts,\n",
    "                threshold=experiment_config.get(\"threshold\", 0.5),\n",
    "                loss_fn=end_to_end_model.loss,\n",
    "                top_k=experiment_config.get(\"top_k\", 32),\n",
    "                lambda1=experiment_config.get(\"lambda1\", 0.1),\n",
    "                lambda2=experiment_config.get(\"lambda2\", 0.1),\n",
    "                seed=experiment_config.get(\"seed\", None),\n",
    "                eps=experiment_config.get(\"eps\", 1e-5),\n",
    "                data_format=experiment_config.get(\n",
    "                    \"data_format\",\n",
    "                    \"channels_last\"\n",
    "                ),\n",
    "                allow_gradient_flow_to_c2l=experiment_config.get(\n",
    "                    'allow_gradient_flow_to_c2l',\n",
    "                    False,\n",
    "                ),\n",
    "                acc_metric=(\n",
    "                    tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "                    if experiment_config[\"num_outputs\"] > 1 else\n",
    "                    tf.keras.metrics.BinaryAccuracy()\n",
    "                ),\n",
    "                g_model=load_model(\n",
    "                    os.path.join(\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                        f\"models/{ds_name}_topic_g_model_num_concepts_{num_concepts}_trial_{trial}\"\n",
    "                    )\n",
    "                ),\n",
    "                initial_topic_vector=topic_vector,\n",
    "            )\n",
    "            \n",
    "            print(f\"\\t\\tComputing purity score...\")\n",
    "            topic_model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(\n",
    "                    experiment_config.get(\"learning_rate\", 1e-3),\n",
    "                )\n",
    "            )\n",
    "            c_train_pred = topic_model.concept_scores(encoder(x_train)).numpy()\n",
    "            c_test_pred = topic_model.concept_scores(encoder(x_test)).numpy()\n",
    "            \n",
    "            \n",
    "            print(\"\\t\\tComputing niching scores...\")\n",
    "            # finding niches\n",
    "            print(\"Topic model evaluation:\", sklearn.metrics.accuracy_score(\n",
    "                np.argmax(y_test, axis=-1),\n",
    "                np.argmax(topic_model(encoder(x_test))[0], axis=-1),\n",
    "            ))\n",
    "            \n",
    "            experiment_variables['niss'].append(\n",
    "                niching.niche_impurity_score(\n",
    "                    c_soft=c_test_pred,\n",
    "                    c_true=c_test,\n",
    "                    c_soft_train=c_train_pred,\n",
    "                    c_true_train=c_train,\n",
    "                )\n",
    "            )\n",
    "            print(f'\\t\\tNIS: {experiment_variables[\"niss\"][-1]:.2f}')\n",
    "            \n",
    "            os.makedirs(res_dir, exist_ok=True)\n",
    "            dump(experiment_variables, os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            \n",
    "    return experiment_variables\n",
    "            \n",
    "\n",
    "def ccd_compute_k(y, batch_size):\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    avg_class_ratio = np.mean(counts) / y.shape[0]\n",
    "    return int((avg_class_ratio * batch_size) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(CBM)\n",
    "reload(CCD)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "ccd_experiment_config = dict(\n",
    "    batch_size=64,\n",
    "    max_epochs=100,\n",
    "    topic_model_train_epochs=50,\n",
    "    num_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    latent_dims=10,\n",
    "    threshold=0.0,\n",
    "    top_k=ccd_compute_k(y=multiclass_task_bin_concepts_dep_0_complete_train[1], batch_size=32),\n",
    "    lambda1=0.1,\n",
    "    lambda2=0.1,\n",
    "    seed=42,\n",
    "    eps=1e-5,\n",
    "    learning_rate=1e-3,\n",
    "    encoder_filter_groups=[\n",
    "        [(8, (7, 7))],\n",
    "        [(16, (5, 5))],\n",
    "        [(32, (3, 3))],\n",
    "        [(64, (3, 3))],\n",
    "    ],\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[64, 64],\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    pax_pool_stride=2,\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"ccd/balanced_multiclass_thresh_0_num_concepts_{balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}\"\n",
    "    ),\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        f\"ccd/balanced_multiclass_thresh_0_num_concepts_{balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}\"\n",
    "    ),\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    holdout_fraction=0.1,\n",
    "    trials=5,\n",
    "    verbosity=0,\n",
    "    early_stop_metric=\"val_loss\",\n",
    "    early_stop_mode=\"min\",\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "num_concepts=multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]\n",
    "ccd_results = ccd_experiment_loop(\n",
    "# encoder, topic_model, x_train, y_train = ccd_experiment_loop(\n",
    "    experiment_config=ccd_experiment_config,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", train_0, test_0),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", train_1, test_1),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", train_2, test_2),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", train_3, test_3),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", train_4, test_4),\n",
    "    ],\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(CBM)\n",
    "reload(CCD)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "ccd_extended_experiment_config = dict(\n",
    "    batch_size=64,\n",
    "    max_epochs=100,\n",
    "    topic_model_train_epochs=50,\n",
    "    num_concepts=2*balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    latent_dims=10,\n",
    "    threshold=0.0,\n",
    "    top_k=ccd_compute_k(y=balanced_multiclass_task_bin_concepts_dep_0_complete_train[1], batch_size=32),\n",
    "    lambda1=0.1,\n",
    "    lambda2=0.1,\n",
    "    seed=42,\n",
    "    eps=1e-5,\n",
    "    learning_rate=1e-3,\n",
    "    encoder_filter_groups=[\n",
    "        [(8, (7, 7))],\n",
    "        [(16, (5, 5))],\n",
    "        [(32, (3, 3))],\n",
    "        [(64, (3, 3))],\n",
    "    ],\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[64, 64],\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    pax_pool_stride=2,\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"ccd/balanced_multiclass_thresh_0_num_concepts_{2*balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}\"\n",
    "    ),\n",
    "    niching_results_dir=os.path.join(\n",
    "        NICHING_RESULTS_DIR,\n",
    "        f\"ccd/balanced_multiclass_thresh_0_num_concepts_{2*balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}\"\n",
    "    ),\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    holdout_fraction=0.1,\n",
    "    trials=5,\n",
    "    verbosity=0,\n",
    "    early_stop_metric=\"val_loss\",\n",
    "    early_stop_mode=\"min\",\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "ccd_extended_results = ccd_experiment_loop(\n",
    "# encoder, topic_model, x_train, y_train = ccd_experiment_loop(\n",
    "    experiment_config=ccd_extended_experiment_config,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", train_0, test_0),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", train_1, test_1),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", train_2, test_2),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", train_3, test_3),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", train_4, test_4),\n",
    "    ],\n",
    "    load_from_cache=FROM_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENN Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.SENN.base_senn as SENN\n",
    "import concepts_xai.methods.SENN.aggregators as aggregators\n",
    "reload(SENN)\n",
    "reload(aggregators)\n",
    "\n",
    "\n",
    "def construct_senn_coefficient_model(units, num_concepts, num_outputs):\n",
    "    decoder_layers = [tf.keras.layers.Flatten()] + [\n",
    "        tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation=tf.nn.relu,\n",
    "            name=f\"coefficient_model_dense_{i+1}\",\n",
    "        ) for i, units in enumerate(units)\n",
    "    ]\n",
    "    return tf.keras.Sequential(decoder_layers + [\n",
    "        tf.keras.layers.Dense(\n",
    "            num_concepts * num_outputs,\n",
    "            activation=None,\n",
    "            name=\"coefficient_model_output\",\n",
    "        ),\n",
    "        tf.keras.layers.Reshape([num_outputs, num_concepts])\n",
    "    ])\n",
    "\n",
    "def construct_senn_encoder(\n",
    "    input_shape,\n",
    "    filter_groups,\n",
    "    units,\n",
    "    latent_dims,\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    include_norm=False,\n",
    "    include_pool=False,\n",
    "):\n",
    "    encoder_inputs = tf.keras.Input(shape=input_shape)\n",
    "    encoder_compute_graph = encoder_inputs\n",
    "    \n",
    "    # Start with our convolutions\n",
    "    num_convs = 0\n",
    "    for filter_group in filter_groups:\n",
    "        for filter_args in filter_group:\n",
    "            if len(filter_args) == 2:\n",
    "                filter_args = (*filter_args, 1)\n",
    "            (num_filters, kernel_size, stride) = filter_args\n",
    "            encoder_compute_graph = tf.keras.layers.Conv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                strides=stride,\n",
    "                padding=\"SAME\",\n",
    "                activation=None if include_norm else \"relu\",\n",
    "                name=f'encoder_conv_{num_convs}',\n",
    "            )(encoder_compute_graph)\n",
    "            num_convs += 1\n",
    "            if include_norm:\n",
    "                encoder_compute_graph = tf.keras.layers.BatchNormalization()(\n",
    "                    encoder_compute_graph\n",
    "                )\n",
    "                encoder_compute_graph = tf.keras.activations.relu(encoder_compute_graph)\n",
    "        if include_pool:\n",
    "            # Then do a max pool here to control the parameter count of the model\n",
    "            # at the end of each group\n",
    "            encoder_compute_graph = tf.keras.layers.MaxPooling2D(\n",
    "                pool_size=max_pool_window,\n",
    "                strides=max_pool_stride,\n",
    "            )(\n",
    "                encoder_compute_graph\n",
    "            )\n",
    "    \n",
    "    # Flatten this guy\n",
    "    encoder_compute_graph = tf.keras.layers.Flatten()(encoder_compute_graph)\n",
    "    \n",
    "    # Add a dropout if requested\n",
    "    if drop_prob:\n",
    "        encoder_compute_graph = tf.keras.layers.Dropout(drop_prob)(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Finally, include the fully connected bottleneck here\n",
    "    for i, units in enumerate(units):\n",
    "        encoder_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(encoder_compute_graph)\n",
    "\n",
    "    mean = tf.keras.layers.Dense(latent_dims, activation=None, name=\"means\")(encoder_compute_graph)\n",
    "    log_var = tf.keras.layers.Dense(latent_dims, activation=None, name=\"log_var\")(encoder_compute_graph)\n",
    "    senn_encoder = tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        mean,\n",
    "        name=\"senn_encoder\",\n",
    "    )\n",
    "    vae_encoder = tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        [mean, log_var],\n",
    "        name=\"vae_encoder\",\n",
    "    )\n",
    "    return senn_encoder, vae_encoder\n",
    "\n",
    "def construct_senn_model(\n",
    "    concept_encoder,\n",
    "    concept_decoder,\n",
    "    coefficient_model,\n",
    "    num_outputs,\n",
    "    regularization_strength=0.1,\n",
    "    learning_rate=1e-3,\n",
    "    sparsity_strength=2e-5,\n",
    "):\n",
    "    def reconstruction_loss_fn(y_true, y_pred):\n",
    "        return vae_losses.bernoulli_fn_wrapper()(y_true, concept_decoder(y_pred))\n",
    "\n",
    "    senn_model = SENN.SelfExplainingNN(\n",
    "        encoder_model=concept_encoder,\n",
    "        coefficient_model=coefficient_model,\n",
    "        aggregator_fn=(\n",
    "            aggregators.multiclass_additive_aggregator if (num_outputs > 2)\n",
    "            else aggregators.scalar_additive_aggregator\n",
    "        ),\n",
    "        task_loss_fn=(\n",
    "            tf.keras.losses.BinaryCrossentropy(from_logits=True) if (num_outputs <= 2)\n",
    "            else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        ),\n",
    "        reconstruction_loss_fn=reconstruction_loss_fn,\n",
    "        regularization_strength=regularization_strength,\n",
    "        sparsity_strength=sparsity_strength,\n",
    "        name=\"SENN\",\n",
    "        metrics=[\n",
    "            tf.keras.metrics.BinaryAccuracy() if (num_outputs <= 2)\n",
    "            else tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "        ],\n",
    "    )\n",
    "    senn_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    )\n",
    "    return senn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.evaluation.metrics.purity as purity\n",
    "\n",
    "############################################################################\n",
    "## Experiment loop\n",
    "############################################################################\n",
    "\n",
    "def construct_trivial_auc_mat(num_concepts):\n",
    "    result = np.ones((num_concepts, num_concepts), dtype=np.float32) * 0.5\n",
    "    return result + np.eye(num_concepts, dtype=np.float32) * 0.5\n",
    "    \n",
    "def get_argmax_concept_explanations(preds, class_theta_scores):\n",
    "    inds = np.argmax(preds, axis=-1)\n",
    "    result = np.take_along_axis(\n",
    "        class_theta_scores,\n",
    "        np.expand_dims(np.expand_dims(inds, axis=-1), axis=-1),\n",
    "        axis=1,\n",
    "    )\n",
    "    return np.squeeze(result, axis=1)\n",
    "\n",
    "def senn_experiment_loop(\n",
    "    experiment_config,\n",
    "    datasets,\n",
    "    load_from_cache=False,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    experiment_variables = dict(\n",
    "        config = experiment_config,\n",
    "        niss=[],\n",
    "    )\n",
    "    num_concepts = experiment_config[\"num_concepts\"]\n",
    "    res_dir = experiment_config['niching_results_dir']\n",
    "    if load_from_cache:\n",
    "        if os.path.exists(os.path.join(res_dir, 'results_niching.joblib')):\n",
    "            experiment_variables = load(os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    count = 0\n",
    "    start_ind = 0\n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    for (ds_name, (x_train, y_train, c_train), (x_test, y_test, c_test)) in datasets[start_ind:]:\n",
    "        print(\"Training with concepts\", num_concepts, \"in dataset\", ds_name)\n",
    "        \n",
    "        channels_axis = (\n",
    "            -1 if experiment_config.get(\"data_format\", \"channels_last\") == \"channels_last\"\n",
    "            else 1\n",
    "        )\n",
    "        \n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} for {num_concepts} concepts\")\n",
    "            concept_encoder = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/concept_encoder_{ds_name}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            concept_decoder = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/concept_decoder_{ds_name}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            coefficient_model = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/coefficient_model_{ds_name}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            senn_model = construct_senn_model(\n",
    "                concept_encoder=concept_encoder,\n",
    "                concept_decoder=concept_decoder,\n",
    "                coefficient_model=coefficient_model,\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "                regularization_strength=experiment_config.get(\"regularization_strength\", 0.1),\n",
    "                learning_rate=experiment_config.get(\"learning_rate\", 1e-3),\n",
    "                sparsity_strength=experiment_config.get(\"sparsity_strength\", 2e-5),\n",
    "            )\n",
    "            \n",
    "            x_train_preds, (_, x_train_theta_class_scores) = senn_model(x_train)\n",
    "            c_train_pred = get_argmax_concept_explanations(\n",
    "                x_train_preds.numpy(),\n",
    "                x_train_theta_class_scores.numpy(),\n",
    "            )\n",
    "            \n",
    "            x_test_preds, (_, x_test_theta_class_scores) = senn_model(x_test)\n",
    "            c_test_pred = get_argmax_concept_explanations(\n",
    "                x_test_preds.numpy(),\n",
    "                x_test_theta_class_scores.numpy(),\n",
    "            )\n",
    "            \n",
    "            print(\"\\t\\tComputing niching scores...\")\n",
    "            experiment_variables['niss'].append(\n",
    "                niching.niche_impurity_score(\n",
    "                    c_soft=c_test_pred,\n",
    "                    c_true=c_test,\n",
    "                    c_soft_train=c_train_pred,\n",
    "                    c_true_train=c_train,\n",
    "                )\n",
    "            )\n",
    "            print(f'\\t\\tNIS: {experiment_variables[\"niss\"][-1]:.2f}')\n",
    "            \n",
    "            os.makedirs(res_dir, exist_ok=True)\n",
    "            dump(experiment_variables, os.path.join(res_dir, 'results_niching.joblib'))\n",
    "            \n",
    "    return experiment_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(SENN)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "senn_experiment_config = dict(\n",
    "    max_epochs=50,\n",
    "    pretrain_autoencoder_epochs=50,\n",
    "    predictor_max_epochs=100,\n",
    "    batch_size=64,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    \n",
    "    num_concepts=(balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]),\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    \n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    encoder_units=[64, 64],\n",
    "    include_pool=True,\n",
    "    decoder_units=[256, 512],\n",
    "    coefficient_model_units=[64, 64],\n",
    "    latent_decoder_units=[64, 64],\n",
    "    drop_prob=0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    \n",
    "    \n",
    "    regularization_strength=0.1,\n",
    "    sparsity_strength=2e-5,\n",
    "    \n",
    "    concept_cardinality=[2 for _ in range(balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1])],\n",
    "    results_dir=os.path.join(RESULTS_DIR, \"senn/dependency_multiclass\"),\n",
    "    niching_results_dir=os.path.join(NICHING_RESULTS_DIR, \"senn/dependency_multiclass\"),\n",
    "    verbosity=0,\n",
    "\n",
    "    holdout_fraction=0.1,\n",
    "    patience=float(\"inf\"),\n",
    "    early_stop_metric=\"val_loss\",\n",
    "    early_stop_mode=\"min\",\n",
    "    min_delta=1e-5,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(senn_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "senn_figure_dir = os.path.join(senn_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(senn_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "senn_results = senn_experiment_loop(\n",
    "    senn_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", train_0, test_0),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", train_1, test_1),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", train_2, test_2),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", train_3, test_3),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", train_4, test_4),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(SENN)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "senn_extended_experiment_config = dict(\n",
    "    max_epochs=50,\n",
    "    pretrain_autoencoder_epochs=50,\n",
    "    predictor_max_epochs=100,\n",
    "    batch_size=64,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    \n",
    "    num_concepts=(2 * balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]),\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    \n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "#     encoder_units=[128, 64, 64, 64, 32],\n",
    "    encoder_units=[64, 64],\n",
    "    include_pool=True,\n",
    "    decoder_units=[256, 512],\n",
    "    coefficient_model_units=[64, 64],\n",
    "    latent_decoder_units=[64, 64],\n",
    "    drop_prob=0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    \n",
    "    \n",
    "    regularization_strength=0.1,\n",
    "    sparsity_strength=2e-5,\n",
    "    \n",
    "    results_dir=os.path.join(RESULTS_DIR, \"senn/dependency_multiclass_extended\"),\n",
    "    niching_results_dir=os.path.join(NICHING_RESULTS_DIR, \"senn/dependency_multiclass_extended\"),\n",
    "    verbosity=0,\n",
    "\n",
    "    holdout_fraction=0.1,\n",
    "    patience=float(\"inf\"),\n",
    "    early_stop_metric=\"val_loss\",\n",
    "    early_stop_mode=\"min\",\n",
    "    min_delta=1e-5,\n",
    "    delta_beta=0.05,\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(senn_extended_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "senn_extended_figure_dir = os.path.join(senn_extended_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(senn_extended_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "senn_extended_results = senn_experiment_loop(\n",
    "    senn_extended_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", train_0, test_0),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", train_1, test_1),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", train_2, test_2),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", train_3, test_3),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", train_4, test_4),\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "purity_dsprites.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
