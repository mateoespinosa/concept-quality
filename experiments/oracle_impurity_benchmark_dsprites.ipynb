{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dIcmOacShy-p",
    "outputId": "e47e6328-6477-4d52-d914-4dbf25f7fb6b"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xD3cPbsJhy-r"
   },
   "source": [
    "# Purity dSprites Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwclGfwnhy-s"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYPJQZB-hy-t"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import concepts_xai\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import rc\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "import scipy\n",
    "import utils\n",
    "import model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktrTSqfchy-u"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "## Set seeds up for reproducibility\n",
    "################################################################################\n",
    "\n",
    "utils.reseed(87)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "## Global Variables Defining Experiment Flow\n",
    "################################################################################\n",
    "\n",
    "_LATEX_SYMBOL = \"$\"\n",
    "RESULTS_DIR = \"results/dsprites\"\n",
    "DATASETS_DIR = os.path.join(\"results/dsprites\", \"datasets/\")\n",
    "Path(DATASETS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "rc('text', usetex=(_LATEX_SYMBOL == \"$\"))\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold_text(x):\n",
    "    if _LATEX_SYMBOL == \"$\":\n",
    "        return r\"$\\textbf{\" + x + \"}$\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRdZhyY9hy-v"
   },
   "source": [
    "# Graph Dependency Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.datasets.dSprites as dsprites\n",
    "import concepts_xai.datasets.latentFactorData as latentFactorData\n",
    "\n",
    "def generate_dsprites_dataset(\n",
    "    label_fn,\n",
    "    filter_fn=None,\n",
    "    dataset_path=None,\n",
    "    concept_map_fn=lambda x: x,\n",
    "    sample_map_fn=lambda x: x,\n",
    "    dsprites_path=\"dsprites/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz\",\n",
    "    force_reload=False,\n",
    "):\n",
    "    if (not force_reload) and dataset_path and os.path.exists(dataset_path):\n",
    "        # Them time to load up this dataset!\n",
    "        ds = np.load(dataset_path)\n",
    "        return (\n",
    "            (ds[\"x_train\"], ds[\"y_train\"], ds[\"c_train\"]),\n",
    "            (ds[\"x_test\"], ds[\"y_test\"], ds[\"c_test\"])\n",
    "        )\n",
    "    \n",
    "    def _task_fn(x_data, c_data):\n",
    "        return latentFactorData.get_task_data(\n",
    "            x_data=x_data,\n",
    "            c_data=c_data,\n",
    "            label_fn=label_fn,\n",
    "            filter_fn=filter_fn,\n",
    "        )\n",
    "\n",
    "    loaded_dataset = dsprites.dSprites(\n",
    "        dataset_path=dsprites_path,\n",
    "        train_size=0.8,\n",
    "        random_state=42,\n",
    "        task=_task_fn,\n",
    "    )\n",
    "    _, _, _ = loaded_dataset.load_data()\n",
    "\n",
    "    x_train = sample_map_fn(loaded_dataset.x_train)\n",
    "    y_train = loaded_dataset.y_train\n",
    "    c_train = concept_map_fn(loaded_dataset.c_train)\n",
    "    \n",
    "    x_test = sample_map_fn(loaded_dataset.x_test)\n",
    "    y_test = loaded_dataset.y_test\n",
    "    c_test = concept_map_fn(loaded_dataset.c_test)\n",
    "    \n",
    "    if dataset_path:\n",
    "        # Then serialize it to speed up things next time\n",
    "        np.savez(\n",
    "            dataset_path,\n",
    "            x_train=x_train,\n",
    "            y_train=y_train,\n",
    "            c_train=c_train,\n",
    "            x_test=x_test,\n",
    "            y_test=y_test,\n",
    "            c_test=c_test,\n",
    "        )\n",
    "    return (x_train, y_train, c_train), (x_test, y_test, c_test),\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Task Dataset Construction (Dependent Binary Concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_class_balance(y):\n",
    "    one_hot = tf.keras.utils.to_categorical(y)\n",
    "    return np.sum(one_hot, axis=0) / one_hot.shape[0]\n",
    "\n",
    "def multiclass_binary_concepts_map_fn(concepts):\n",
    "    new_concepts = np.zeros((concepts.shape[0], 5))\n",
    "    # We will have 5 concepts:\n",
    "    # (0) \"is it ellipse or square?\"\n",
    "    new_concepts[:, 0] = (concepts[:, 0] < 2).astype(np.int)\n",
    "\n",
    "    # (1) \"is_size < 3?\"\n",
    "    num_sizes = len(set(concepts[:, 1]))\n",
    "    new_concepts[:, 1] = (concepts[:, 1] < num_sizes/2).astype(np.int)\n",
    "\n",
    "    # (2) \"is rotation < PI/2?\"\n",
    "    num_rots = len(set(concepts[:, 2]))\n",
    "    new_concepts[:, 2] = (concepts[:, 2] < num_rots/2).astype(np.int)\n",
    "\n",
    "    # (3) \"is x <= 16?\"\n",
    "    num_x_coords = len(set(concepts[:, 3]))\n",
    "    new_concepts[:, 3] = (concepts[:, 3] < num_x_coords // 2).astype(np.int)\n",
    "\n",
    "    # (4) \"is y <= 16?\"\n",
    "    num_y_coords = len(set(concepts[:, 4]))\n",
    "    new_concepts[:, 4] = (concepts[:, 4] < num_y_coords // 2).astype(np.int)\n",
    "    \n",
    "    return new_concepts\n",
    "\n",
    "def _get_concept_vector(c_data):\n",
    "    return np.array([\n",
    "        # First check if it is an ellipse or a square\n",
    "        int(c_data[0] < 2),\n",
    "        # Now check that it is \"small\"\n",
    "        int(c_data[1] < 3),\n",
    "        # And it has not been rotated more than PI/2 radians\n",
    "        int(c_data[2] < 20),\n",
    "        # Finally, check whether it is in not in the the upper-left quadrant\n",
    "        int(c_data[3] < 15),\n",
    "        int(c_data[4] < 15),\n",
    "    ])\n",
    "\n",
    "def multiclass_task_label_fn(c_data):\n",
    "    # Our task will be a binary task where we are interested in determining\n",
    "    # whether an image is a \"small\" ellipse not in the upper-left\n",
    "    # quadrant that has been rotated less than 3*PI/2 radians\n",
    "    concept_vector = _get_concept_vector(c_data)\n",
    "    binary_label_encoding = [\n",
    "        concept_vector[0] or concept_vector[1],\n",
    "        concept_vector[2] or concept_vector[3],\n",
    "        concept_vector[4],\n",
    "    ]\n",
    "    return int(\n",
    "        \"\".join(list(map(str, binary_label_encoding))),\n",
    "        2\n",
    "    )\n",
    "\n",
    "def dep_0_filter_fn(concept):\n",
    "    ranges = [\n",
    "        list(range(3)),\n",
    "        list(range(0, 6, 2)),\n",
    "        list(range(0, 40, 4)),\n",
    "        list(range(0, 32, 2)),\n",
    "        list(range(0, 32, 2)),\n",
    "    ]\n",
    "    return all([\n",
    "        (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "scale_shape_sets_lower = [\n",
    "    list(np.random.permutation(4))[:3] for i in range(3)\n",
    "]\n",
    "\n",
    "scale_shape_sets_upper = [\n",
    "    list(2 + np.random.permutation(4))[:3] for i in range(3)\n",
    "]\n",
    "def dep_1_filter_fn(concept):\n",
    "    ranges = [\n",
    "        list(range(3)),\n",
    "        list(range(6)),\n",
    "        list(range(0, 40, 4)),\n",
    "        list(range(0, 32, 2)),\n",
    "        list(range(0, 32, 2)),\n",
    "    ]\n",
    "    \n",
    "    concept_vector = _get_concept_vector(concept)\n",
    "\n",
    "    # First filter as in small dataset to constraint the size of the data a bit\n",
    "    if not all([\n",
    "        (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "    ]):\n",
    "        return False\n",
    "    if concept_vector[0]:\n",
    "        if concept[1] not in scale_shape_sets_lower[concept[0]]:\n",
    "            return False\n",
    "    else:\n",
    "        if concept[0] not in scale_shape_sets_upper[concept[0]]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "rotation_scale_sets_lower = [\n",
    "    list(np.random.permutation(30))[:20] for i in range(6)\n",
    "]\n",
    "\n",
    "rotation_scale_sets_upper = [\n",
    "    list(10 + np.random.permutation(30))[:20] for i in range(6)\n",
    "]\n",
    "def dep_2_filter_fn(concept):\n",
    "    ranges = [\n",
    "        list(range(3)),\n",
    "        list(range(6)),\n",
    "        list(range(0, 40, 2)),\n",
    "        list(range(0, 32, 2)),\n",
    "        list(range(0, 32, 2)),\n",
    "    ]\n",
    "    \n",
    "    concept_vector = _get_concept_vector(concept)\n",
    "\n",
    "    # First filter as in small dataset to constraint the size of the data a bit\n",
    "    if not all([\n",
    "        (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "    ]):\n",
    "        return False\n",
    "    if concept_vector[0]:\n",
    "        if concept[1] not in scale_shape_sets_lower[concept[0]]:\n",
    "            return False\n",
    "    else:\n",
    "        if concept[0] not in scale_shape_sets_upper[concept[0]]:\n",
    "            return False\n",
    "    \n",
    "    if concept_vector[1]:\n",
    "        if concept[2] not in rotation_scale_sets_lower[concept[1]]:\n",
    "            return False\n",
    "    else:\n",
    "        if concept[2] not in rotation_scale_sets_upper[concept[1]]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "x_pos_rotation_sets_lower = [\n",
    "    list(np.random.permutation(20))[:16]\n",
    "    for i in range(40)\n",
    "]\n",
    "\n",
    "x_pos_rotation_sets_upper = [\n",
    "    list(12 + np.random.permutation(20))[:16]\n",
    "    for i in range(40)\n",
    "]\n",
    "def dep_3_filter_fn(concept):\n",
    "    ranges = [\n",
    "        list(range(3)),\n",
    "        list(range(6)),\n",
    "        list(range(0, 40, 2)),\n",
    "        list(range(0, 32)),\n",
    "        list(range(0, 32, 2)),\n",
    "    ]\n",
    "    \n",
    "    concept_vector = _get_concept_vector(concept)\n",
    "\n",
    "    # First filter as in small dataset to constraint the size of the data a bit\n",
    "    if not all([\n",
    "        (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "    ]):\n",
    "        return False\n",
    "    if concept_vector[0]:\n",
    "        if concept[1] not in scale_shape_sets_lower[concept[0]]:\n",
    "            return False\n",
    "    else:\n",
    "        if concept[0] not in scale_shape_sets_upper[concept[0]]:\n",
    "            return False\n",
    "    \n",
    "    if concept_vector[1]:\n",
    "        if concept[2] not in rotation_scale_sets_lower[concept[1]]:\n",
    "            return False\n",
    "    else:\n",
    "        if concept[2] not in rotation_scale_sets_upper[concept[1]]:\n",
    "            return False\n",
    "        \n",
    "    if concept_vector[2]:\n",
    "        if concept[3] not in x_pos_rotation_sets_lower[concept[2]]:\n",
    "            return False\n",
    "    else:\n",
    "        if concept[3] not in x_pos_rotation_sets_upper[concept[2]]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "y_pos_x_pos_sets_lower = [\n",
    "    list(np.random.permutation(20))[:16]\n",
    "    for i in range(32)\n",
    "]\n",
    "\n",
    "y_pos_x_pos_sets_upper = [\n",
    "    list(12 + np.random.permutation(20))[:16]\n",
    "    for i in range(32)\n",
    "]\n",
    "def dep_4_filter_fn(concept):\n",
    "    ranges = [\n",
    "        list(range(3)),\n",
    "        list(range(6)),\n",
    "        list(range(0, 40, 2)),\n",
    "        list(range(0, 32)),\n",
    "        list(range(0, 32)),\n",
    "    ]\n",
    "    \n",
    "    concept_vector = _get_concept_vector(concept)\n",
    "\n",
    "    # First filter as in small dataset to constraint the size of the data a bit\n",
    "    if not all([\n",
    "        (concept[i] in ranges[i]) for i in range(len(ranges))\n",
    "    ]):\n",
    "        return False\n",
    "    if concept_vector[0]:\n",
    "        if concept[1] not in scale_shape_sets_lower[concept[0]]:\n",
    "            return False\n",
    "    else:\n",
    "        if concept[0] not in scale_shape_sets_upper[concept[0]]:\n",
    "            return False\n",
    "    \n",
    "    if concept_vector[1]:\n",
    "        if concept[2] not in rotation_scale_sets_lower[concept[1]]:\n",
    "            return False\n",
    "    else:\n",
    "        if concept[2] not in rotation_scale_sets_upper[concept[1]]:\n",
    "            return False\n",
    "        \n",
    "    if concept_vector[2]:\n",
    "        if concept[3] not in x_pos_rotation_sets_lower[concept[2]]:\n",
    "            return False\n",
    "    else:\n",
    "        if concept[3] not in x_pos_rotation_sets_upper[concept[2]]:\n",
    "            return False\n",
    "    \n",
    "    if concept_vector[3]:\n",
    "        if concept[4] not in y_pos_x_pos_sets_lower[concept[3]]:\n",
    "            return False\n",
    "    else:\n",
    "        if concept[4] not in y_pos_x_pos_sets_upper[concept[3]]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_multiclass_task_label_fn(c_data):\n",
    "    # Our task will be a binary task where we are interested in determining\n",
    "    # whether an image is a \"small\" ellipse not in the upper-left\n",
    "    # quadrant that has been rotated less than 3*PI/2 radians\n",
    "    concept_vector = _get_concept_vector(c_data)\n",
    "    threshold = 0\n",
    "    if concept_vector[0] == 1:\n",
    "        binary_label_encoding = [\n",
    "            concept_vector[1],\n",
    "            concept_vector[2],\n",
    "        ]\n",
    "    else:\n",
    "        threshold = 4\n",
    "        binary_label_encoding = [\n",
    "            concept_vector[3],\n",
    "            concept_vector[4],\n",
    "        ]\n",
    "    return threshold + int(\n",
    "        \"\".join(list(map(str, binary_label_encoding))),\n",
    "        2\n",
    "    )\n",
    "\n",
    "balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test = generate_dsprites_dataset(\n",
    "    label_fn=balanced_multiclass_task_label_fn,\n",
    "    filter_fn=dep_0_filter_fn,\n",
    "    dataset_path=os.path.join(DATASETS_DIR, \"balanced_multiclass_task_bin_concepts_dep_0_complete_dataset.npz\"),\n",
    "    dsprites_path=\"dsprites/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz\",\n",
    "    concept_map_fn=multiclass_binary_concepts_map_fn,\n",
    "#     force_reload=True,\n",
    ")\n",
    "\n",
    "dep_0_corr_mat = np.ones(\n",
    "    (\n",
    "        balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1]))\n",
    "    )\n",
    ")\n",
    "for c in range(dep_0_corr_mat.shape[0]):\n",
    "    for l in range(dep_0_corr_mat.shape[1]):\n",
    "        dep_0_corr_mat[c][l] = np.corrcoef(\n",
    "            balanced_multiclass_task_bin_concepts_dep_0_complete_train[2][:, c],\n",
    "            (balanced_multiclass_task_bin_concepts_dep_0_complete_train[1] == l).astype(np.int32),\n",
    "        )[0, 1]\n",
    "fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "im, cbar = utils.heatmap(\n",
    "    np.abs(dep_0_corr_mat),\n",
    "    [f\"$c_{i}$\" for i in range(dep_0_corr_mat.shape[0])],\n",
    "    [f\"$l_{i}$\" for i in range(dep_0_corr_mat.shape[1])],\n",
    "    ax=ax,\n",
    "    cmap=\"magma\",\n",
    "    cbarlabel=f\"Correlation Coef\",\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    ")\n",
    "texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.suptitle(f\"dSprites Concept-Label Absolute Correlations ($\\lambda = 0$)\", fontsize=25)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "plt.show()\n",
    "\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_0_complete_dataset train size:\", balanced_multiclass_task_bin_concepts_dep_0_complete_train[0].shape[0])\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_0_complete_dataset train concept size:\", balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape)\n",
    "print(\"\\tTrain balance:\", count_class_balance(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1]))\n",
    "print(\"\\tConcept balance:\", np.sum(balanced_multiclass_task_bin_concepts_dep_0_complete_train[2], axis=0)/balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[0])\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test = generate_dsprites_dataset(\n",
    "    label_fn=balanced_multiclass_task_label_fn,\n",
    "    filter_fn=dep_1_filter_fn,\n",
    "    dataset_path=os.path.join(DATASETS_DIR, \"balanced_multiclass_task_bin_concepts_dep_1_complete_dataset.npz\"),\n",
    "    dsprites_path=\"dsprites/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz\",\n",
    "    concept_map_fn=multiclass_binary_concepts_map_fn,\n",
    "#     force_reload=True,\n",
    ")\n",
    "\n",
    "dep_1_corr_mat = np.ones(\n",
    "    (\n",
    "        balanced_multiclass_task_bin_concepts_dep_1_complete_train[2].shape[-1],\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_1_complete_train[1]))\n",
    "    )\n",
    ")\n",
    "for c in range(dep_1_corr_mat.shape[0]):\n",
    "    for l in range(dep_1_corr_mat.shape[1]):\n",
    "        dep_1_corr_mat[c][l] = np.corrcoef(\n",
    "            balanced_multiclass_task_bin_concepts_dep_1_complete_train[2][:, c],\n",
    "            (balanced_multiclass_task_bin_concepts_dep_1_complete_train[1] == l).astype(np.int32),\n",
    "        )[0, 1]\n",
    "fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "im, cbar = utils.heatmap(\n",
    "    np.abs(dep_1_corr_mat),\n",
    "    [f\"$c_{i}$\" for i in range(dep_1_corr_mat.shape[0])],\n",
    "    [f\"$l_{i}$\" for i in range(dep_1_corr_mat.shape[1])],\n",
    "    ax=ax,\n",
    "    cmap=\"magma\",\n",
    "    cbarlabel=f\"Correlation Coef\",\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    ")\n",
    "texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.suptitle(f\"dSprites Concept-Label Absolute Correlations ($\\lambda = 1$)\", fontsize=25)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_1_complete_dataset train size:\", balanced_multiclass_task_bin_concepts_dep_1_complete_train[0].shape[0])\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_1_complete_dataset train concept size:\", balanced_multiclass_task_bin_concepts_dep_1_complete_train[2].shape)\n",
    "print(\"\\tTrain balance:\", count_class_balance(balanced_multiclass_task_bin_concepts_dep_1_complete_train[1]))\n",
    "print(\"\\tConcept balance:\", np.sum(balanced_multiclass_task_bin_concepts_dep_1_complete_train[2], axis=0)/balanced_multiclass_task_bin_concepts_dep_1_complete_train[2].shape[0])\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test = generate_dsprites_dataset(\n",
    "    label_fn=balanced_multiclass_task_label_fn,\n",
    "    filter_fn=dep_2_filter_fn,\n",
    "    dataset_path=os.path.join(DATASETS_DIR, \"balanced_multiclass_task_bin_concepts_dep_2_complete_dataset.npz\"),\n",
    "    dsprites_path=\"dsprites/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz\",\n",
    "    concept_map_fn=multiclass_binary_concepts_map_fn,\n",
    "#     force_reload=True,\n",
    ")\n",
    "\n",
    "dep_2_corr_mat = np.ones(\n",
    "    (\n",
    "        balanced_multiclass_task_bin_concepts_dep_2_complete_train[2].shape[-1],\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_2_complete_train[1]))\n",
    "    )\n",
    ")\n",
    "for c in range(dep_2_corr_mat.shape[0]):\n",
    "    for l in range(dep_2_corr_mat.shape[1]):\n",
    "        dep_2_corr_mat[c][l] = np.corrcoef(\n",
    "            balanced_multiclass_task_bin_concepts_dep_2_complete_train[2][:, c],\n",
    "            (balanced_multiclass_task_bin_concepts_dep_2_complete_train[1] == l).astype(np.int32),\n",
    "        )[0, 1]\n",
    "fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "im, cbar = utils.heatmap(\n",
    "    np.abs(dep_2_corr_mat),\n",
    "    [f\"$c_{i}$\" for i in range(dep_2_corr_mat.shape[0])],\n",
    "    [f\"$l_{i}$\" for i in range(dep_2_corr_mat.shape[1])],\n",
    "    ax=ax,\n",
    "    cmap=\"magma\",\n",
    "    cbarlabel=f\"Correlation Coef\",\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    ")\n",
    "texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.suptitle(f\"dSprites Concept-Label Absolute Correlations ($\\lambda = 2$)\", fontsize=25)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_2_complete_dataset train size:\", balanced_multiclass_task_bin_concepts_dep_2_complete_train[0].shape[0])\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_2_complete_dataset train concept size:\", balanced_multiclass_task_bin_concepts_dep_2_complete_train[2].shape)\n",
    "print(\"\\tTrain balance:\", count_class_balance(balanced_multiclass_task_bin_concepts_dep_2_complete_train[1]))\n",
    "print(\"\\tConcept balance:\", np.sum(balanced_multiclass_task_bin_concepts_dep_2_complete_train[2], axis=0)/balanced_multiclass_task_bin_concepts_dep_2_complete_train[2].shape[0])\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test = generate_dsprites_dataset(\n",
    "    label_fn=balanced_multiclass_task_label_fn,\n",
    "    filter_fn=dep_3_filter_fn,\n",
    "    dataset_path=os.path.join(DATASETS_DIR, \"balanced_multiclass_task_bin_concepts_dep_3_complete_dataset.npz\"),\n",
    "    dsprites_path=\"dsprites/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz\",\n",
    "    concept_map_fn=multiclass_binary_concepts_map_fn,\n",
    "#     force_reload=True,\n",
    ")\n",
    "\n",
    "dep_3_corr_mat = np.ones(\n",
    "    (\n",
    "        balanced_multiclass_task_bin_concepts_dep_3_complete_train[2].shape[-1],\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_3_complete_train[1]))\n",
    "    )\n",
    ")\n",
    "for c in range(dep_3_corr_mat.shape[0]):\n",
    "    for l in range(dep_3_corr_mat.shape[1]):\n",
    "        dep_3_corr_mat[c][l] = np.corrcoef(\n",
    "            balanced_multiclass_task_bin_concepts_dep_3_complete_train[2][:, c],\n",
    "            (balanced_multiclass_task_bin_concepts_dep_3_complete_train[1] == l).astype(np.int32),\n",
    "        )[0, 1]\n",
    "fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "im, cbar = utils.heatmap(\n",
    "    np.abs(dep_3_corr_mat),\n",
    "    [f\"$c_{i}$\" for i in range(dep_3_corr_mat.shape[0])],\n",
    "    [f\"$l_{i}$\" for i in range(dep_3_corr_mat.shape[1])],\n",
    "    ax=ax,\n",
    "    cmap=\"magma\",\n",
    "    cbarlabel=f\"Correlation Coef\",\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    ")\n",
    "texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.suptitle(f\"dSprites Concept-Label Absolute Correlations ($\\lambda = 3$)\", fontsize=25)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "plt.show()\n",
    "\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_3_complete_dataset train size:\", balanced_multiclass_task_bin_concepts_dep_3_complete_train[0].shape[0])\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_3_complete_dataset train concept size:\", balanced_multiclass_task_bin_concepts_dep_3_complete_train[2].shape)\n",
    "print(\"\\tTrain balance:\", count_class_balance(balanced_multiclass_task_bin_concepts_dep_3_complete_train[1]))\n",
    "print(\"\\tConcept balance:\", np.sum(balanced_multiclass_task_bin_concepts_dep_3_complete_train[2], axis=0)/balanced_multiclass_task_bin_concepts_dep_3_complete_train[2].shape[0])\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test = generate_dsprites_dataset(\n",
    "    label_fn=balanced_multiclass_task_label_fn,\n",
    "    filter_fn=dep_4_filter_fn,\n",
    "    dataset_path=os.path.join(DATASETS_DIR, \"balanced_multiclass_task_bin_concepts_dep_4_complete_dataset.npz\"),\n",
    "    dsprites_path=\"dsprites/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz\",\n",
    "    concept_map_fn=multiclass_binary_concepts_map_fn,\n",
    "#     force_reload=True,\n",
    ")\n",
    "\n",
    "dep_4_corr_mat = np.ones(\n",
    "    (\n",
    "        balanced_multiclass_task_bin_concepts_dep_4_complete_train[2].shape[-1],\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_4_complete_train[1]))\n",
    "    )\n",
    ")\n",
    "for c in range(dep_4_corr_mat.shape[0]):\n",
    "    for l in range(dep_4_corr_mat.shape[1]):\n",
    "        dep_4_corr_mat[c][l] = np.corrcoef(\n",
    "            balanced_multiclass_task_bin_concepts_dep_4_complete_train[2][:, c],\n",
    "            (balanced_multiclass_task_bin_concepts_dep_4_complete_train[1] == l).astype(np.int32),\n",
    "        )[0, 1]\n",
    "fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "im, cbar = utils.heatmap(\n",
    "    np.abs(dep_4_corr_mat),\n",
    "    [f\"$c_{i}$\" for i in range(dep_4_corr_mat.shape[0])],\n",
    "    [f\"$l_{i}$\" for i in range(dep_4_corr_mat.shape[1])],\n",
    "    ax=ax,\n",
    "    cmap=\"magma\",\n",
    "    cbarlabel=f\"Correlation Coef\",\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    ")\n",
    "texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.suptitle(f\"dSprites Concept-Label Absolute Correlations ($\\lambda = 4$)\", fontsize=25)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "plt.show()\n",
    "\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_4_complete_dataset train size:\", balanced_multiclass_task_bin_concepts_dep_4_complete_train[0].shape[0])\n",
    "print(\"balanced_multiclass_task_bin_concepts_dep_4_complete_dataset train concept size:\", balanced_multiclass_task_bin_concepts_dep_4_complete_train[2].shape)\n",
    "print(\"\\tTrain balance:\", count_class_balance(balanced_multiclass_task_bin_concepts_dep_4_complete_train[1]))\n",
    "print(\"\\tConcept balance:\", np.sum(balanced_multiclass_task_bin_concepts_dep_4_complete_train[2], axis=0)/balanced_multiclass_task_bin_concepts_dep_4_complete_train[2].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wciMdtP9hy-1"
   },
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ey3yb8HIhy-2",
    "outputId": "08bc7f0b-146f-48e5-8f9f-40b608e09cb3"
   },
   "outputs": [],
   "source": [
    "# Construct the encoder model\n",
    "def _extract_concepts(activations, concept_cardinality):\n",
    "    concepts = []\n",
    "    total_seen = 0\n",
    "    if all(np.array(concept_cardinality) <= 1):\n",
    "        # Then nothing to do here as they are all binary concepts\n",
    "        return activations\n",
    "    for num_values in concept_cardinality:\n",
    "        concepts.append(activations[:, total_seen: total_seen + num_values])\n",
    "        total_seen += num_values\n",
    "    return concepts\n",
    "    \n",
    "def construct_encoder(\n",
    "    input_shape,\n",
    "    filter_groups,\n",
    "    units,\n",
    "    concept_cardinality,\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    latent_dims=0,\n",
    "    output_logits=False,\n",
    "):\n",
    "    encoder_inputs = tf.keras.Input(shape=input_shape)\n",
    "    encoder_compute_graph = encoder_inputs\n",
    "    \n",
    "    # Start with our convolutions\n",
    "    num_convs = 0\n",
    "    for filter_group in filter_groups:\n",
    "        for (num_filters, kernel_size) in filter_group:\n",
    "            encoder_compute_graph = tf.keras.layers.Conv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=\"SAME\",\n",
    "                activation=None,\n",
    "                name=f'encoder_conv_{num_convs}',\n",
    "            )(encoder_compute_graph)\n",
    "            num_convs += 1\n",
    "            encoder_compute_graph = tf.keras.layers.BatchNormalization()(\n",
    "                encoder_compute_graph\n",
    "            )\n",
    "            encoder_compute_graph = tf.keras.activations.relu(encoder_compute_graph)\n",
    "        # Then do a max pool here to control the parameter count of the model\n",
    "        # at the end of each group\n",
    "        encoder_compute_graph = tf.keras.layers.MaxPooling2D(\n",
    "            pool_size=max_pool_window,\n",
    "            strides=max_pool_stride,\n",
    "        )(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Flatten this guy\n",
    "    encoder_compute_graph = tf.keras.layers.Flatten()(encoder_compute_graph)\n",
    "    \n",
    "    # Add a dropout if requested\n",
    "    if drop_prob:\n",
    "        encoder_compute_graph = tf.keras.layers.Dropout(drop_prob)(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Finally, include the fully connected bottleneck here\n",
    "    for i, units in enumerate(units):\n",
    "        encoder_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(encoder_compute_graph)\n",
    "    \n",
    "    if latent_dims:\n",
    "        bypass = tf.keras.layers.Dense(\n",
    "            latent_dims,\n",
    "            activation=\"sigmoid\",\n",
    "            name=\"encoder_bypass_channel\",\n",
    "        )(encoder_compute_graph)\n",
    "    else:\n",
    "        bypass = None\n",
    "    \n",
    "    # Map to our output distribution to a flattened\n",
    "    # vector where we will extract distributions over\n",
    "    # all concept values\n",
    "    encoder_compute_graph = tf.keras.layers.Dense(\n",
    "        sum(concept_cardinality),\n",
    "        activation=None,\n",
    "        name=\"encoder_concept_outputs\",\n",
    "    )(encoder_compute_graph)\n",
    "        \n",
    "    # Separate this vector into all of its heads\n",
    "    concept_outputs = _extract_concepts(\n",
    "        encoder_compute_graph,\n",
    "        concept_cardinality,\n",
    "    )\n",
    "    if not output_logits:\n",
    "        if isinstance(concept_outputs, list):\n",
    "            for i, concept_vec in enumerate(concept_outputs):\n",
    "                if concept_vec.shape[-1] == 1:\n",
    "                    # Then this is a binary concept so simply apply sigmoid\n",
    "                    concept_outputs[i] = tf.keras.activations.sigmoid(concept_vec)\n",
    "                else:\n",
    "                    # Else we will apply a softmax layer as we assume that all of these\n",
    "                    # entries represent a multi-modal probability distribution\n",
    "                    concept_outputs[i] = tf.keras.activations.softmax(\n",
    "                        concept_vec,\n",
    "                        axis=-1,\n",
    "                    )\n",
    "        else:\n",
    "            # Else they are allbinary concepts so let's sigmoid them\n",
    "            concept_outputs = tf.keras.activations.sigmoid(concept_outputs)\n",
    "    return tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        [concept_outputs, bypass] if bypass is not None else concept_outputs,\n",
    "        name=\"encoder\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7lu1IS3hy-4"
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## Build concepts-to-labels model\n",
    "############################################################################\n",
    "\n",
    "def construct_decoder(units, num_outputs):\n",
    "    decoder_layers = [tf.keras.layers.Flatten()] + [\n",
    "        tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation=tf.nn.relu,\n",
    "            name=f\"decoder_dense_{i+1}\",\n",
    "        ) for i, units in enumerate(units)\n",
    "    ]\n",
    "    return tf.keras.Sequential(decoder_layers + [\n",
    "        tf.keras.layers.Dense(\n",
    "            num_outputs if num_outputs > 2 else 1,\n",
    "            activation=None,\n",
    "            name=\"decoder_model_output\",\n",
    "        )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjd9d2sDhy-4",
    "outputId": "f8a8faed-079a-475b-d5be-adad132a272d"
   },
   "outputs": [],
   "source": [
    "# Construct the complete model\n",
    "def construct_end_to_end_model(\n",
    "    input_shape,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    num_outputs,\n",
    "    learning_rate=1e-3,\n",
    "):\n",
    "    model_inputs = tf.keras.Input(shape=input_shape)\n",
    "    latent = encoder(model_inputs)\n",
    "    if isinstance(latent, list):\n",
    "        if len(latent) > 1:\n",
    "            compacted_vector = tf.keras.layers.Concatenate(axis=-1)(\n",
    "                latent\n",
    "            )\n",
    "        else:\n",
    "            compacted_vector = latent[0]\n",
    "    else:\n",
    "        compacted_vector = latent\n",
    "    model_compute_graph = decoder(compacted_vector)\n",
    "    # Now time to collapse all the concepts again back into a single vector\n",
    "    model = tf.keras.Model(\n",
    "        model_inputs,\n",
    "        model_compute_graph,\n",
    "        name=\"complete_model\",\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "        loss=(\n",
    "            tf.keras.losses.BinaryCrossentropy(from_logits=True) if (num_outputs <= 2)\n",
    "            else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        ),\n",
    "        metrics=[\n",
    "            \"binary_accuracy\" if (num_outputs <= 2)\n",
    "            else tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "        ],\n",
    "    )\n",
    "    return model, encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_UzDOFVhy-5"
   },
   "source": [
    "# CBM Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTjkp5B3hy-6",
    "outputId": "35f4b4fe-7295-4747-a9d4-c52f4c75dab3"
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## Build CBM\n",
    "############################################################################\n",
    "import concepts_xai.methods.CBM.CBModel as CBM\n",
    "\n",
    "def construct_cbm(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    num_outputs,\n",
    "    alpha=0.1,\n",
    "    learning_rate=1e-3,\n",
    "    latent_dims=0,\n",
    "    encoder_output_logits=False,\n",
    "):\n",
    "    model_factory = CBM.BypassJointCBM if latent_dims else CBM.JointConceptBottleneckModel\n",
    "    cbm_model = model_factory(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        task_loss=(\n",
    "            tf.keras.losses.BinaryCrossentropy(from_logits=True) if (num_outputs <= 2)\n",
    "            else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        ),\n",
    "        name=\"joint_cbm\",\n",
    "        metrics=[\n",
    "            tf.keras.metrics.BinaryAccuracy() if (num_outputs <= 2)\n",
    "            else tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "        ],\n",
    "        alpha=alpha,\n",
    "        pass_concept_logits=encoder_output_logits,\n",
    "    )\n",
    "\n",
    "    ############################################################################\n",
    "    ## Compile CBM Model\n",
    "    ############################################################################\n",
    "\n",
    "    cbm_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    )\n",
    "    return cbm_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.evaluation.metrics.oracle as oracle\n",
    "\n",
    "############################################################################\n",
    "## Experiment loop\n",
    "############################################################################\n",
    "\n",
    "def construct_trivial_auc_mat(num_concepts):\n",
    "    result = np.ones((num_concepts, num_concepts), dtype=np.float32) * 0.5\n",
    "    return result + np.eye(num_concepts, dtype=np.float32) * 0.5\n",
    "    \n",
    "def cbm_experiment_loop(\n",
    "    experiment_config,\n",
    "    datasets,\n",
    "    load_from_cache=False,\n",
    "    oracle_matrix_cache=None,\n",
    "):\n",
    "    oracle_matrix_cache = oracle_matrix_cache or {}\n",
    "    utils.reseed(87)\n",
    "    experiment_variables = dict(\n",
    "        task_accuracies=[],\n",
    "        task_aucs=[],\n",
    "        concept_accuracies=[],\n",
    "        purity_scores=[],\n",
    "        non_oracle_purity_scores=[],\n",
    "        purity_matrices=[],\n",
    "        oracle_matrices=[],\n",
    "    )\n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(datasets):\n",
    "                print(\"Found\", len(loaded_means), \"means for variable\", var_name, \"vs\", len(datasets), \"datasets\")\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)}.'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "    \n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    for (ds_name, (x_train, y_train, c_train), (x_test, y_test, c_test)) in datasets[start_ind:]:\n",
    "        print(\"Training with dataset:\", ds_name)\n",
    "        task_accs = []\n",
    "        concept_accs = []\n",
    "        aucs = []\n",
    "        purity_mats = []\n",
    "        oracle_mats = []\n",
    "        purities = []\n",
    "        non_oracle_purities = []\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} of dataset {ds_name}\")\n",
    "            \n",
    "            # Proceed to do and end-to-end model in case we want to\n",
    "            # do some task-specific pretraining\n",
    "            end_to_end_model, encoder, decoder = construct_end_to_end_model(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "                learning_rate=experiment_config[\"learning_rate\"],\n",
    "                encoder=construct_encoder(\n",
    "                    input_shape=experiment_config[\"input_shape\"],\n",
    "                    filter_groups=experiment_config[\"encoder_filter_groups\"],\n",
    "                    units=experiment_config[\"encoder_units\"],\n",
    "                    concept_cardinality=experiment_config[\"concept_cardinality\"],\n",
    "                    drop_prob=experiment_config.get(\"drop_prob\", 0.5),\n",
    "                    max_pool_window=experiment_config.get(\"max_pool_window\", (2, 2)),\n",
    "                    max_pool_stride=experiment_config.get(\"max_pool_stride\", (2, 2)),\n",
    "                    latent_dims=experiment_config.get(\"latent_dims\", 0),\n",
    "                    output_logits=experiment_config.get(\"encoder_output_logits\", False),\n",
    "                ),\n",
    "                decoder=construct_decoder(\n",
    "                    units=experiment_config[\"decoder_units\"],\n",
    "                    num_outputs=experiment_config[\"num_outputs\"],\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            if experiment_config.get(\"pre_train_epochs\"):\n",
    "                print(\"\\tModel pre-training...\")\n",
    "                end_to_end_model.fit(\n",
    "                    x=x_train,\n",
    "                    y=y_train,\n",
    "                    epochs=experiment_config[\"pre_train_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "                print(\"\\t\\tModel pre-training completed\")\n",
    "            \n",
    "            # Now time to actually construct and train the CBM\n",
    "            cbm_model = construct_cbm(\n",
    "                encoder=encoder,\n",
    "                decoder=decoder,\n",
    "                alpha=experiment_config[\"alpha\"],\n",
    "                learning_rate=experiment_config[\"learning_rate\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "                latent_dims=experiment_config.get(\"latent_dims\", 0),\n",
    "                encoder_output_logits=experiment_config.get(\"encoder_output_logits\", False),\n",
    "            )\n",
    "\n",
    "            early_stopping_monitor = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=experiment_config.get(\n",
    "                    \"early_stop_metric\",\n",
    "                    \"val_concept_accuracy\",\n",
    "                ),\n",
    "                min_delta=experiment_config[\"min_delta\"],\n",
    "                patience=experiment_config[\"patience\"],\n",
    "                restore_best_weights=True,\n",
    "                verbose=2,\n",
    "                mode=experiment_config.get(\n",
    "                    \"early_stop_mode\",\n",
    "                    \"max\",\n",
    "                ),\n",
    "            )\n",
    "            if experiment_config[\"warmup_epochs\"]:\n",
    "                print(\"\\tWarmup training...\")\n",
    "                cbm_model.fit(\n",
    "                    x=x_train,\n",
    "                    y=(y_train, c_train),\n",
    "                    epochs=experiment_config[\"warmup_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "                print(\"\\t\\tWarmup training completed\")\n",
    "\n",
    "\n",
    "            print(\"\\tCBM training...\")\n",
    "            cbm_model.fit(\n",
    "                x=x_train,\n",
    "                y=(y_train, c_train),\n",
    "                epochs=experiment_config[\"max_epochs\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                callbacks=[\n",
    "                    early_stopping_monitor,\n",
    "                ],\n",
    "                validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                verbose=verbosity,\n",
    "            )\n",
    "            print(\"\\t\\tCBM training completed\")\n",
    "            print(\"\\tSerializing model\")\n",
    "            encoder.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/encoder_{ds_name}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            decoder.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/decoder_{ds_name}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            print(\"\\tEvaluating model\")\n",
    "            test_result = cbm_model.evaluate(\n",
    "                x_test,\n",
    "                (y_test, c_test),\n",
    "                verbose=0,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            task_accs.append(\n",
    "                test_result['sparse_top_k_categorical_accuracy']\n",
    "                if experiment_config['num_outputs'] > 1 else\n",
    "                test_result['binary_accuracy']\n",
    "            )\n",
    "            concept_accs.append(test_result['concept_accuracy'])\n",
    "            \n",
    "            if experiment_config['num_outputs'] > 1:\n",
    "                # Then lets apply a softmax activation over all the probability\n",
    "                # classes\n",
    "                preds = scipy.special.softmax(cbm_model.predict(x_test)[0], axis=-1)\n",
    "\n",
    "                # And select just the labels that are in fact being used\n",
    "                one_hot_labels = tf.keras.utils.to_categorical(y_test)\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    one_hot_labels,\n",
    "                    preds,\n",
    "                    multi_class='ovo',\n",
    "                ))\n",
    "            else:\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    y_test,\n",
    "                    cbm_model.predict(x_test)[0],\n",
    "                ))\n",
    "            \n",
    "            print(\n",
    "                f\"\\t\\tTest auc = {aucs[-1]:.4f}, \"\n",
    "                f\"test concept accuracy = {concept_accs[-1]:.4f}, \"\n",
    "                f\"task accuracy = {task_accs[-1]:.4f}\"\n",
    "            )\n",
    "\n",
    "            print(f\"\\t\\tComputing purity score...\")\n",
    "            purity_score, purity_mat, oracle_mat = oracle.oracle_impurity_score(\n",
    "                encoder_model=cbm_model.encoder,\n",
    "                features=x_test,\n",
    "                concepts=c_test,\n",
    "                output_matrices=True,\n",
    "                oracle_matrix=oracle_matrix_cache.get(ds_name),\n",
    "            )\n",
    "            purity_mats.append(purity_mat)\n",
    "            oracle_mats.append(oracle_mat)\n",
    "            purities.append(purity_score)\n",
    "            print(f\"\\t\\t\\tDone {purity_score:.4f}\")\n",
    "\n",
    "            print(\"\\t\\tComputing non-oracle purity score...\")\n",
    "            non_oracle_purities.append(oracle.oracle_impurity_score(\n",
    "                encoder_model=cbm_model.encoder,\n",
    "                features=x_test,\n",
    "                concepts=c_test,\n",
    "                oracle_matrix=construct_trivial_auc_mat(\n",
    "                    experiment_config[\"num_concepts\"]\n",
    "                ),\n",
    "                purity_matrix=purity_mat,\n",
    "            ))\n",
    "            print(f\"\\t\\t\\tDone {non_oracle_purities[-1]:.4f}\")\n",
    "            print(\"\\t\\tDone with trial\", trial + 1)\n",
    "\n",
    "        task_acc_mean, task_acc_std = np.mean(task_accs), np.std(task_accs)\n",
    "        experiment_variables[\"task_accuracies\"].append((task_acc_mean, task_acc_std))\n",
    "        print(f\"\\tTest task accuracy: {task_acc_mean:.4f}  {task_acc_std:.4f}\")\n",
    "\n",
    "        concept_acc_mean, concept_acc_std = np.mean(concept_accs), np.std(concept_accs)\n",
    "        experiment_variables[\"concept_accuracies\"].append((concept_acc_mean, concept_acc_std))\n",
    "        print(f\"\\tTest concept accuracy: {concept_acc_mean:.4f}  {concept_acc_std:.4f}\")\n",
    "\n",
    "\n",
    "        task_auc_mean, task_auc_std = np.mean(aucs), np.std(aucs)\n",
    "        experiment_variables[\"task_aucs\"].append((task_auc_mean, task_auc_std))\n",
    "        print(f\"\\tTest task AUC: {task_auc_mean:.4f}  {task_auc_std:.4f}\")\n",
    "\n",
    "        purity_mats = np.stack(purity_mats, axis=0)\n",
    "        purity_mat_mean = np.mean(purity_mats, axis=0)\n",
    "        purity_mat_std = np.std(purity_mats, axis=0)\n",
    "        print(\"\\tPurity matrix:\")\n",
    "        for i in range(purity_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(purity_mat_mean.shape[1]):\n",
    "                line += f'{purity_mat_mean[i, j]:.4f}  {purity_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"purity_matrices\"].append((purity_mat_mean, purity_mat_std))\n",
    "\n",
    "\n",
    "        oracle_mats = np.stack(oracle_mats, axis=0)\n",
    "        oracle_mat_mean = np.mean(oracle_mats, axis=0)\n",
    "        oracle_mat_std = np.std(oracle_mats, axis=0)\n",
    "        print(\"\\tOracle matrix:\")\n",
    "        for i in range(oracle_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(oracle_mat_mean.shape[1]):\n",
    "                line += f'{oracle_mat_mean[i, j]:.4f}  {oracle_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"oracle_matrices\"].append((oracle_mat_mean, oracle_mat_std))\n",
    "\n",
    "        purity_mean, purity_std = np.mean(purities), np.std(purities)\n",
    "        experiment_variables[\"purity_scores\"].append((purity_mean, purity_std))\n",
    "        print(f\"\\tPurity score: {purity_mean:.4f}  {purity_std:.4f}\")\n",
    "\n",
    "        non_oracle_purity_mean, non_oracle_purity_std = np.mean(non_oracle_purities), np.std(non_oracle_purities)\n",
    "        experiment_variables[\"non_oracle_purity_scores\"].append((non_oracle_purity_mean, non_oracle_purity_std))\n",
    "        print(f\"\\tNon-oracle purity score: {non_oracle_purity_mean:.4f}  {non_oracle_purity_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables\n",
    "\n",
    "def cbm_bottleneck_predict_experiment_loop(\n",
    "    experiment_config,\n",
    "    datasets,\n",
    "    load_from_cache=False,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    experiment_variables = dict(\n",
    "        latent_predictive_accuracies=[],\n",
    "        latent_predictive_aucs=[],\n",
    "    )\n",
    "\n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                print(\"Could not find\", f'\"{file_name}_means.npz\" or \"{file_name}_stds.npz\" in cache.')\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(datasets):\n",
    "                print(\"Found\", len(loaded_means), \"means for variable\", var_name, \"vs\", len(datasets), \"datasets\")\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)} ({file_name}).'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            print(experiment_variables)\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    for (ds_name, (x_train, y_train, c_train), (x_test, y_test, c_test)) in datasets[start_ind:]:\n",
    "        print(\"Training with dataset\", ds_name)\n",
    "        task_accs = []\n",
    "        aucs = []\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']}\")\n",
    "            encoder = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/encoder_{ds_name}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            predictive_decoder = construct_decoder(\n",
    "                units=experiment_config[\"latent_decoder_units\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "            )\n",
    "            predictive_decoder.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(experiment_config[\"learning_rate\"]),\n",
    "                loss=(\n",
    "                    tf.keras.losses.BinaryCrossentropy(from_logits=True) if (experiment_config[\"num_outputs\"] <= 2)\n",
    "                    else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "                ),\n",
    "                metrics=[\n",
    "                    \"binary_accuracy\" if (experiment_config[\"num_outputs\"] <= 2)\n",
    "                    else tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            print(\"\\tTraining model\")\n",
    "            train_codes = encoder(x_train)\n",
    "            if isinstance(train_codes, list):\n",
    "                train_codes = np.concatenate(list(map(lambda x: x.numpy(), train_codes)), axis=-1)\n",
    "            else:\n",
    "                train_codes = train_codes.numpy()\n",
    "            test_codes = encoder(x_test)\n",
    "            if isinstance(test_codes, list):\n",
    "                test_codes = np.concatenate(list(map(lambda x: x.numpy(), test_codes)), axis=-1)\n",
    "            else:\n",
    "                test_codes = test_codes.numpy()\n",
    "            predictive_decoder.fit(\n",
    "                x=train_codes,\n",
    "                y=y_train,\n",
    "                epochs=experiment_config[\"predictor_max_epochs\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                verbose=verbosity,\n",
    "            )\n",
    "            print(\"\\tEvaluating model\")\n",
    "            test_result = predictive_decoder.evaluate(\n",
    "                test_codes,\n",
    "                y_test,\n",
    "                verbose=0,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            task_accs.append(\n",
    "                test_result['sparse_top_k_categorical_accuracy']\n",
    "                if experiment_config['num_outputs'] > 1 else\n",
    "                test_result['binary_accuracy']\n",
    "            )\n",
    "\n",
    "            if experiment_config['num_outputs'] > 1:\n",
    "                # Then lets apply a softmax activation over all the probability\n",
    "                # classes\n",
    "                preds = scipy.special.softmax(\n",
    "                    predictive_decoder.predict(test_codes),\n",
    "                    axis=-1,\n",
    "                )\n",
    "\n",
    "                # And select just the labels that are in fact being used\n",
    "                print(np.sum(preds[:100, :], axis=-1))\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    y_test,\n",
    "                    preds,\n",
    "                    multi_class='ovo',\n",
    "                ))\n",
    "            else:\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    y_test,\n",
    "                    predictive_decoder.predict(test_codes),\n",
    "                ))\n",
    "            \n",
    "            print(\n",
    "                f\"\\t\\tTest auc = {aucs[-1]:.4f}, \"\n",
    "                f\"task accuracy = {task_accs[-1]:.4f}\"\n",
    "            )\n",
    "            print(\"\\t\\tDone with trial\", trial + 1)\n",
    "\n",
    "        task_acc_mean, task_acc_std = np.mean(task_accs), np.std(task_accs)\n",
    "        experiment_variables[\"latent_predictive_accuracies\"].append((task_acc_mean, task_acc_std))\n",
    "        print(f\"\\tTest task accuracy: {task_acc_mean:.4f}  {task_acc_std:.4f}\")\n",
    "\n",
    "        task_auc_mean, task_auc_std = np.mean(aucs), np.std(aucs)\n",
    "        experiment_variables[\"latent_predictive_aucs\"].append((task_auc_mean, task_auc_std))\n",
    "        print(f\"\\tTest task AUC: {task_auc_mean:.4f}  {task_auc_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables\n",
    "\n",
    "def cbm_bottleneck_concept_predict_experiment_loop(\n",
    "    experiment_config,\n",
    "    datasets,\n",
    "    load_from_cache=False,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    experiment_variables = dict(\n",
    "        latent_avg_concept_predictive_accuracies=[],\n",
    "        latent_avg_concept_predictive_aucs=[],\n",
    "    )\n",
    "\n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                print(\"Could not find\", f'\"{file_name}_means.npz\" or \"{file_name}_stds.npz\" in cache.')\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(datasets):\n",
    "                print(\"Found\", len(loaded_means), \"means for variable\", var_name, \"vs\", len(datasets), \"datasets\")\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)} ({file_name}).'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            print(experiment_variables)\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    for (ds_name, (x_train, y_train, c_train), (x_test, y_test, c_test)) in datasets[start_ind:]:\n",
    "        print(\"Training with dataset\", ds_name)\n",
    "        avg_concept_accs = []\n",
    "        avg_concept_aucs = []\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']}\")\n",
    "            encoder = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/encoder_{ds_name}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "\n",
    "            train_codes = encoder(x_train)\n",
    "            if isinstance(train_codes, list):\n",
    "                train_codes = np.concatenate(list(map(lambda x: x.numpy(), train_codes)), axis=-1)\n",
    "            else:\n",
    "                train_codes = train_codes.numpy()\n",
    "            test_codes = encoder(x_test)\n",
    "            if isinstance(test_codes, list):\n",
    "                test_codes = np.concatenate(list(map(lambda x: x.numpy(), test_codes)), axis=-1)\n",
    "            else:\n",
    "                test_codes = test_codes.numpy()\n",
    "            \n",
    "            current_accuracies = []\n",
    "            current_aucs = []\n",
    "            for concept_idx in range(experiment_config[\"num_concepts\"]):\n",
    "                print(\"\\tTraining model for concept\", concept_idx)\n",
    "                predictive_decoder = construct_decoder(\n",
    "                    units=experiment_config[\"latent_decoder_units\"],\n",
    "                    num_outputs=1,\n",
    "                )\n",
    "                predictive_decoder.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(experiment_config[\"learning_rate\"]),\n",
    "                    loss=(\n",
    "                        tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "                    ),\n",
    "                    metrics=[\n",
    "                        \"binary_accuracy\"\n",
    "                    ],\n",
    "                )\n",
    "                predictive_decoder.fit(\n",
    "                    x=train_codes,\n",
    "                    y=c_train[:, concept_idx],\n",
    "                    epochs=experiment_config[\"concept_predictor_max_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "                print(\"\\t\\tEvaluating model\")\n",
    "                test_result = predictive_decoder.evaluate(\n",
    "                    test_codes,\n",
    "                    c_test[:, concept_idx],\n",
    "                    verbose=0,\n",
    "                    return_dict=True,\n",
    "                )\n",
    "                current_accuracies.append(test_result['binary_accuracy'])\n",
    "                \n",
    "                current_aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    c_test[:, concept_idx],\n",
    "                    predictive_decoder.predict(test_codes),\n",
    "                ))\n",
    "                print(\n",
    "                f\"\\t\\t\\tAverage test concept accuracy = {current_accuracies[-1]:.4f}, \"\n",
    "                f\"average test concept AUC = {current_aucs[-1]:.4f}\"\n",
    "            )\n",
    "            \n",
    "            avg_concept_accs.append(np.mean(current_accuracies))\n",
    "            avg_concept_aucs.append(np.mean(current_aucs))\n",
    "            print(\n",
    "                f\"\\t\\tAverage test concept accuracy = {avg_concept_accs[-1]:.4f}, \"\n",
    "                f\"average test concept AUC = {avg_concept_aucs[-1]:.4f}\"\n",
    "            )\n",
    "            print(\"\\t\\tDone with trial\", trial + 1)\n",
    "\n",
    "        avg_concept_acc_mean, avg_concept_acc_std = np.mean(avg_concept_accs), np.std(avg_concept_accs)\n",
    "        experiment_variables[\"latent_avg_concept_predictive_accuracies\"].append((avg_concept_acc_mean, avg_concept_acc_std))\n",
    "        print(f\"\\tTest task accuracy: {avg_concept_acc_mean:.4f}  {avg_concept_acc_std:.4f}\")\n",
    "\n",
    "        avg_concept_auc_mean, avg_concept_auc_std = np.mean(avg_concept_aucs), np.std(avg_concept_aucs)\n",
    "        experiment_variables[\"latent_avg_concept_predictive_aucs\"].append((avg_concept_auc_mean, avg_concept_auc_std))\n",
    "        print(f\"\\tTest task AUC: {avg_concept_auc_mean:.4f}  {avg_concept_auc_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBM Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(CBM)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "graph_dependency_balanced_multiclass_experiment_config = dict(\n",
    "    batch_size=64,\n",
    "    max_epochs=100,\n",
    "    warmup_epochs=0,\n",
    "    pre_train_epochs=0,\n",
    "    trials=5,\n",
    "    alpha=10,\n",
    "    learning_rate=1e-3,\n",
    "    encoder_filter_groups=[\n",
    "        [(8, (7, 7))],\n",
    "        [(16, (5, 5))],\n",
    "        [(32, (3, 3))],\n",
    "        [(64, (3, 3))]\n",
    "    ],\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[64, 64],\n",
    "    \n",
    "    latent_decoder_units=[64, 64],\n",
    "    predictor_max_epochs=100,\n",
    "    concept_predictor_max_epochs=100,\n",
    "    \n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    pax_pool_stride=2,\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1]))\n",
    "        if len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2 else 1\n",
    "    ),\n",
    "    concept_cardinality=[\n",
    "        2 for _ in range(balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1])\n",
    "    ],\n",
    "    num_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(RESULTS_DIR, \"cbm/graph_dependency_balanced_multiclass_tasks_purity\"),\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    latent_dims=0,\n",
    "    holdout_fraction=0.1,\n",
    "    verbosity=0,\n",
    "    early_stop_metric=\"val_concept_accuracy\",\n",
    "    early_stop_mode=\"max\",\n",
    "    encoder_output_logits=False,\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(graph_dependency_balanced_multiclass_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "graph_dependency_balanced_multiclass_figure_dir = os.path.join(graph_dependency_balanced_multiclass_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(graph_dependency_balanced_multiclass_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "graph_dependency_balanced_multiclass_results = cbm_experiment_loop(\n",
    "    graph_dependency_balanced_multiclass_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    ")\n",
    "print(\"task_accuracies:\", graph_dependency_balanced_multiclass_results[\"task_accuracies\"])\n",
    "print(\"concept_accuracies:\", graph_dependency_balanced_multiclass_results[\"concept_accuracies\"])\n",
    "print(\"task_aucs:\", graph_dependency_balanced_multiclass_results[\"task_aucs\"])\n",
    "print(\"purity_scores:\", graph_dependency_balanced_multiclass_results[\"purity_scores\"])\n",
    "\n",
    "# And let's generate an oracle matrix cache to accelerate the experiments that follow\n",
    "# up with the same datasets\n",
    "balanced_oracle_matrix_cache = {\n",
    "    \"balanced_multiclass_task_bin_concepts_dep_0_complete\": graph_dependency_balanced_multiclass_results[\"oracle_matrices\"][0][0],\n",
    "    \"balanced_multiclass_task_bin_concepts_dep_1_complete\": graph_dependency_balanced_multiclass_results[\"oracle_matrices\"][1][0],\n",
    "    \"balanced_multiclass_task_bin_concepts_dep_2_complete\": graph_dependency_balanced_multiclass_results[\"oracle_matrices\"][2][0],\n",
    "    \"balanced_multiclass_task_bin_concepts_dep_3_complete\": graph_dependency_balanced_multiclass_results[\"oracle_matrices\"][3][0],\n",
    "    \"balanced_multiclass_task_bin_concepts_dep_4_complete\": graph_dependency_balanced_multiclass_results[\"oracle_matrices\"][4][0],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dependency_balanced_multiclass_results.update(cbm_bottleneck_predict_experiment_loop(\n",
    "    graph_dependency_balanced_multiclass_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dependency_balanced_multiclass_results.update(cbm_bottleneck_concept_predict_experiment_loop(\n",
    "    graph_dependency_balanced_multiclass_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBM With Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(CBM)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "graph_dependency_balanced_multiclass_from_logits_experiment_config = dict(\n",
    "    batch_size=64,\n",
    "    max_epochs=100,\n",
    "    warmup_epochs=0,\n",
    "    pre_train_epochs=0,\n",
    "    trials=5,\n",
    "    alpha=10,\n",
    "    learning_rate=1e-3,\n",
    "    encoder_filter_groups=[\n",
    "        [(8, (7, 7))],\n",
    "        [(16, (5, 5))],\n",
    "        [(32, (3, 3))],\n",
    "        [(64, (3, 3))]\n",
    "    ],\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[64, 64],\n",
    "    \n",
    "    latent_decoder_units=[64, 64],\n",
    "    predictor_max_epochs=100,\n",
    "    concept_predictor_max_epochs=100,\n",
    "    \n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    pax_pool_stride=2,\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1]))\n",
    "        if len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2 else 1\n",
    "    ),\n",
    "    concept_cardinality=[\n",
    "        2 for _ in range(balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1])\n",
    "    ],\n",
    "    num_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(RESULTS_DIR, \"cbm/graph_dependency_balanced_multiclass_from_logits_tasks_purity\"),\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    latent_dims=0,\n",
    "    holdout_fraction=0.1,\n",
    "    verbosity=0,\n",
    "    early_stop_metric=\"val_concept_accuracy\",\n",
    "    early_stop_mode=\"max\",\n",
    "    encoder_output_logits=True,\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(graph_dependency_balanced_multiclass_from_logits_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "graph_dependency_balanced_multiclass_from_logits_figure_dir = os.path.join(graph_dependency_balanced_multiclass_from_logits_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(graph_dependency_balanced_multiclass_from_logits_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "graph_dependency_balanced_multiclass_from_logits_results = cbm_experiment_loop(\n",
    "    graph_dependency_balanced_multiclass_from_logits_experiment_config,\n",
    "    oracle_matrix_cache=balanced_oracle_matrix_cache,\n",
    "    load_from_cache=True,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    ")\n",
    "print(\"task_accuracies:\", graph_dependency_balanced_multiclass_from_logits_results[\"task_accuracies\"])\n",
    "print(\"concept_accuracies:\", graph_dependency_balanced_multiclass_from_logits_results[\"concept_accuracies\"])\n",
    "print(\"task_aucs:\", graph_dependency_balanced_multiclass_from_logits_results[\"task_aucs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dependency_balanced_multiclass_from_logits_results.update(cbm_bottleneck_predict_experiment_loop(\n",
    "    graph_dependency_balanced_multiclass_from_logits_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dependency_balanced_multiclass_from_logits_results.update(cbm_bottleneck_concept_predict_experiment_loop(\n",
    "    graph_dependency_balanced_multiclass_from_logits_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CW Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.CW.CWLayer as CW\n",
    "\n",
    "def conv_predictor_model_fn(\n",
    "    input_concept_classes=1,\n",
    "    output_concept_classes=2,\n",
    "):\n",
    "    estimator = tf.keras.models.Sequential([\n",
    "         tf.keras.layers.Conv2D(\n",
    "            filters=16,\n",
    "            kernel_size=(3,3),\n",
    "            padding=\"SAME\",\n",
    "            activation=\"relu\",\n",
    "            data_format=\"channels_last\",\n",
    "        ),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=(3,3),\n",
    "            padding=\"SAME\",\n",
    "            activation=\"relu\",\n",
    "            data_format=\"channels_last\",\n",
    "        ),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(\n",
    "            output_concept_classes if output_concept_classes > 2 else 1,\n",
    "            # We will merge the activation into the loss for numerical\n",
    "            # stability\n",
    "            activation=None,\n",
    "        ),\n",
    "    ])\n",
    "    estimator.compile(\n",
    "        # Use ADAM optimizer by default\n",
    "        optimizer='adam',\n",
    "        # Note: we assume labels come without a one-hot-encoding in the\n",
    "        #       case when the concepts are categorical.\n",
    "        loss=(\n",
    "            tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                from_logits=True\n",
    "            ) if output_concept_classes > 2 else\n",
    "            tf.keras.losses.BinaryCrossentropy(\n",
    "                from_logits=True,\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    return estimator\n",
    "\n",
    "\n",
    "def construct_cw_model(\n",
    "    input_shape,\n",
    "    num_outputs,\n",
    "    filter_groups,\n",
    "    units,\n",
    "    activation_mode,\n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    T=5,\n",
    "    eps=1e-5,\n",
    "    momentum=0.9,\n",
    "    c1=1e-4,\n",
    "    c2=0.9,\n",
    "    max_tau_iterations=500,\n",
    "    initial_tau=1000.0,\n",
    "    initial_beta=1e8,\n",
    "    initial_alpha=0,\n",
    "):\n",
    "    model_inputs = tf.keras.Input(shape=input_shape)\n",
    "    model_compute_graph = model_inputs\n",
    "    \n",
    "    # Start with our convolutions\n",
    "    num_convs = 0\n",
    "    cw_inputs = []\n",
    "    cw_ouputs = []\n",
    "    for filter_group in filter_groups:\n",
    "        # Add a default \"no CW layer\" to each filter group\n",
    "        # if they have not specified this feature\n",
    "        filter_group = list(map(\n",
    "            lambda x: x if len(x) == 3 else (x[0], x[1], False),\n",
    "            filter_group\n",
    "        ))\n",
    "        for (num_filters, kernel_size, cw_layer) in filter_group:\n",
    "            model_compute_graph = tf.keras.layers.Conv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=\"SAME\",\n",
    "                activation=None,\n",
    "                name=f'encoder_conv_{num_convs}',\n",
    "            )(model_compute_graph)\n",
    "            num_convs += 1\n",
    "            if cw_layer:\n",
    "                cw_inputs.append(model_compute_graph)\n",
    "                model_compute_graph = CW.ConceptWhiteningLayer(\n",
    "                    data_format=\"channels_last\",\n",
    "                    activation_mode=activation_mode,\n",
    "                    T=T,\n",
    "                    eps=eps,\n",
    "                    momentum=momentum,\n",
    "                    c1=c1,\n",
    "                    c2=c2,\n",
    "                    max_tau_iterations=max_tau_iterations,\n",
    "                    initial_tau=initial_tau,\n",
    "                    initial_beta=initial_beta,\n",
    "                    initial_alpha=initial_alpha,\n",
    "                )(\n",
    "                    model_compute_graph\n",
    "                )\n",
    "                cw_ouputs.append(model_compute_graph)\n",
    "            else:\n",
    "                model_compute_graph = tf.keras.layers.BatchNormalization(\n",
    "                    axis=-1,\n",
    "                )(model_compute_graph)\n",
    "            model_compute_graph = tf.keras.activations.relu(model_compute_graph)\n",
    "        # Then do a max pool here to control the parameter count of the model\n",
    "        # at the end of each group\n",
    "        model_compute_graph = tf.keras.layers.MaxPooling2D(\n",
    "            pool_size=max_pool_window,\n",
    "            strides=max_pool_stride,\n",
    "        )(\n",
    "            model_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Flatten this guy\n",
    "    model_compute_graph = tf.keras.layers.Flatten()(model_compute_graph)\n",
    "    \n",
    "    # Add a dropout if requested\n",
    "    if drop_prob:\n",
    "        model_compute_graph = tf.keras.layers.Dropout(drop_prob)(\n",
    "            model_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Finally, include the fully connected bottleneck here\n",
    "    for i, units in enumerate(units):\n",
    "        model_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(model_compute_graph)\n",
    "    \n",
    "    # Map to our output distribution to a flattened\n",
    "    # vector where we will extract distributions over\n",
    "    # all concept values\n",
    "    model_compute_graph = tf.keras.layers.Dense(\n",
    "        num_outputs,\n",
    "        activation=None,\n",
    "        name=\"logits\",\n",
    "    )(model_compute_graph)\n",
    "    \n",
    "  \n",
    "    cw_model = tf.keras.Model(\n",
    "        model_inputs,\n",
    "        model_compute_graph,\n",
    "        name=\"cw_model\",\n",
    "    )\n",
    "    cw_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "        loss=(\n",
    "            tf.keras.losses.BinaryCrossentropy(from_logits=True) if (num_outputs <= 2)\n",
    "            else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        ),\n",
    "        metrics=[\n",
    "            \"binary_accuracy\" if (num_outputs <= 2)\n",
    "            else tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    encoder = tf.keras.Model(\n",
    "        model_inputs,\n",
    "        cw_inputs,\n",
    "        name=\"encoder_model\",\n",
    "    )\n",
    "\n",
    "    cw_output_model = tf.keras.Model(\n",
    "        model_inputs,\n",
    "        cw_ouputs,\n",
    "        name=\"cw_output_model\",\n",
    "    )\n",
    "    return cw_model, encoder, cw_output_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.evaluation.metrics.leakage as leakage\n",
    "\n",
    "def channels_corr_mat(outputs):\n",
    "    if len(outputs.shape) == 2:\n",
    "        outputs = np.expand_dims(\n",
    "            np.expand_dims(outputs, axis=1),\n",
    "            axis=1,\n",
    "        )\n",
    "    # Change (N, H, W, C) to (C, N, H, W)\n",
    "    outputs = np.transpose(outputs, [3, 0, 1, 2])\n",
    "    # Change (C, N, H, W) to (C, NxHxW)\n",
    "    cnhw_shape = outputs.shape\n",
    "    outputs = np.transpose(np.reshape(outputs, [cnhw_shape[0], -1]))\n",
    "    outputs -= np.mean(outputs, axis=0, keepdims=True)\n",
    "    outputs = outputs / np.std(outputs, axis=0, keepdims=True)\n",
    "    return np.dot(outputs.transpose(), outputs) / outputs.shape[0]\n",
    "\n",
    "def conv_predictor_model_fn(\n",
    "    input_concept_classes=1,\n",
    "    output_concept_classes=2,\n",
    "):\n",
    "    estimator = tf.keras.models.Sequential([\n",
    "         tf.keras.layers.Conv2D(\n",
    "            filters=16,\n",
    "            kernel_size=(3,3),\n",
    "            padding=\"SAME\",\n",
    "            activation=\"relu\",\n",
    "            data_format=\"channels_last\",\n",
    "        ),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=(3,3),\n",
    "            padding=\"SAME\",\n",
    "            activation=\"relu\",\n",
    "            data_format=\"channels_last\",\n",
    "        ),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(\n",
    "            output_concept_classes if output_concept_classes > 2 else 1,\n",
    "            # We will merge the activation into the loss for numerical\n",
    "            # stability\n",
    "            activation=None,\n",
    "        ),\n",
    "    ])\n",
    "    estimator.compile(\n",
    "        # Use ADAM optimizer by default\n",
    "        optimizer='adam',\n",
    "        # Note: we assume labels come without a one-hot-encoding in the\n",
    "        #       case when the concepts are categorical.\n",
    "        loss=(\n",
    "            tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                from_logits=True\n",
    "            ) if output_concept_classes > 2 else\n",
    "            tf.keras.losses.BinaryCrossentropy(\n",
    "                from_logits=True,\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    return estimator\n",
    "\n",
    "\n",
    "def cw_experiment_loop(\n",
    "    experiment_config,\n",
    "    datasets,\n",
    "    load_from_cache=False,\n",
    "    oracle_matrix_cache=None,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    oracle_matrix_cache = oracle_matrix_cache or {}\n",
    "    experiment_variables = dict(\n",
    "        task_accuracies=[],\n",
    "        task_aucs=[],\n",
    "        concept_aucs=[],\n",
    "        purity_scores=[],\n",
    "        repr_purity_scores=[],\n",
    "        non_oracle_purity_scores=[],\n",
    "        purity_matrices=[],\n",
    "        repr_purity_matrices=[],\n",
    "        oracle_matrices=[],\n",
    "        similarity_ratio_matrices=[],\n",
    "        correlation_matrices=[],\n",
    "    )\n",
    "    experiment_config[\"data_concepts\"] = experiment_config.get(\n",
    "        \"data_concepts\",\n",
    "        experiment_config[\"num_concepts\"],\n",
    "    )\n",
    "    \n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(datasets):\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)}.'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "    \n",
    "    # Let's save our config here either way\n",
    "    serialize_experiment_config(\n",
    "        experiment_config,\n",
    "        experiment_config[\"results_dir\"],\n",
    "    )\n",
    "    \n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    for (ds_name, (x_train, y_train, c_train), (x_test, y_test, c_test)) in datasets[start_ind:]:\n",
    "        print(\"Training with dataset:\", ds_name)\n",
    "        task_accs = []\n",
    "        c_aucs = []\n",
    "        aucs = []\n",
    "        purity_mats = []\n",
    "        repr_purity_mats = []\n",
    "        oracle_mats = []\n",
    "        purities = []\n",
    "        repr_purities = []\n",
    "        non_oracle_purities = []\n",
    "        similarities = []\n",
    "        correlations = []\n",
    "        \n",
    "        if not experiment_config.get(\"exclusive_concepts\", False):\n",
    "            concept_groups = [\n",
    "                x_train[c_train[:, i] == 1, :, :, :]\n",
    "                for i in range(c_train.shape[-1])\n",
    "            ]\n",
    "        else:\n",
    "            concept_groups = [\n",
    "                x_train[np.logical_and(c_train[:, i] == 1, np.sum(c_train, axis=-1) == 1), :, :, :]\n",
    "                for i in range(c_train.shape[-1])\n",
    "            ]\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} of {ds_name}\")\n",
    "            if not experiment_config.get(\"exclusive_test_concepts\", False):\n",
    "                test_concept_groups = [\n",
    "                    x_train[c_train[:, i] == 1, :, :, :]\n",
    "                    for i in range(c_test.shape[-1])\n",
    "                ]\n",
    "            else:\n",
    "                test_concept_groups = [\n",
    "                    x_test[np.logical_and(c_test[:, i] == 1, np.sum(c_test, axis=-1) == 1), :, :, :]\n",
    "                    for i in range(c_test.shape[-1])\n",
    "                ]\n",
    "            print(\"Sizes of test_concept_groups:\", list(map(lambda x: x.shape, test_concept_groups)))\n",
    "            for i, group in enumerate(test_concept_groups):\n",
    "                max_test_size = experiment_config.get(\"max_concept_group_size\", group.shape[-1])\n",
    "                test_concept_groups[i] = (\n",
    "                    group[np.random.choice(group.shape[0], max_test_size), : :, :]\n",
    "                    if group.shape[0] > max_test_size else group\n",
    "                )\n",
    "\n",
    "\n",
    "            # Construct our CW model\n",
    "            model, encoder, cw_model = construct_cw_model(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "                filter_groups=experiment_config[\"filter_groups\"],\n",
    "                units=experiment_config[\"units\"],\n",
    "                drop_prob=experiment_config.get(\"drop_prob\", 0),\n",
    "                max_pool_window=experiment_config.get(\"max_pool_window\", (2, 2)),\n",
    "                max_pool_stride=experiment_config.get(\"max_pool_stride\", (2, 2)),\n",
    "                T=experiment_config.get(\"T\", 5),\n",
    "                eps=experiment_config.get(\"eps\", 1e-5),\n",
    "                momentum=experiment_config.get(\"momentum\", 0.9),\n",
    "                activation_mode=experiment_config[\"activation_mode\"],\n",
    "                c1=experiment_config.get(\"c1\", 1e-4),\n",
    "                c2=experiment_config.get(\"c2\", 0.9),\n",
    "                max_tau_iterations=experiment_config.get(\"max_tau_iterations\", 500),\n",
    "                initial_tau=experiment_config.get(\"initial_tau\", 1000),\n",
    "                initial_beta=experiment_config.get(\"initial_beta\", 1e8),\n",
    "                initial_alpha=experiment_config.get(\"initial_alpha\", 0),\n",
    "            )\n",
    "            # First do some pretraining for warming up the estimates if needed\n",
    "            if experiment_config.get(\"pre_train_epochs\"):\n",
    "                print(\"\\tModel pre-training...\")\n",
    "                model.fit(\n",
    "                    x=x_train,\n",
    "                    y=y_train,\n",
    "                    epochs=experiment_config[\"pre_train_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "                if experiment_config.get(\"heatmap_display\"):\n",
    "                    fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "                    similarity_ratio = oracle.concept_similarity_matrix(\n",
    "                        concept_representations=list(map(\n",
    "                            lambda x: np.mean(cw_model(x).numpy(), axis=(1, 2)),\n",
    "                            concept_groups\n",
    "                        )),\n",
    "                        compute_ratios=True,\n",
    "                    )\n",
    "                    im, cbar = utils.heatmap(\n",
    "                        similarity_ratio,\n",
    "                        [f\"$c_{i}$\" for i in range(len(concept_groups))],\n",
    "                        [f\"$c_{i}$\" for i in range(len(concept_groups))],\n",
    "                        ax=ax,\n",
    "                        cmap=\"magma\",\n",
    "                        cbarlabel=f\"Similarity Ratio\",\n",
    "                        vmin=0,\n",
    "                        vmax=1,\n",
    "                    )\n",
    "                    texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "                    fig.tight_layout()\n",
    "\n",
    "                    fig.suptitle(f\"Baseline Concept Axis Separability\", fontsize=25)\n",
    "                    fig.subplots_adjust(top=0.85)\n",
    "                    plt.show()\n",
    "                    \n",
    "                    fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "                    corr_mat = channels_corr_mat(cw_model(x_test).numpy())[:len(test_concept_groups), :len(test_concept_groups)]\n",
    "                    im, cbar = utils.heatmap(\n",
    "                        np.abs(corr_mat),\n",
    "                        [f\"{_LATEX_SYMBOL}f_{i}{_LATEX_SYMBOL}\" for i in range(corr_mat.shape[-1])],\n",
    "                        [f\"{_LATEX_SYMBOL}f_{i}{_LATEX_SYMBOL}\" for i in range(corr_mat.shape[-1])],\n",
    "                        ax=ax,\n",
    "                        cmap=\"magma\",\n",
    "                        cbarlabel=f\"Mean Correlation Coef\",\n",
    "                        vmin=0,\n",
    "                        vmax=1,\n",
    "                    )\n",
    "                    texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "                    fig.tight_layout()\n",
    "\n",
    "                    fig.suptitle(f\"Latent Dimension Correlation\", fontsize=25)\n",
    "                    fig.subplots_adjust(top=0.85)\n",
    "                    plt.show()\n",
    "                print(\"\\t\\tModel pre-training completed\")\n",
    "            \n",
    "            # Set up the dataset in a nice usable form for unrolling the training\n",
    "            # loop\n",
    "            main_dataset_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            main_dataset_loader = main_dataset_loader.shuffle(buffer_size=1000).batch(\n",
    "                experiment_config[\"batch_size\"]\n",
    "            )\n",
    "            min_size = min(list(map(lambda x: x.shape[0], concept_groups)))\n",
    "            print(\"Minimum size is\", min_size, \"given concept datasets\", list(map(lambda x: x.shape[0], concept_groups)))\n",
    "            loader_concept_groups = list(map(lambda x: x[:min_size, :, :, :], concept_groups))\n",
    "            concept_group_loader = tf.data.Dataset.from_tensor_slices(tuple(loader_concept_groups))\n",
    "            concept_group_loader = concept_group_loader.shuffle(buffer_size=1000).batch(\n",
    "                experiment_config[\"batch_size\"]\n",
    "            )\n",
    "\n",
    "            @tf.function\n",
    "            def _train_step(model, x_batch_train, y_batch_train):\n",
    "                # Update the other model parameters\n",
    "                with tf.GradientTape() as tape:\n",
    "                    logits = model(x_batch_train, training=True)\n",
    "                    loss_value = model.loss(y_batch_train, logits)\n",
    "\n",
    "                grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "                model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "                return loss_value\n",
    "\n",
    "            total_steps = 0\n",
    "            for epoch in range(experiment_config[\"max_epochs\"]):\n",
    "                num_batches = len(main_dataset_loader)\n",
    "                for current_step, (x_batch_train, y_batch_train) in enumerate(main_dataset_loader):\n",
    "                    print(\n",
    "                        f'Epoch {epoch + 1} and step {current_step}/{num_batches}         ',\n",
    "                        end=\"\\r\",\n",
    "                    )\n",
    "                    # Need to update the rotation matrix\n",
    "                    if (total_steps + 1) % experiment_config[\"cw_train_freq\"] == 0:\n",
    "                        for _ in range(experiment_config.get(\"cw_train_iterations\", 1)):\n",
    "                            cw_batch_steps = 0\n",
    "                            for concept_groups_batch in concept_group_loader:\n",
    "                                if cw_batch_steps > experiment_config.get(\"cw_train_batch_steps\", float(\"inf\")):\n",
    "                                    break\n",
    "                                model.layers[experiment_config[\"cw_layer\"]].update_rotation_matrix(\n",
    "                                    concept_groups=list(map(lambda x: encoder(x), concept_groups_batch)),\n",
    "                                )\n",
    "                                cw_batch_steps += 1\n",
    "                    if experiment_config.get(\"concept_auc_freq\"):\n",
    "                        if (total_steps % experiment_config[\"concept_auc_freq\"]) == 0:\n",
    "                            concept_aucs = leakage.compute_concept_aucs(\n",
    "                                cw_model=model,\n",
    "                                encoder=encoder,\n",
    "                                cw_layer=experiment_config[\"cw_layer\"],\n",
    "                                x_test=x_test,\n",
    "                                c_test=c_test,\n",
    "                                num_concepts=experiment_config[\"num_concepts\"],\n",
    "                                aggregator=experiment_config['aggregator'],\n",
    "                            )\n",
    "                            print(\n",
    "                                f'Concept AUC at step {total_steps}:',\n",
    "                                concept_aucs\n",
    "                            )\n",
    "                            print(\"Similarity ratios...\")\n",
    "                            print(oracle.concept_similarity_matrix(\n",
    "                                concept_representations=list(map(\n",
    "                                    lambda x: np.mean(cw_model(x).numpy(), axis=(1, 2)),\n",
    "                                    test_concept_groups,\n",
    "                                )),\n",
    "                                compute_ratios=True,\n",
    "                            ))\n",
    "                            \n",
    "                            print(\"Correlation matrix...\")\n",
    "                            print(np.abs(channels_corr_mat(cw_model(x_test).numpy())[:len(test_concept_groups), :len(test_concept_groups)]))\n",
    "                    _train_step(model, x_batch_train, y_batch_train)\n",
    "                    total_steps += 1\n",
    "            \n",
    "            print(\"\\tBegining post-training of CW module\")\n",
    "            for epoch in range(experiment_config[\"post_train_epochs\"]):\n",
    "                # Need to update the rotation matrix\n",
    "                num_batches = len(concept_group_loader)\n",
    "                for i, concept_groups_batch in enumerate(concept_group_loader):\n",
    "                    print(\n",
    "                        f'Epoch {epoch + 1} and step {i}/{num_batches}         ',\n",
    "                        end=\"\\r\",\n",
    "                    )\n",
    "                    model.layers[experiment_config[\"cw_layer\"]].update_rotation_matrix(\n",
    "                        concept_groups=list(map(lambda x: encoder(x), concept_groups_batch)),\n",
    "                    )\n",
    "            \n",
    "            print(\"\\t\\tCW training completed\")\n",
    "            print(\"\\tSerializing model\")\n",
    "            model.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/end_to_end_model_{ds_name}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            print(\"\\tEvaluating model\")\n",
    "            test_result = model.evaluate(\n",
    "                x_test,\n",
    "                y_test,\n",
    "                verbose=0,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            task_accs.append(test_result[\n",
    "                'sparse_top_k_categorical_accuracy' if experiment_config[\"num_outputs\"] > 1\n",
    "                else 'binary_accuracy'\n",
    "            ])\n",
    "            c_aucs.append(leakage.compute_concept_aucs(\n",
    "                cw_model=model,\n",
    "                encoder=encoder,\n",
    "                cw_layer=experiment_config[\"cw_layer\"],\n",
    "                x_test=x_test,\n",
    "                c_test=c_test,\n",
    "                num_concepts=experiment_config[\"num_concepts\"],\n",
    "                aggregator=experiment_config['aggregator'],\n",
    "            ))\n",
    "            \n",
    "            if experiment_config[\"num_outputs\"] > 1:\n",
    "                # Then lets apply a softmax activation over all the probability\n",
    "                # classes\n",
    "                preds = scipy.special.softmax(model.predict(x_test), axis=-1)\n",
    "\n",
    "                # And select just the labels that are in fact being used\n",
    "                one_hot_labels = tf.keras.utils.to_categorical(y_test)\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    one_hot_labels,\n",
    "                    preds,\n",
    "                    multi_class='ovo',\n",
    "                ))\n",
    "            else:\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    y_test,\n",
    "                    model.predict(x_test),\n",
    "                ))\n",
    "            \n",
    "            print(\n",
    "                f\"\\t\\tTest auc = {aucs[-1]:.4f}, \"\n",
    "                f\"test concept AUCs = {c_aucs[-1]}, \"\n",
    "                f\"task accuracy = {task_accs[-1]:.4f}\"\n",
    "            )\n",
    "            \n",
    "            \n",
    "            print(\"\\t\\tComputing purity score...\")\n",
    "            concept_scores = cw_model.layers[-1].concept_scores(\n",
    "                encoder(x_test),\n",
    "                aggregator=experiment_config['aggregator'],\n",
    "            ).numpy()[:, list(range(experiment_config[\"num_concepts\"]))]\n",
    "            purity_score, purity_mat, oracle_mat = oracle.oracle_impurity_score(\n",
    "                c_soft=concept_scores,\n",
    "                c_true=c_test,\n",
    "                output_matrices=True,\n",
    "                oracle_matrix=oracle_matrix_cache.get(ds_name),\n",
    "            )\n",
    "            purity_mats.append(purity_mat)\n",
    "            oracle_mats.append(oracle_mat)\n",
    "            purities.append(purity_score)\n",
    "            \n",
    "            print(f\"\\t\\t\\tDone {purity_score:.4f}\")\n",
    "            \n",
    "            if len(experiment_config[\"input_shape\"]) > 2:\n",
    "                print(\"\\t\\tComputing full representation purity score...\")\n",
    "                repr_purity_score, repr_purity_mat, _ = oracle.oracle_impurity_score(\n",
    "                    c_soft=cw_model(x_test).numpy()[:, :, :, :experiment_config[\"num_concepts\"]],\n",
    "                    c_true=c_test,\n",
    "                    output_matrices=True,\n",
    "                    oracle_matrix=oracle_mat,\n",
    "                    predictor_model_fn=conv_predictor_model_fn,\n",
    "                )\n",
    "                repr_purity_mats.append(repr_purity_mat)\n",
    "                repr_purities.append(repr_purity_score)\n",
    "                print(f\"\\t\\t\\tDone {repr_purity_score:.4f}\")\n",
    "\n",
    "            print(\"\\t\\tComputing non-oracle purity score...\")\n",
    "            non_oracle_purities.append(oracle.oracle_impurity_score(\n",
    "                c_soft=concept_scores,\n",
    "                c_true=c_test,\n",
    "                oracle_matrix=construct_trivial_auc_mat(\n",
    "                    experiment_config[\"data_concepts\"]\n",
    "                ),\n",
    "                purity_matrix=purity_mat,\n",
    "            ))\n",
    "            print(f\"\\t\\t\\tDone {non_oracle_purities[-1]:.4f}\")\n",
    "            \n",
    "            # Compute similarity matrices\n",
    "            print(\"\\t\\tComputing similarity ratios...\")\n",
    "            similarity_ratio = oracle.concept_similarity_matrix(\n",
    "                concept_representations=list(map(\n",
    "                    lambda x: np.mean(cw_model(x).numpy(), axis=(1, 2)),\n",
    "                    test_concept_groups,\n",
    "                )),\n",
    "                compute_ratios=True,\n",
    "            )\n",
    "            if experiment_config.get(\"heatmap_display\"):\n",
    "                fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "                im, cbar = utils.heatmap(\n",
    "                    similarity_ratio,\n",
    "                    [f\"{_LATEX_SYMBOL}c_{i}{_LATEX_SYMBOL}\" for i in range(len(concept_groups))],\n",
    "                    [f\"{_LATEX_SYMBOL}c_{i}{_LATEX_SYMBOL}\" for i in range(len(concept_groups))],\n",
    "                    ax=ax,\n",
    "                    cmap=\"magma\",\n",
    "                    cbarlabel=f\"Similarity Ratio\",\n",
    "                    vmin=0,\n",
    "                    vmax=1,\n",
    "                )\n",
    "                texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "                fig.tight_layout()\n",
    "\n",
    "                fig.suptitle(f\"Concept Axis Separability\", fontsize=25)\n",
    "                fig.subplots_adjust(top=0.85)\n",
    "                plt.show()\n",
    "            similarities.append(similarity_ratio)\n",
    "\n",
    "            \n",
    "            # Compute correlation matrices\n",
    "            print(\"\\t\\tComputing correlation matrix...\")\n",
    "            corr_mat = channels_corr_mat(cw_model(x_test).numpy())\n",
    "            correlations.append(corr_mat)\n",
    "            \n",
    "            if experiment_config.get(\"heatmap_display\"):\n",
    "                corr_mat = corr_mat[:len(test_concept_groups), :len(test_concept_groups)]\n",
    "                fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "                im, cbar = utils.heatmap(\n",
    "                    np.abs(corr_mat),\n",
    "                    [f\"{_LATEX_SYMBOL}f_{i}{_LATEX_SYMBOL}\" for i in range(corr_mat.shape[-1])],\n",
    "                    [f\"{_LATEX_SYMBOL}f_{i}{_LATEX_SYMBOL}\" for i in range(corr_mat.shape[-1])],\n",
    "                    ax=ax,\n",
    "                    cmap=\"magma\",\n",
    "                    cbarlabel=f\"Correlation Coef\",\n",
    "                    vmin=0,\n",
    "                    vmax=1,\n",
    "                )\n",
    "                texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "                fig.tight_layout()\n",
    "\n",
    "                fig.suptitle(f\"Latent Dimension Correlation\", fontsize=25)\n",
    "                fig.subplots_adjust(top=0.85)\n",
    "                plt.show()\n",
    "                \n",
    "            # Compute representation purity score\n",
    "            \n",
    "            print(\"\\t\\tDone with trial\", trial + 1)\n",
    "\n",
    "        task_acc_mean, task_acc_std = np.mean(task_accs), np.std(task_accs)\n",
    "        experiment_variables[\"task_accuracies\"].append((task_acc_mean, task_acc_std))\n",
    "        print(f\"\\tTest task accuracy: {task_acc_mean:.4f}  {task_acc_std:.4f}\")\n",
    "\n",
    "        concept_aucs_mean = np.mean(np.stack(c_aucs, axis=0), axis=0)\n",
    "        concept_aucs_std = np.std(np.stack(c_aucs, axis=0), axis=0)\n",
    "        experiment_variables[\"concept_aucs\"].append((concept_aucs_mean, concept_aucs_std))\n",
    "        print(f\"\\tConcept AUCS:\")\n",
    "        line = \"\\t\\t\"\n",
    "        for i in range(concept_aucs_mean.shape[0]):\n",
    "            line += f'{concept_aucs_mean[i]:.4f}  {concept_aucs_std[i]:.4f}    '\n",
    "        print(line)\n",
    "\n",
    "\n",
    "        task_auc_mean, task_auc_std = np.mean(aucs), np.std(aucs)\n",
    "        experiment_variables[\"task_aucs\"].append((task_auc_mean, task_auc_std))\n",
    "        print(f\"\\tTest task AUC: {task_auc_mean:.4f}  {task_auc_std:.4f}\")\n",
    "\n",
    "        purity_mats = np.stack(purity_mats, axis=0)\n",
    "        purity_mat_mean = np.mean(purity_mats, axis=0)\n",
    "        purity_mat_std = np.std(purity_mats, axis=0)\n",
    "        print(\"\\tPurity matrix:\")\n",
    "        for i in range(purity_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(purity_mat_mean.shape[1]):\n",
    "                line += f'{purity_mat_mean[i, j]:.4f}  {purity_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "        experiment_variables[\"purity_matrices\"].append((purity_mat_mean, purity_mat_std))\n",
    "        \n",
    "        repr_purity_mats = np.stack(repr_purity_mats, axis=0)\n",
    "        repr_purity_mat_mean = np.mean(repr_purity_mats, axis=0)\n",
    "        repr_purity_mat_std = np.std(repr_purity_mats, axis=0)\n",
    "        print(\"\\tRepresentation Purity matrix:\")\n",
    "        for i in range(repr_purity_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(repr_purity_mat_mean.shape[1]):\n",
    "                line += f'{repr_purity_mat_mean[i, j]:.4f}  {repr_purity_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "        experiment_variables[\"repr_purity_matrices\"].append((repr_purity_mat_mean, repr_purity_mat_std))\n",
    "        \n",
    "        similarities = np.stack(similarities, axis=0)\n",
    "        similarities_mean = np.mean(similarities, axis=0)\n",
    "        similarities_std = np.std(similarities, axis=0)\n",
    "        print(\"\\tSimilarity ratio matrix:\")\n",
    "        for i in range(similarities_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(similarities_mean.shape[1]):\n",
    "                line += f'{similarities_mean[i, j]:.4f}  {similarities_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "        experiment_variables[\"similarity_ratio_matrices\"].append(\n",
    "            (similarities_mean, similarities_std)\n",
    "        )\n",
    "        \n",
    "        if experiment_config.get(\"heatmap_display\"):\n",
    "            fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "            im, cbar = utils.heatmap(\n",
    "                similarities_mean,\n",
    "                [f\"{_LATEX_SYMBOL}c_{i}{_LATEX_SYMBOL}\" for i in range(len(concept_groups))],\n",
    "                [f\"{_LATEX_SYMBOL}c_{i}{_LATEX_SYMBOL}\" for i in range(len(concept_groups))],\n",
    "                ax=ax,\n",
    "                cmap=\"magma\",\n",
    "                cbarlabel=f\"Similarity Ratio\",\n",
    "                vmin=0,\n",
    "                vmax=1,\n",
    "            )\n",
    "            texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "            fig.tight_layout()\n",
    "\n",
    "            fig.suptitle(f\"Mean Concept Axis Separability\", fontsize=25)\n",
    "            fig.subplots_adjust(top=0.85)\n",
    "            plt.show()\n",
    "        \n",
    "        \n",
    "        correlations = np.stack(correlations, axis=0)\n",
    "        correlations_mean = np.mean(correlations, axis=0)\n",
    "        correlations_std = np.std(correlations, axis=0)\n",
    "        print(\"\\tCorrelation ratio matrix:\")\n",
    "        for i in range(len(concept_groups)):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(len(concept_groups)):\n",
    "                line += f'{correlations_mean[i, j]:.4f}  {correlations_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "        experiment_variables[\"correlation_matrices\"].append(\n",
    "            (correlations_mean, correlations_std)\n",
    "        )\n",
    "        if experiment_config.get(\"heatmap_display\"):\n",
    "            fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "            im, cbar = utils.heatmap(\n",
    "                np.abs(correlations_mean[:len(concept_groups), :len(concept_groups)]),\n",
    "                [f\"{_LATEX_SYMBOL}f_{i}{_LATEX_SYMBOL}\" for i in range(len(concept_groups))],\n",
    "                [f\"{_LATEX_SYMBOL}f_{i}{_LATEX_SYMBOL}\" for i in range(len(concept_groups))],\n",
    "                ax=ax,\n",
    "                cmap=\"magma\",\n",
    "                cbarlabel=f\"Mean Correlation Coef\",\n",
    "                vmin=0,\n",
    "                vmax=1,\n",
    "            )\n",
    "            texts = utils.annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "            fig.tight_layout()\n",
    "\n",
    "            fig.suptitle(f\"Latent Dimension Correlation\", fontsize=25)\n",
    "            fig.subplots_adjust(top=0.85)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        oracle_mats = np.stack(oracle_mats, axis=0)\n",
    "        oracle_mat_mean = np.mean(oracle_mats, axis=0)\n",
    "        oracle_mat_std = np.std(oracle_mats, axis=0)\n",
    "        print(\"\\tOracle matrix:\")\n",
    "        for i in range(oracle_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(oracle_mat_mean.shape[1]):\n",
    "                line += f'{oracle_mat_mean[i, j]:.4f}  {oracle_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"oracle_matrices\"].append((oracle_mat_mean, oracle_mat_std))\n",
    "\n",
    "        purity_mean, purity_std = np.mean(purities), np.std(purities)\n",
    "        experiment_variables[\"purity_scores\"].append((purity_mean, purity_std))\n",
    "        print(f\"\\tPurity score: {purity_mean:.4f}  {purity_std:.4f}\")\n",
    "        \n",
    "        repr_purity_mean, repr_purity_std = np.mean(repr_purities), np.std(repr_purities)\n",
    "        experiment_variables[\"repr_purity_scores\"].append((repr_purity_mean, repr_purity_std))\n",
    "        print(f\"\\tRepresentation Purity score: {repr_purity_mean:.4f}  {repr_purity_std:.4f}\")\n",
    "\n",
    "        non_oracle_purity_mean, non_oracle_purity_std = np.mean(non_oracle_purities), np.std(non_oracle_purities)\n",
    "        experiment_variables[\"non_oracle_purity_scores\"].append((non_oracle_purity_mean, non_oracle_purity_std))\n",
    "        print(f\"\\tNon-oracle purity score: {non_oracle_purity_mean:.4f}  {non_oracle_purity_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables\n",
    "\n",
    "# HACK: deserialization messes up with custome methods so reusing this here\n",
    "def concept_scores(\n",
    "    self,\n",
    "    inputs,\n",
    "    aggregator='max_pool_mean',\n",
    "    concept_indices=None,\n",
    "    data_format=\"channels_last\",\n",
    "):\n",
    "    outputs = self(inputs, training=False)\n",
    "    if len(tf.shape(outputs)) == 2:\n",
    "        # Then the scores are already computed by our forward pass\n",
    "        scores = outputs\n",
    "    else:\n",
    "        if data_format == \"channels_last\":\n",
    "            # Then we will transpose to make things simpler so that\n",
    "            # downstream we can always assume it is channels first\n",
    "            # NHWC -> NCHW\n",
    "            outputs = tf.transpose(\n",
    "                outputs,\n",
    "                perm=[0, 3, 1, 2],\n",
    "            )\n",
    "\n",
    "        # Else, we need to do some aggregation\n",
    "        if aggregator == 'mean':\n",
    "            # Compute the mean over all channels\n",
    "            scores = tf.math.reduce_mean(outputs, axis=[2, 3])\n",
    "        elif aggregator == 'max_pool_mean':\n",
    "            # First downsample using a max pool and then continue with\n",
    "            # a mean\n",
    "            window_size = min(\n",
    "                2,\n",
    "                outputs.shape[-1],\n",
    "                outputs.shape[-2],\n",
    "            )\n",
    "            scores = tf.nn.max_pool(\n",
    "                outputs,\n",
    "                ksize=window_size,\n",
    "                strides=window_size,\n",
    "                padding=\"SAME\",\n",
    "                data_format=\"NCHW\",\n",
    "            )\n",
    "            scores = tf.math.reduce_mean(scores, axis=[2, 3])\n",
    "        elif aggregator == 'max':\n",
    "            # Simply select the maximum value across a given channel\n",
    "            scores = tf.math.reduce_max(outputs, axis=[2, 3])\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported aggregator {aggregator}.')\n",
    "\n",
    "    if concept_indices is not None:\n",
    "        return scores[:, concept_indices]\n",
    "    return scores\n",
    "\n",
    "def cw_bottleneck_predict_experiment_loop(\n",
    "    experiment_config,\n",
    "    datasets,\n",
    "    load_from_cache=False,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    experiment_variables = dict(\n",
    "        latent_predictive_accuracies=[],\n",
    "        latent_predictive_aucs=[],\n",
    "        latent_feature_predictive_accuracies=[],\n",
    "        latent_feature_predictive_aucs=[],\n",
    "    )\n",
    "\n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                print(\"Could not find\", f'\"{file_name}_means.npz\" or \"{file_name}_stds.npz\" in cache.')\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(datasets):\n",
    "                print(\"Found\", len(loaded_means), \"means for variable\", var_name, \"vs\", len(datasets), \"datasets\")\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)} ({file_name}).'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            print(experiment_variables)\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    for (ds_name, (x_train, y_train, c_train), (x_test, y_test, c_test)) in datasets[start_ind:]:\n",
    "        print(\"Training with dataset\", ds_name)\n",
    "        task_accs = []\n",
    "        aucs = []\n",
    "        feat_task_accs = []\n",
    "        feat_aucs = []\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']}\")\n",
    "            complete_model = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/end_to_end_model_{ds_name}_trial_{trial}\"\n",
    "                ),\n",
    "#                 custom_objects={\"ConceptWhiteningLayer\": CW.ConceptWhiteningLayer},\n",
    "            )\n",
    "            cw_output_model = tf.keras.Model(\n",
    "                complete_model.inputs,\n",
    "                [complete_model.layers[experiment_config[\"cw_layer\"]].output],\n",
    "                name=\"cw_output_model\",\n",
    "            )\n",
    "            \n",
    "            feature_predictive_decoder = construct_decoder(\n",
    "                units=experiment_config[\"latent_decoder_units\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "            )\n",
    "            \n",
    "            feature_predictive_decoder.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(experiment_config[\"learning_rate\"]),\n",
    "                loss=(\n",
    "                    tf.keras.losses.BinaryCrossentropy(from_logits=True) if (experiment_config[\"num_outputs\"] <= 2)\n",
    "                    else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "                ),\n",
    "                metrics=[\n",
    "                    \"binary_accuracy\" if (experiment_config[\"num_outputs\"] <= 2)\n",
    "                    else tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            print(\"\\tTraining full representation model\")\n",
    "            train_codes = cw_output_model(x_train).numpy()\n",
    "            print(\"train_codes.shape =\", train_codes.shape)\n",
    "            print(\"y_train.shape =\", y_train.shape)\n",
    "            test_codes = cw_output_model(x_test).numpy()\n",
    "            feature_predictive_decoder.fit(\n",
    "                x=train_codes,\n",
    "                y=y_train,\n",
    "                epochs=experiment_config[\"predictor_max_epochs\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                verbose=verbosity,\n",
    "            )\n",
    "            \n",
    "            \n",
    "            print(\"\\tEvaluating feature model\")\n",
    "            test_result = feature_predictive_decoder.evaluate(\n",
    "                test_codes,\n",
    "                y_test,\n",
    "                verbose=0,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            feat_task_accs.append(\n",
    "                test_result['sparse_top_k_categorical_accuracy']\n",
    "                if experiment_config['num_outputs'] > 1 else\n",
    "                test_result['binary_accuracy']\n",
    "            )\n",
    "\n",
    "            if experiment_config['num_outputs'] > 1:\n",
    "                # Then lets apply a softmax activation over all the probability\n",
    "                # classes\n",
    "                preds = scipy.special.softmax(\n",
    "                    feature_predictive_decoder.predict(test_codes),\n",
    "                    axis=-1,\n",
    "                )\n",
    "\n",
    "                # And select just the labels that are in fact being used\n",
    "                feat_aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    y_test,\n",
    "                    preds,\n",
    "                    multi_class='ovo',\n",
    "                ))\n",
    "            else:\n",
    "                feat_aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    y_test,\n",
    "                    feature_predictive_decoder.predict(test_codes),\n",
    "                ))\n",
    "            \n",
    "            print(\n",
    "                f\"\\t\\tFeature Test auc = {feat_aucs[-1]:.4f}, \"\n",
    "                f\"feature task accuracy = {feat_task_accs[-1]:.4f}\"\n",
    "            )\n",
    "            \n",
    "            \n",
    "            encoder_model = tf.keras.Model(\n",
    "                complete_model.inputs,\n",
    "                [complete_model.layers[experiment_config[\"cw_layer\"] - 1].output],\n",
    "                name=\"cw_output_model\",\n",
    "            )\n",
    "            \n",
    "            score_predictive_decoder = construct_decoder(\n",
    "                units=experiment_config[\"latent_decoder_units\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "            )\n",
    "            \n",
    "            score_predictive_decoder.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(experiment_config[\"learning_rate\"]),\n",
    "                loss=(\n",
    "                    tf.keras.losses.BinaryCrossentropy(from_logits=True) if (experiment_config[\"num_outputs\"] <= 2)\n",
    "                    else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "                ),\n",
    "                metrics=[\n",
    "                    \"binary_accuracy\" if (experiment_config[\"num_outputs\"] <= 2)\n",
    "                    else tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            print(\"\\tTraining score model\")\n",
    "            score_train_codes = concept_scores(\n",
    "                cw_output_model.layers[-1],\n",
    "                encoder_model(x_train),\n",
    "                aggregator=experiment_config['aggregator'],\n",
    "            ).numpy()[:, list(range(experiment_config[\"num_concepts\"]))]\n",
    "            score_test_codes = concept_scores(\n",
    "                cw_output_model.layers[-1],\n",
    "                encoder_model(x_test),\n",
    "                aggregator=experiment_config['aggregator'],\n",
    "            ).numpy()[:, list(range(experiment_config[\"num_concepts\"]))]\n",
    "            score_predictive_decoder.fit(\n",
    "                x=score_train_codes,\n",
    "                y=y_train,\n",
    "                epochs=experiment_config[\"predictor_max_epochs\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                verbose=verbosity,\n",
    "            )\n",
    "            \n",
    "            \n",
    "            print(\"\\tEvaluating score model\")\n",
    "            test_result = score_predictive_decoder.evaluate(\n",
    "                score_test_codes,\n",
    "                y_test,\n",
    "                verbose=0,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            task_accs.append(\n",
    "                test_result['sparse_top_k_categorical_accuracy']\n",
    "                if experiment_config['num_outputs'] > 1 else\n",
    "                test_result['binary_accuracy']\n",
    "            )\n",
    "\n",
    "            if experiment_config['num_outputs'] > 1:\n",
    "                # Then lets apply a softmax activation over all the probability\n",
    "                # classes\n",
    "                preds = scipy.special.softmax(\n",
    "                    score_predictive_decoder.predict(score_test_codes),\n",
    "                    axis=-1,\n",
    "                )\n",
    "\n",
    "                # And select just the labels that are in fact being used\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    y_test,\n",
    "                    preds,\n",
    "                    multi_class='ovo',\n",
    "                ))\n",
    "            else:\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    y_test,\n",
    "                    score_predictive_decoder.predict(score_test_codes),\n",
    "                ))\n",
    "            \n",
    "            print(\n",
    "                f\"\\t\\tFeature Test auc = {aucs[-1]:.4f}, \"\n",
    "                f\"feature task accuracy = {task_accs[-1]:.4f}\"\n",
    "            )\n",
    "            \n",
    "            print(\"\\t\\tDone with trial\", trial + 1)\n",
    "\n",
    "        task_acc_mean, task_acc_std = np.mean(task_accs), np.std(task_accs)\n",
    "        experiment_variables[\"latent_predictive_accuracies\"].append((task_acc_mean, task_acc_std))\n",
    "        print(f\"\\tTest task accuracy: {task_acc_mean:.4f}  {task_acc_std:.4f}\")\n",
    "\n",
    "        task_auc_mean, task_auc_std = np.mean(aucs), np.std(aucs)\n",
    "        experiment_variables[\"latent_predictive_aucs\"].append((task_auc_mean, task_auc_std))\n",
    "        print(f\"\\tTest task AUC: {task_auc_mean:.4f}  {task_auc_std:.4f}\")\n",
    "        \n",
    "        feat_task_acc_mean, feat_task_acc_std = np.mean(feat_task_accs), np.std(feat_task_accs)\n",
    "        experiment_variables[\"latent_feature_predictive_accuracies\"].append((feat_task_acc_mean, feat_task_acc_std))\n",
    "        print(f\"\\tTest feature task accuracy: {feat_task_acc_mean:.4f}  {feat_task_acc_std:.4f}\")\n",
    "\n",
    "        feat_task_auc_mean, feat_task_auc_std = np.mean(feat_aucs), np.std(feat_aucs)\n",
    "        experiment_variables[\"latent_feature_predictive_aucs\"].append((feat_task_auc_mean, feat_task_auc_std))\n",
    "        print(f\"\\tTest feature task AUC: {feat_task_auc_mean:.4f}  {feat_task_auc_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables\n",
    "\n",
    "def cw_bottleneck_concept_predict_experiment_loop(\n",
    "    experiment_config,\n",
    "    datasets,\n",
    "    load_from_cache=False,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    experiment_variables = dict(\n",
    "        latent_avg_concept_predictive_accuracies=[],\n",
    "        latent_avg_concept_predictive_aucs=[],\n",
    "        latent_feature_avg_concept_predictive_accuracies=[],\n",
    "        latent_feature_avg_concept_predictive_aucs=[],\n",
    "    )\n",
    "\n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                print(\"Could not find\", f'\"{file_name}_means.npz\" or \"{file_name}_stds.npz\" in cache.')\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(datasets):\n",
    "                print(\"Found\", len(loaded_means), \"means for variable\", var_name, \"vs\", len(datasets), \"datasets\")\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)} ({file_name}).'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            print(experiment_variables)\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    for (ds_name, (x_train, y_train, c_train), (x_test, y_test, c_test)) in datasets[start_ind:]:\n",
    "        print(\"Training with dataset\", ds_name)\n",
    "        avg_concept_accs = []\n",
    "        avg_concept_aucs = []\n",
    "        feat_avg_concept_accs = []\n",
    "        feat_avg_concept_aucs = []\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']}\")\n",
    "            complete_model = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/end_to_end_model_{ds_name}_trial_{trial}\"\n",
    "                ),\n",
    "#                 custom_objects={\"ConceptWhiteningLayer\": CW.ConceptWhiteningLayer},\n",
    "            )\n",
    "            cw_output_model = tf.keras.Model(\n",
    "                complete_model.inputs,\n",
    "                [complete_model.layers[experiment_config[\"cw_layer\"]].output],\n",
    "                name=\"cw_output_model\",\n",
    "            )\n",
    "            \n",
    "            current_accs = []\n",
    "            current_aucs = []\n",
    "            current_feat_accs = []\n",
    "            current_feat_aucs = []\n",
    "            for concept_idx in range(experiment_config[\"num_concepts\"]):\n",
    "                feature_predictive_decoder = construct_decoder(\n",
    "                    units=experiment_config[\"latent_decoder_units\"],\n",
    "                    num_outputs=1,\n",
    "                )\n",
    "\n",
    "                feature_predictive_decoder.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(experiment_config[\"learning_rate\"]),\n",
    "                    loss=(\n",
    "                        tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "                    ),\n",
    "                    metrics=[\n",
    "                        \"binary_accuracy\"\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "                print(\"\\tTraining full representation model for concept\", concept_idx)\n",
    "                train_codes = cw_output_model(x_train).numpy()\n",
    "                test_codes = cw_output_model(x_test).numpy()\n",
    "                feature_predictive_decoder.fit(\n",
    "                    x=train_codes,\n",
    "                    y=c_train[:, concept_idx],\n",
    "                    epochs=experiment_config[\"predictor_max_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "\n",
    "\n",
    "                print(\"\\t\\tEvaluating feature model\")\n",
    "                test_result = feature_predictive_decoder.evaluate(\n",
    "                    test_codes,\n",
    "                    c_test[:, concept_idx],\n",
    "                    verbose=0,\n",
    "                    return_dict=True,\n",
    "                )\n",
    "                current_feat_accs.append(\n",
    "                    test_result['binary_accuracy']\n",
    "                )\n",
    "\n",
    "\n",
    "                current_feat_aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    c_test[:, concept_idx],\n",
    "                    feature_predictive_decoder.predict(test_codes),\n",
    "                ))\n",
    "\n",
    "                print(\n",
    "                    f\"\\t\\tFeature Test concept auc = {current_feat_aucs[-1]:.4f}, \"\n",
    "                    f\"feature concept accuracy = {current_feat_accs[-1]:.4f}\"\n",
    "                )\n",
    "\n",
    "\n",
    "                encoder_model = tf.keras.Model(\n",
    "                    complete_model.inputs,\n",
    "                    [complete_model.layers[experiment_config[\"cw_layer\"] - 1].output],\n",
    "                    name=\"cw_output_model\",\n",
    "                )\n",
    "\n",
    "                score_predictive_decoder = construct_decoder(\n",
    "                    units=experiment_config[\"latent_decoder_units\"],\n",
    "                    num_outputs=1,\n",
    "                )\n",
    "\n",
    "                score_predictive_decoder.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(experiment_config[\"learning_rate\"]),\n",
    "                    loss=(\n",
    "                        tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "                    ),\n",
    "                    metrics=[\n",
    "                        \"binary_accuracy\"\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "                print(\"\\tTraining score model for concept\", concept_idx)\n",
    "                score_train_codes = concept_scores(\n",
    "                    cw_output_model.layers[-1],\n",
    "                    encoder_model(x_train),\n",
    "                    aggregator=experiment_config['aggregator'],\n",
    "                ).numpy()[:, list(range(experiment_config[\"num_concepts\"]))]\n",
    "                score_test_codes = concept_scores(\n",
    "                    cw_output_model.layers[-1],\n",
    "                    encoder_model(x_test),\n",
    "                    aggregator=experiment_config['aggregator'],\n",
    "                ).numpy()[:, list(range(experiment_config[\"num_concepts\"]))]\n",
    "                score_predictive_decoder.fit(\n",
    "                    x=score_train_codes,\n",
    "                    y=c_train[:, concept_idx],\n",
    "                    epochs=experiment_config[\"predictor_max_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "\n",
    "\n",
    "                print(\"\\t\\tEvaluating score model\")\n",
    "                test_result = score_predictive_decoder.evaluate(\n",
    "                    score_test_codes,\n",
    "                    c_test[:, concept_idx],\n",
    "                    verbose=0,\n",
    "                    return_dict=True,\n",
    "                )\n",
    "                current_accs.append(\n",
    "                    test_result['binary_accuracy']\n",
    "                )\n",
    "                \n",
    "                current_aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    c_test[:, concept_idx],\n",
    "                    score_predictive_decoder.predict(score_test_codes),\n",
    "                ))\n",
    "\n",
    "                print(\n",
    "                    f\"\\t\\tTest concept auc = {current_aucs[-1]:.4f}, \"\n",
    "                    f\"feature concept accuracy = {current_accs[-1]:.4f}\"\n",
    "                )\n",
    "            \n",
    "            avg_concept_accs.append(np.mean(current_accs))\n",
    "            avg_concept_aucs.append(np.mean(current_aucs))\n",
    "            feat_avg_concept_accs.append(np.mean(current_feat_accs))\n",
    "            feat_avg_concept_aucs.append(np.mean(current_feat_aucs))\n",
    "            print(\"\\tDone with trial\", trial + 1)\n",
    "\n",
    "        avg_concept_acc_mean, avg_concept_acc_std = np.mean(avg_concept_accs), np.std(avg_concept_accs)\n",
    "        experiment_variables[\"latent_avg_concept_predictive_accuracies\"].append((avg_concept_acc_mean, avg_concept_acc_std))\n",
    "        print(f\"\\tTest avg concept accuracy: {avg_concept_acc_mean:.4f}  {avg_concept_acc_std:.4f}\")\n",
    "\n",
    "        avg_concept_auc_mean, avg_concept_auc_std = np.mean(avg_concept_aucs), np.std(avg_concept_aucs)\n",
    "        experiment_variables[\"latent_avg_concept_predictive_aucs\"].append((avg_concept_auc_mean, avg_concept_auc_std))\n",
    "        print(f\"\\tTest avg concept AUC: {avg_concept_auc_mean:.4f}  {avg_concept_auc_std:.4f}\")\n",
    "        \n",
    "        feat_avg_concept_acc_mean, feat_avg_concept_acc_std = np.mean(feat_avg_concept_accs), np.std(feat_avg_concept_accs)\n",
    "        experiment_variables[\"latent_feature_avg_concept_predictive_accuracies\"].append((feat_avg_concept_acc_mean, feat_avg_concept_acc_std))\n",
    "        print(f\"\\tTest feature avg concept accuracy: {feat_avg_concept_acc_mean:.4f}  {feat_avg_concept_acc_std:.4f}\")\n",
    "\n",
    "        feat_avg_concept_auc_mean, feat_avg_concept_auc_std = np.mean(feat_avg_concept_aucs), np.std(feat_avg_concept_aucs)\n",
    "        experiment_variables[\"latent_feature_avg_concept_predictive_aucs\"].append((feat_avg_concept_auc_mean, feat_avg_concept_auc_std))\n",
    "        print(f\"\\tTest feature avg concept AUC: {feat_avg_concept_auc_mean:.4f}  {feat_avg_concept_auc_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(CW)\n",
    "reload(leakage)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "cw_binary_balanced_multiclass_experiment_config = dict(\n",
    "    batch_size=128,\n",
    "    max_epochs=50,\n",
    "    pre_train_epochs=50,\n",
    "    post_train_epochs=0,\n",
    "    cw_train_freq=20,\n",
    "    cw_train_batch_steps=20,\n",
    "    cw_train_iterations=1,\n",
    "    exclusive_concepts=False,\n",
    "    exclusive_test_concepts=False,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    filter_groups=[\n",
    "        [(8, (7, 7), False)],\n",
    "        [(16, (5, 5), False)],\n",
    "        [(32, (3, 3), False)],\n",
    "        [(64, (3, 3), True)],\n",
    "    ],\n",
    "    units=[64, 64, 64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1]))\n",
    "        if len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2 else 1\n",
    "    ),\n",
    "    \n",
    "    latent_decoder_units=[64, 64],\n",
    "    predictor_max_epochs=100,\n",
    "    \n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(RESULTS_DIR, \"cw/balanced_multiclass_tasks_purity_max_pool_mean\"),\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    num_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    cw_layer=14,\n",
    "    aggregator='max_pool_mean',\n",
    "    activation_mode='max_pool_mean',\n",
    "    concept_auc_freq=0,\n",
    "    holdout_fraction=0.1,\n",
    "    heatmap_display=True,\n",
    "    T=5,\n",
    "    eps=1e-5,\n",
    "    momentum=0.9,\n",
    "    initial_tau=1000,\n",
    "    initial_beta=1e8,\n",
    "    initial_alpha=0,\n",
    "    drop_prob=0,    \n",
    ")\n",
    "\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(cw_binary_balanced_multiclass_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "cw_binary_balanced_multiclass_figure_dir = os.path.join(cw_binary_balanced_multiclass_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(cw_binary_balanced_multiclass_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "cw_binary_balanced_multiclass_results = cw_experiment_loop(\n",
    "    cw_binary_balanced_multiclass_experiment_config,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "    load_from_cache=True,\n",
    "    oracle_matrix_cache=balanced_oracle_matrix_cache,\n",
    ")\n",
    "print(\"task_accuracies:\", cw_binary_balanced_multiclass_results[\"task_accuracies\"])\n",
    "print(\"purity_scores:\", cw_binary_balanced_multiclass_results[\"purity_scores\"])\n",
    "print(\"concept_aucs:\", cw_binary_balanced_multiclass_results[\"concept_aucs\"])\n",
    "print(\"task_aucs:\", cw_binary_balanced_multiclass_results[\"task_aucs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_binary_balanced_multiclass_results.update(cw_bottleneck_predict_experiment_loop(\n",
    "    cw_binary_balanced_multiclass_experiment_config,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "    load_from_cache=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_binary_balanced_multiclass_results.update(cw_bottleneck_concept_predict_experiment_loop(\n",
    "    cw_binary_balanced_multiclass_experiment_config,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "    load_from_cache=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(CW)\n",
    "reload(leakage)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "cw_mean_binary_balanced_multiclass_experiment_config = dict(\n",
    "    batch_size=128,\n",
    "    max_epochs=50,\n",
    "    pre_train_epochs=50,\n",
    "    post_train_epochs=0,\n",
    "    cw_train_freq=20,\n",
    "    cw_train_batch_steps=20,\n",
    "    cw_train_iterations=1,\n",
    "    exclusive_concepts=False,\n",
    "    exclusive_test_concepts=False,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    filter_groups=[\n",
    "        [(8, (7, 7), False)],\n",
    "        [(16, (5, 5), False)],\n",
    "        [(32, (3, 3), False)],\n",
    "        [(64, (3, 3), True)],\n",
    "    ],\n",
    "    units=[64, 64, 64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1]))\n",
    "        if len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2 else 1\n",
    "    ),\n",
    "    \n",
    "    latent_decoder_units=[64, 64],\n",
    "    predictor_max_epochs=100,\n",
    "    \n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(RESULTS_DIR, \"cw/balanced_multiclass_tasks_purity_mean\"),\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    num_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    cw_layer=14,\n",
    "    aggregator='mean',\n",
    "    activation_mode='mean',\n",
    "    concept_auc_freq=0,\n",
    "    holdout_fraction=0.1,\n",
    "    heatmap_display=True,\n",
    "    T=5,\n",
    "    eps=1e-5,\n",
    "    momentum=0.9,\n",
    "    initial_tau=1000,\n",
    "    initial_beta=1e8,\n",
    "    initial_alpha=0,\n",
    "    drop_prob=0,    \n",
    ")\n",
    "\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(cw_mean_binary_balanced_multiclass_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "cw_mean_binary_balanced_multiclass_figure_dir = os.path.join(cw_mean_binary_balanced_multiclass_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(cw_mean_binary_balanced_multiclass_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "cw_mean_binary_balanced_multiclass_results = cw_experiment_loop(\n",
    "    cw_mean_binary_balanced_multiclass_experiment_config,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "    load_from_cache=True,\n",
    "    oracle_matrix_cache=balanced_oracle_matrix_cache,\n",
    ")\n",
    "print(\"task_accuracies:\", cw_mean_binary_balanced_multiclass_results[\"task_accuracies\"])\n",
    "print(\"purity_scores:\", cw_mean_binary_balanced_multiclass_results[\"purity_scores\"])\n",
    "print(\"concept_aucs:\", cw_mean_binary_balanced_multiclass_results[\"concept_aucs\"])\n",
    "print(\"task_aucs:\", cw_mean_binary_balanced_multiclass_results[\"task_aucs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_mean_binary_balanced_multiclass_results.update(cw_bottleneck_predict_experiment_loop(\n",
    "    cw_mean_binary_balanced_multiclass_experiment_config,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "    load_from_cache=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_mean_binary_balanced_multiclass_results.update(cw_bottleneck_concept_predict_experiment_loop(\n",
    "    cw_mean_binary_balanced_multiclass_experiment_config,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "    load_from_cache=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada-ML-VAE Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weakly Labelled Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weak_sample_pairs(x_train, y_train, c_train, size):\n",
    "    result_samples = []\n",
    "    result_labels = []\n",
    "    result_concepts = []\n",
    "    used_cache = set()\n",
    "    num_samples = x_train.shape[0]\n",
    "    while (len(used_cache) < (num_samples * num_samples)) and (\n",
    "        len(result_samples) < size\n",
    "    ):\n",
    "        next_to_try_1 = np.random.choice(x_train.shape[0], 1)[0]\n",
    "        next_to_try_2 = np.random.choice(x_train.shape[0], 1)[0]\n",
    "        if (next_to_try_1, next_to_try_2) in used_cache:\n",
    "            continue\n",
    "        num_equal = np.sum(c_train[next_to_try_1, :] == c_train[next_to_try_2, :])\n",
    "        if (num_equal == 0) or (num_equal == c_train.shape[-1]):\n",
    "            # If they are all different or all the same, then we cannot use this\n",
    "            used_cache.add((next_to_try_1, next_to_try_2))\n",
    "        else:\n",
    "            # Then this is something we can use\n",
    "            result_samples.append((x_train[next_to_try_1], x_train[next_to_try_2]))\n",
    "            result_labels.append((y_train[next_to_try_1], y_train[next_to_try_2]))\n",
    "            result_concepts.append((c_train[next_to_try_1], c_train[next_to_try_2]))\n",
    "            used_cache.add((next_to_try_1, next_to_try_2))\n",
    "    \n",
    "    return result_samples, result_labels, result_concepts\n",
    "\n",
    "def _join_pair_samples(pairs, axis=1):\n",
    "    return np.concatenate(\n",
    "        [\n",
    "            np.stack(\n",
    "                list(map(lambda x: x[0], pairs)),\n",
    "                axis=0,\n",
    "            ),\n",
    "            np.stack(\n",
    "                list(map(lambda x: x[1], pairs)),\n",
    "                axis=0,\n",
    "            ),\n",
    "        ],\n",
    "        axis=axis,\n",
    "    )\n",
    "\n",
    "def generate_wvae_dataset(\n",
    "    train_data,\n",
    "    test_data,\n",
    "    train_size,\n",
    "    test_size,\n",
    "):\n",
    "    train_sample_pairs, train_label_pairs, train_concept_pairs = generate_weak_sample_pairs(\n",
    "        train_data[0],\n",
    "        train_data[1],\n",
    "        train_data[2],\n",
    "        train_size,\n",
    "    )\n",
    "    train_pairs = _join_pair_samples(train_sample_pairs)\n",
    "    train_label_pairs = _join_pair_samples(train_label_pairs, axis=0)\n",
    "    train_concept_pairs = _join_pair_samples(train_concept_pairs)\n",
    "    \n",
    "    test_sample_pairs, test_label_pairs, test_concept_pairs = generate_weak_sample_pairs(\n",
    "        test_data[0],\n",
    "        test_data[1],\n",
    "        test_data[2],\n",
    "        test_size,\n",
    "    )\n",
    "    test_pairs = _join_pair_samples(test_sample_pairs)\n",
    "    test_label_pairs = _join_pair_samples(test_label_pairs, axis=0)\n",
    "    test_concept_pairs = _join_pair_samples(test_concept_pairs)\n",
    "    return train_pairs, train_label_pairs, train_concept_pairs, test_pairs, test_label_pairs, test_concept_pairs\n",
    "\n",
    "balanced_multiclass_wvae_datasets = [\n",
    "    generate_wvae_dataset(\n",
    "        train,\n",
    "        test,\n",
    "        int(train[0].shape[0])/1.5,\n",
    "        int(test[0].shape[0])/1.5,\n",
    "    ) for (train, test) in [\n",
    "        (balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.VAE.weak_vae as weak_vae\n",
    "import concepts_xai.methods.VAE.baseVAE as base_vae\n",
    "import concepts_xai.methods.VAE.losses as vae_losses\n",
    "reload(vae_losses)\n",
    "reload(weak_vae)\n",
    "\n",
    "def construct_vae_encoder(\n",
    "    input_shape,\n",
    "    filter_groups,\n",
    "    units,\n",
    "    latent_dims,\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    include_norm=False,\n",
    "    include_pool=False,\n",
    "):\n",
    "    encoder_inputs = tf.keras.Input(shape=input_shape)\n",
    "    encoder_compute_graph = encoder_inputs\n",
    "    \n",
    "    # Start with our convolutions\n",
    "    num_convs = 0\n",
    "    for filter_group in filter_groups:\n",
    "        for filter_args in filter_group:\n",
    "            if len(filter_args) == 2:\n",
    "                filter_args = (*filter_args, 1)\n",
    "            (num_filters, kernel_size, stride) = filter_args\n",
    "            encoder_compute_graph = tf.keras.layers.Conv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                strides=stride,\n",
    "                padding=\"SAME\",\n",
    "                activation=None if include_norm else \"relu\",\n",
    "                name=f'encoder_conv_{num_convs}',\n",
    "            )(encoder_compute_graph)\n",
    "            num_convs += 1\n",
    "            if include_norm:\n",
    "                encoder_compute_graph = tf.keras.layers.BatchNormalization()(\n",
    "                    encoder_compute_graph\n",
    "                )\n",
    "                encoder_compute_graph = tf.keras.activations.relu(encoder_compute_graph)\n",
    "        if include_pool:\n",
    "            # Then do a max pool here to control the parameter count of the model\n",
    "            # at the end of each group\n",
    "            encoder_compute_graph = tf.keras.layers.MaxPooling2D(\n",
    "                pool_size=max_pool_window,\n",
    "                strides=max_pool_stride,\n",
    "            )(\n",
    "                encoder_compute_graph\n",
    "            )\n",
    "    \n",
    "    # Flatten this guy\n",
    "    encoder_compute_graph = tf.keras.layers.Flatten()(encoder_compute_graph)\n",
    "    \n",
    "    # Add a dropout if requested\n",
    "    if drop_prob:\n",
    "        encoder_compute_graph = tf.keras.layers.Dropout(drop_prob)(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Finally, include the fully connected bottleneck here\n",
    "    for i, units in enumerate(units):\n",
    "        encoder_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(encoder_compute_graph)\n",
    "\n",
    "    mean = tf.keras.layers.Dense(latent_dims, activation=None, name=\"means\")(encoder_compute_graph)\n",
    "    log_var = tf.keras.layers.Dense(latent_dims, activation=None, name=\"log_var\")(encoder_compute_graph)\n",
    "    return tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        [mean, log_var],\n",
    "        name=\"encoder\",\n",
    "    )\n",
    "\n",
    "def construct_vae_decoder(\n",
    "    units,\n",
    "    output_shape,\n",
    "    latent_dims,\n",
    "):\n",
    "    \"\"\"CNN decoder architecture used in the 'Challenging Common Assumptions in the Unsupervised Learning\n",
    "       of Disentangled Representations' paper (https://arxiv.org/abs/1811.12359)\n",
    "\n",
    "       Note: model is uncompiled\n",
    "    \"\"\"\n",
    "\n",
    "    latent_inputs = tf.keras.Input(shape=(latent_dims,))\n",
    "    model_out = latent_inputs\n",
    "    for unit in units:\n",
    "        model_out = tf.keras.layers.Dense(\n",
    "            unit,\n",
    "            activation='relu',\n",
    "        )(model_out)\n",
    "    model_out = tf.keras.layers.Reshape([4, 4, 32])(model_out)\n",
    "\n",
    "    model_out = tf.keras.layers.Conv2DTranspose(\n",
    "        filters=64,\n",
    "        kernel_size=4,\n",
    "        strides=2,\n",
    "        activation='relu',\n",
    "        padding=\"same\"\n",
    "    )(model_out)\n",
    "\n",
    "    model_out = tf.keras.layers.Conv2DTranspose(\n",
    "        filters=32,\n",
    "        kernel_size=4,\n",
    "        strides=2,\n",
    "        activation='relu',\n",
    "        padding=\"same\"\n",
    "    )(model_out)\n",
    "\n",
    "    model_out = tf.keras.layers.Conv2DTranspose(\n",
    "        filters=32,\n",
    "        kernel_size=4,\n",
    "        strides=2,\n",
    "        activation='relu',\n",
    "        padding=\"same\",\n",
    "    )(model_out)\n",
    "\n",
    "    model_out = tf.keras.layers.Conv2DTranspose(\n",
    "        filters=output_shape[-1],\n",
    "        kernel_size=4,\n",
    "        strides=2,\n",
    "        padding=\"same\",\n",
    "        activation=None,\n",
    "    )(model_out)\n",
    "    model_out = tf.keras.layers.Reshape(output_shape)(model_out)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=latent_inputs,\n",
    "        outputs=[model_out],\n",
    "    )\n",
    "\n",
    "def construct_wvae(\n",
    "    input_shape,\n",
    "    latent_dims,\n",
    "    filter_groups,\n",
    "    encoder_units,\n",
    "    decoder_units,\n",
    "    drop_prob=0.5,\n",
    "    include_pool=False,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    learning_rate=1e-3,\n",
    "    beta=1.0,\n",
    "    vae_model=weak_vae.MLVaeArgmax,\n",
    "):\n",
    "    wvae_encoder = construct_vae_encoder(\n",
    "        input_shape=input_shape,\n",
    "        filter_groups=filter_groups,\n",
    "        units=encoder_units,\n",
    "        drop_prob=drop_prob,\n",
    "        include_pool=include_pool,\n",
    "        max_pool_window=max_pool_window,\n",
    "        max_pool_stride=max_pool_stride,\n",
    "        latent_dims=latent_dims,\n",
    "    )\n",
    "    wvae_decoder = construct_vae_decoder(\n",
    "        output_shape=input_shape,\n",
    "        units=decoder_units,\n",
    "        latent_dims=latent_dims,\n",
    "    )\n",
    "\n",
    "    wvae_model = vae_model(\n",
    "        encoder=wvae_encoder,\n",
    "        decoder=wvae_decoder,\n",
    "        loss_fn=vae_losses.bernoulli_fn_wrapper(),\n",
    "        beta=beta,\n",
    "    )\n",
    "    wvae_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    )\n",
    "    return wvae_model\n",
    "\n",
    "def construct_pretrained_wvae(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    learning_rate=1e-3,\n",
    "    beta=1.0,\n",
    "    vae_model=weak_vae.MLVaeArgmax,\n",
    "):\n",
    "    wvae_model = vae_model(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        loss_fn=vae_losses.bernoulli_fn_wrapper(),\n",
    "        beta=beta,\n",
    "    )\n",
    "    wvae_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    )\n",
    "    return wvae_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.evaluation.metrics.oracle as oracle\n",
    "import concepts_xai.methods.VAE.betaVAE as beta_vae\n",
    "\n",
    "############################################################################\n",
    "## Experiment loop\n",
    "############################################################################\n",
    "\n",
    "def construct_trivial_auc_mat(num_concepts):\n",
    "    result = np.ones((num_concepts, num_concepts), dtype=np.float32) * 0.5\n",
    "    return result + np.eye(num_concepts, dtype=np.float32) * 0.5\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def wvae_experiment_loop(\n",
    "    experiment_config,\n",
    "    datasets,\n",
    "    load_from_cache=False,\n",
    "    include_all_losses=True,\n",
    "    vae_model=weak_vae.MLVaeArgmax,\n",
    "    oracle_matrix_cache=None,\n",
    "    model_cache=None,\n",
    "    include_encoder_purity=False,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    if vae_model != beta_vae.BetaVAE:\n",
    "        split_fn = lambda x: x[:, :x.shape[1]//2, ...] if len(x.shape) > 1 else x[:x.shape[0]//2]\n",
    "    else:\n",
    "        split_fn = lambda x: x\n",
    "        \n",
    "    model_cache = model_cache or {}\n",
    "    oracle_matrix_cache = oracle_matrix_cache or None\n",
    "    experiment_variables = dict(\n",
    "        total_losses=[],\n",
    "        purity_scores=[],\n",
    "        non_oracle_purity_scores=[],\n",
    "        purity_matrices=[],\n",
    "        aligned_purity_matrices=[],\n",
    "        oracle_matrices=[],\n",
    "    )\n",
    "    if include_all_losses:\n",
    "        experiment_variables['elbo_losses'] = []\n",
    "        experiment_variables['reconstruction_losses'] = []\n",
    "    if include_encoder_purity:\n",
    "        experiment_variables['encoder_purity_scores'] = []\n",
    "        experiment_variables['encoder_purity_matrices'] = []\n",
    "        experiment_variables['encoder_aligned_purity_matrices'] = []\n",
    "\n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                print(\"Could not find\", f'\"{file_name}_means.npz\" or \"{file_name}_stds.npz\" in cache.')\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(datasets):\n",
    "                print(\"Found\", len(loaded_means), \"means for variable\", var_name, \"vs\", len(datasets), \"datasets\")\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)} ({file_name}).'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    utils.serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    for (ds_name, (x_train, _, c_train), (x_test, _, c_test)) in datasets[start_ind:]:\n",
    "        latent_dim = experiment_config['latent_dim']\n",
    "        print(\"Training with latent dimensions\", latent_dim, \"in dataset\", ds_name)\n",
    "        tot_losses = []\n",
    "        recon_losses = []\n",
    "        el_losses = []\n",
    "        purity_mats = []\n",
    "        aligned_purity_mats = []\n",
    "        purity_mats = []\n",
    "        oracle_mats = []\n",
    "        purities = []\n",
    "        non_oracle_purities = []\n",
    "        if include_encoder_purity:\n",
    "            encoder_purity_mats = []\n",
    "            encoder_aligned_purity_mats = []\n",
    "            encoder_purities = []\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']}\")\n",
    "            if (ds_name, trial) in model_cache:\n",
    "                print(\"Found trial\", trial, \"model for dataset\", ds_name, \"in model cache\")\n",
    "                print(\"\\tDeserializing it...\")\n",
    "                encoder_model_dir, decoder_model_dir = model_cache[(ds_name, trial)]\n",
    "                encoder = tf.keras.models.load_model(encoder_model_dir)\n",
    "                decoder = tf.keras.models.load_model(decoder_model_dir)\n",
    "                wvae_model = construct_pretrained_wvae(\n",
    "                    encoder=encoder,\n",
    "                    decoder=decoder,\n",
    "                    beta=experiment_config['beta'],\n",
    "                    vae_model=vae_model,\n",
    "                )\n",
    "            else:\n",
    "                # Time to actually construct and train the WVAE\n",
    "                wvae_model = construct_wvae(\n",
    "                    input_shape=experiment_config[\"input_shape\"],\n",
    "                    latent_dims=latent_dim,\n",
    "                    filter_groups=experiment_config[\"filter_groups\"],\n",
    "                    encoder_units=experiment_config[\"encoder_units\"],\n",
    "                    decoder_units=experiment_config[\"decoder_units\"],\n",
    "                    drop_prob=experiment_config['drop_prob'],\n",
    "                    max_pool_window=experiment_config['max_pool_window'],\n",
    "                    max_pool_stride=experiment_config['max_pool_stride'],\n",
    "                    include_pool=experiment_config['include_pool'],\n",
    "                    beta=experiment_config['beta'],\n",
    "                    vae_model=vae_model,\n",
    "                )\n",
    "\n",
    "                early_stopping_monitor = tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor=experiment_config.get(\n",
    "                        \"early_stop_metric\",\n",
    "                        \"val_loss\",\n",
    "                    ),\n",
    "                    min_delta=experiment_config[\"min_delta\"],\n",
    "                    patience=experiment_config[\"patience\"],\n",
    "                    restore_best_weights=True,\n",
    "                    verbose=2,\n",
    "                    mode=experiment_config.get(\n",
    "                        \"early_stop_mode\",\n",
    "                        \"min\",\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "                print(\"\\tVAE training...\")\n",
    "                epochs = 0\n",
    "                viz_freq = experiment_config.get(\n",
    "                    \"visualization_frequency\",\n",
    "                    experiment_config[\"max_epochs\"]\n",
    "                )\n",
    "                num_samples = experiment_config.get(\n",
    "                    \"visualization_samples\",\n",
    "                    4,\n",
    "                )\n",
    "                while epochs < experiment_config['max_epochs']:\n",
    "                    \n",
    "                    fig, axs = plt.subplots(1, num_samples, figsize=(12, 8))\n",
    "                    for j in range(num_samples):\n",
    "                        axs[j].imshow(\n",
    "                            x_train[np.random.randint(x_train.shape[0]), ...]\n",
    "                        )\n",
    "                        axs[j].get_xaxis().set_visible(False)\n",
    "                        axs[j].get_yaxis().set_visible(False)\n",
    "                    plt.title(f\"Random training sample\")\n",
    "                    plt.show()\n",
    "                    \n",
    "                    wvae_model.fit(\n",
    "                        x=x_train,\n",
    "                        epochs=min(viz_freq, experiment_config['max_epochs'] - epochs),\n",
    "                        batch_size=experiment_config[\"batch_size\"],\n",
    "                        callbacks=[\n",
    "                            early_stopping_monitor,\n",
    "                        ],\n",
    "                        validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                        verbose=verbosity,\n",
    "                    )\n",
    "\n",
    "                    epochs += viz_freq\n",
    "\n",
    "                    fig, axs = plt.subplots(1, num_samples, figsize=(12, 8))\n",
    "                    for j in range(num_samples):\n",
    "                        if experiment_config[\"input_shape\"][-1] > 1:\n",
    "                            axs[j].imshow(\n",
    "                                sigmoid(wvae_model.generate_random_sample()[0, :, :, :])\n",
    "                            )\n",
    "                        else:\n",
    "                            axs[j].imshow(\n",
    "                                wvae_model.generate_random_sample()[0, :, :, :] >= 0\n",
    "                            )\n",
    "                        axs[j].get_xaxis().set_visible(False)\n",
    "                        axs[j].get_yaxis().set_visible(False)\n",
    "                    plt.title(f\"Random sample at epoch {epochs}\")\n",
    "                    plt.show()\n",
    "\n",
    "                print(\"\\t\\tWVAE training completed\")\n",
    "            print(\"\\tSerializing model\")\n",
    "            wvae_model.encoder.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/{ds_name}_encoder_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            wvae_model.decoder.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/{ds_name}_decoder_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            print(\"\\tEvaluating model\")\n",
    "            test_result = wvae_model.evaluate(\n",
    "                x_test,\n",
    "                verbose=0,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            tot_losses.append(test_result['loss'])\n",
    "            recon_losses.append(test_result['reconstruction_loss'])\n",
    "            el_losses.append(test_result['elbo'])\n",
    "            \n",
    "            print(\n",
    "                f\"\\t\\tTest loss = {tot_losses[-1]:.4f}, \"\n",
    "                f\"test reconstruction loss = {recon_losses[-1]:.4f}, \"\n",
    "                f\"task elbo = {el_losses[-1]:.4f}\"\n",
    "            )\n",
    "\n",
    "            print(f\"\\t\\tComputing purity score...\")\n",
    "            latent_codes = wvae_model.sample_from_latent_distribution(\n",
    "                *wvae_model.encoder(split_fn(x_test))\n",
    "            ).numpy()\n",
    "            purity_score, (purity_mat, aligned_purity_mat), oracle_mat = oracle.oracle_impurity_score(\n",
    "                c_soft=latent_codes,\n",
    "                c_true=split_fn(c_test),\n",
    "                output_matrices=True,\n",
    "                alignment_function=oracle.max_alignment_matrix,\n",
    "                oracle_matrix=oracle_matrix_cache.get(ds_name),\n",
    "            )\n",
    "            \n",
    "            purity_mats.append(purity_mat)\n",
    "            aligned_purity_mats.append(aligned_purity_mat)\n",
    "            oracle_mats.append(oracle_mat)\n",
    "            purities.append(purity_score)\n",
    "            print(f\"\\t\\t\\tDone {purity_score:.4f}\")\n",
    "            print(\"\\t\\tPurity matrix:\")\n",
    "            print(purity_mat)\n",
    "            print(\"\\t\\tAligned purity matrix:\")\n",
    "            print(aligned_purity_mat)\n",
    "            \n",
    "\n",
    "            print(\"\\t\\tComputing non-oracle purity score...\")\n",
    "            non_oracle_purities.append(oracle.oracle_impurity_score(\n",
    "                c_soft=latent_codes,\n",
    "                c_true=split_fn(c_test),\n",
    "                alignment_function=oracle.max_alignment_matrix,\n",
    "                oracle_matrix=construct_trivial_auc_mat(\n",
    "                    experiment_config[\"num_concepts\"]\n",
    "                ),\n",
    "                purity_matrix=aligned_purity_mat,\n",
    "            ))\n",
    "            print(f\"\\t\\t\\tDone {non_oracle_purities[-1]:.4f}\")\n",
    "            \n",
    "            if include_encoder_purity:\n",
    "                # Then compute the purity when the concept representations are the (mean, std) vectors generated\n",
    "                # by the encoder\n",
    "                print(\"\\t\\tComputing encoder purity score...\")\n",
    "                encoder_means, encoder_logvars = wvae_model.encoder(\n",
    "                    split_fn(x_test)\n",
    "                )\n",
    "                encoder_codes = np.stack([encoder_means.numpy(), encoder_logvars.numpy()], axis=1)\n",
    "                encoder_purity_score, (encoder_purity_mat, encoder_aligned_purity_mat), _ = oracle.oracle_impurity_score(\n",
    "                    c_soft=encoder_codes,\n",
    "                    c_true=split_fn(c_test),\n",
    "                    output_matrices=True,\n",
    "                    alignment_function=oracle.max_alignment_matrix,\n",
    "                    oracle_matrix=oracle_mat,\n",
    "                )\n",
    "                print(f\"\\t\\t\\tDone {encoder_purity_score:.4f}\")\n",
    "                print(\"\\t\\tEncoder purity matrix:\")\n",
    "                print(encoder_purity_mat)\n",
    "                print(\"\\t\\tEncoder rligned purity matrix:\")\n",
    "                print(encoder_aligned_purity_mat)\n",
    "                encoder_purities.append(encoder_purity_score)\n",
    "                encoder_purity_mats.append(encoder_purity_mat)\n",
    "                encoder_aligned_purity_mats.append(encoder_aligned_purity_mat)\n",
    "                \n",
    "            print(\"\\t\\tDone with trial\", trial + 1)\n",
    "\n",
    "        tot_loss_mean, tot_loss_std = np.mean(tot_losses), np.std(tot_losses)\n",
    "        experiment_variables[\"total_losses\"].append((tot_loss_mean, tot_loss_std))\n",
    "        print(f\"\\tTest total loss: {tot_loss_mean:.4f}  {tot_loss_std:.4f}\")\n",
    "        \n",
    "        recon_loss_mean, recon_loss_std = np.mean(recon_losses), np.std(recon_losses)\n",
    "        if include_all_losses:\n",
    "            experiment_variables[\"reconstruction_losses\"].append((recon_loss_mean, recon_loss_std))\n",
    "        print(f\"\\tTest reconstruction loss: {recon_loss_mean:.4f}  {recon_loss_std:.4f}\")\n",
    "        \n",
    "        el_loss_mean, el_loss_std = np.mean(el_losses), np.std(el_losses)\n",
    "        if include_all_losses:\n",
    "            experiment_variables[\"elbo_losses\"].append((el_loss_mean, el_loss_std))\n",
    "        print(f\"\\tTest elbo loss: {el_loss_mean:.4f}  {el_loss_std:.4f}\")\n",
    "\n",
    "        purity_mats = np.stack(purity_mats, axis=0)\n",
    "        purity_mat_mean = np.mean(purity_mats, axis=0)\n",
    "        purity_mat_std = np.std(purity_mats, axis=0)\n",
    "        print(\"\\tPurity matrix:\")\n",
    "        for i in range(purity_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(purity_mat_mean.shape[1]):\n",
    "                line += f'{purity_mat_mean[i, j]:.4f}  {purity_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"purity_matrices\"].append((purity_mat_mean, purity_mat_std))\n",
    "        \n",
    "        aligned_purity_mats = np.stack(aligned_purity_mats, axis=0)\n",
    "        aligned_purity_mat_mean = np.mean(aligned_purity_mats, axis=0)\n",
    "        aligned_purity_mat_std = np.std(aligned_purity_mats, axis=0)\n",
    "        print(\"\\tAligned purity matrix:\")\n",
    "        for i in range(aligned_purity_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(aligned_purity_mat_mean.shape[1]):\n",
    "                line += f'{aligned_purity_mat_mean[i, j]:.4f}  {aligned_purity_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"aligned_purity_matrices\"].append((aligned_purity_mat_mean, aligned_purity_mat_std))\n",
    "\n",
    "\n",
    "        oracle_mats = np.stack(oracle_mats, axis=0)\n",
    "        oracle_mat_mean = np.mean(oracle_mats, axis=0)\n",
    "        oracle_mat_std = np.std(oracle_mats, axis=0)\n",
    "        print(\"\\tOracle matrix:\")\n",
    "        for i in range(oracle_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(oracle_mat_mean.shape[1]):\n",
    "                line += f'{oracle_mat_mean[i, j]:.4f}  {oracle_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"oracle_matrices\"].append((oracle_mat_mean, oracle_mat_std))\n",
    "\n",
    "        purity_mean, purity_std = np.mean(purities), np.std(purities)\n",
    "        experiment_variables[\"purity_scores\"].append((purity_mean, purity_std))\n",
    "        print(f\"\\tPurity score: {purity_mean:.4f}  {purity_std:.4f}\")\n",
    "\n",
    "        non_oracle_purity_mean, non_oracle_purity_std = np.mean(non_oracle_purities), np.std(non_oracle_purities)\n",
    "        experiment_variables[\"non_oracle_purity_scores\"].append((non_oracle_purity_mean, non_oracle_purity_std))\n",
    "        print(f\"\\tNon-oracle purity score: {non_oracle_purity_mean:.4f}  {non_oracle_purity_std:.4f}\")\n",
    "        \n",
    "        if include_encoder_purity:\n",
    "            encoder_purity_mats = np.stack(encoder_purity_mats, axis=0)\n",
    "            encoder_purity_mat_mean = np.mean(encoder_purity_mats, axis=0)\n",
    "            encoder_purity_mat_std = np.std(encoder_purity_mats, axis=0)\n",
    "            print(\"\\tEncoder purity matrix:\")\n",
    "            for i in range(encoder_purity_mat_mean.shape[0]):\n",
    "                line = \"\\t\\t\"\n",
    "                for j in range(encoder_purity_mat_mean.shape[1]):\n",
    "                    line += f'{encoder_purity_mat_mean[i, j]:.4f}  {encoder_purity_mat_std[i, j]:.4f}    '\n",
    "                print(line)\n",
    "\n",
    "            experiment_variables[\"encoder_purity_matrices\"].append((encoder_purity_mat_mean, encoder_purity_mat_std))\n",
    "\n",
    "            encoder_aligned_purity_mats = np.stack(encoder_aligned_purity_mats, axis=0)\n",
    "            encoder_aligned_purity_mat_mean = np.mean(encoder_aligned_purity_mats, axis=0)\n",
    "            encoder_aligned_purity_mat_std = np.std(encoder_aligned_purity_mats, axis=0)\n",
    "            print(\"\\tEncoder aligned purity matrix:\")\n",
    "            for i in range(encoder_aligned_purity_mat_mean.shape[0]):\n",
    "                line = \"\\t\\t\"\n",
    "                for j in range(encoder_aligned_purity_mat_mean.shape[1]):\n",
    "                    line += f'{encoder_aligned_purity_mat_mean[i, j]:.4f}  {encoder_aligned_purity_mat_std[i, j]:.4f}    '\n",
    "                print(line)\n",
    "\n",
    "            experiment_variables[\"encoder_aligned_purity_matrices\"].append((encoder_aligned_purity_mat_mean, encoder_aligned_purity_mat_std))\n",
    "            \n",
    "            encoder_purity_mean, encoder_purity_std = np.mean(encoder_purities), np.std(encoder_purities)\n",
    "            experiment_variables[\"encoder_purity_scores\"].append((encoder_purity_mean, encoder_purity_std))\n",
    "            print(f\"\\tEncoder purity score: {encoder_purity_mean:.4f}  {encoder_purity_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables\n",
    "\n",
    "\n",
    "def wvae_bottleneck_predict_experiment_loop(\n",
    "    experiment_config,\n",
    "    datasets,\n",
    "    load_from_cache=False,\n",
    "    vae_model=weak_vae.MLVaeArgmax,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    if vae_model != beta_vae.BetaVAE:\n",
    "        split_fn = lambda x: x[:, :x.shape[1]//2, ...] if len(x.shape) > 1 else x[:x.shape[0]//2]\n",
    "    else:\n",
    "        split_fn = lambda x: x\n",
    "        \n",
    "    experiment_variables = dict(\n",
    "        latent_predictive_accuracies=[],\n",
    "        latent_predictive_aucs=[],\n",
    "    )\n",
    "\n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                print(\"Could not find\", f'\"{file_name}_means.npz\" or \"{file_name}_stds.npz\" in cache.')\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(datasets):\n",
    "                print(\"Found\", len(loaded_means), \"means for variable\", var_name, \"vs\", len(datasets), \"datasets\")\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)} ({file_name}).'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            print(experiment_variables)\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    utils.serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    for (ds_name, (x_train, y_train, c_train), (x_test, y_test, c_test)) in datasets[start_ind:]:\n",
    "        latent_dim = experiment_config['latent_dim']\n",
    "        print(\"Training with latent dimensions\", latent_dim, \"in dataset\", ds_name)\n",
    "        task_accs = []\n",
    "        aucs = []\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']}\")\n",
    "            encoder = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/{ds_name}_encoder_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            decoder = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/{ds_name}_decoder_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            wvae_model = vae_model(\n",
    "                encoder=encoder,\n",
    "                decoder=decoder,\n",
    "                loss_fn=vae_losses.l2_loss_wrapper(),\n",
    "                beta=experiment_config[\"beta\"],\n",
    "            )\n",
    "            \n",
    "            predictive_decoder = construct_decoder(\n",
    "                units=experiment_config[\"latent_decoder_units\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "            )\n",
    "            predictive_decoder.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(experiment_config[\"learning_rate\"]),\n",
    "                loss=(\n",
    "                    tf.keras.losses.BinaryCrossentropy(from_logits=True) if (experiment_config[\"num_outputs\"] <= 2)\n",
    "                    else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "                ),\n",
    "                metrics=[\n",
    "                    \"binary_accuracy\" if (experiment_config[\"num_outputs\"] <= 2)\n",
    "                    else tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            print(\"\\tTraining model\")\n",
    "            train_codes = wvae_model.sample_from_latent_distribution(\n",
    "                *wvae_model.encoder(split_fn(x_train))\n",
    "            ).numpy()\n",
    "            test_codes = wvae_model.sample_from_latent_distribution(\n",
    "                *wvae_model.encoder(split_fn(x_test))\n",
    "            ).numpy()\n",
    "            predictive_decoder.fit(\n",
    "                x=train_codes,\n",
    "                y=split_fn(y_train),\n",
    "                epochs=experiment_config[\"predictor_max_epochs\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                verbose=verbosity,\n",
    "            )\n",
    "            print(\"\\tEvaluating model\")\n",
    "            test_result = predictive_decoder.evaluate(\n",
    "                test_codes,\n",
    "                split_fn(y_test),\n",
    "                verbose=0,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            task_accs.append(\n",
    "                test_result['sparse_top_k_categorical_accuracy']\n",
    "                if experiment_config['num_outputs'] > 1 else\n",
    "                test_result['binary_accuracy']\n",
    "            )\n",
    "\n",
    "            if experiment_config['num_outputs'] > 1:\n",
    "                # Then lets apply a softmax activation over all the probability\n",
    "                # classes\n",
    "                preds = scipy.special.softmax(\n",
    "                    predictive_decoder.predict(test_codes),\n",
    "                    axis=-1,\n",
    "                )\n",
    "\n",
    "                # And select just the labels that are in fact being used\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    split_fn(y_test),\n",
    "                    preds,\n",
    "                    multi_class='ovo',\n",
    "                ))\n",
    "            else:\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    y_test,\n",
    "                    predictive_decoder.predict(test_codes),\n",
    "                ))\n",
    "            \n",
    "            print(\n",
    "                f\"\\t\\tTest auc = {aucs[-1]:.4f}, \"\n",
    "                f\"task accuracy = {task_accs[-1]:.4f}\"\n",
    "            )\n",
    "            print(\"\\t\\tDone with trial\", trial + 1)\n",
    "\n",
    "        task_acc_mean, task_acc_std = np.mean(task_accs), np.std(task_accs)\n",
    "        experiment_variables[\"latent_predictive_accuracies\"].append((task_acc_mean, task_acc_std))\n",
    "        print(f\"\\tTest task accuracy: {task_acc_mean:.4f}  {task_acc_std:.4f}\")\n",
    "\n",
    "        task_auc_mean, task_auc_std = np.mean(aucs), np.std(aucs)\n",
    "        experiment_variables[\"latent_predictive_aucs\"].append((task_auc_mean, task_auc_std))\n",
    "        print(f\"\\tTest task AUC: {task_auc_mean:.4f}  {task_auc_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables\n",
    "\n",
    "def wvae_bottleneck_concept_predict_experiment_loop(\n",
    "    experiment_config,\n",
    "    datasets,\n",
    "    load_from_cache=False,\n",
    "    vae_model=weak_vae.MLVaeArgmax,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    if vae_model != beta_vae.BetaVAE:\n",
    "        split_fn = lambda x: x[:, :x.shape[1]//2, ...] if len(x.shape) > 1 else x[:x.shape[0]//2]\n",
    "    else:\n",
    "        split_fn = lambda x: x\n",
    "        \n",
    "    experiment_variables = dict(\n",
    "        latent_avg_concept_predictive_accuracies=[],\n",
    "        latent_avg_concept_predictive_aucs=[],\n",
    "    )\n",
    "\n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                print(\"Could not find\", f'\"{file_name}_means.npz\" or \"{file_name}_stds.npz\" in cache.')\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(datasets):\n",
    "                print(\"Found\", len(loaded_means), \"means for variable\", var_name, \"vs\", len(datasets), \"datasets\")\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)} ({file_name}).'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            print(experiment_variables)\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    utils.serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    for (ds_name, (x_train, y_train, c_train), (x_test, y_test, c_test)) in datasets[start_ind:]:\n",
    "        latent_dim = experiment_config['latent_dim']\n",
    "        print(\"Training with latent dimensions\", latent_dim, \"in dataset\", ds_name)\n",
    "        avg_concept_accs = []\n",
    "        avg_concept_aucs = []\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']}\")\n",
    "            encoder = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/{ds_name}_encoder_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            decoder = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/{ds_name}_decoder_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            wvae_model = vae_model(\n",
    "                encoder=encoder,\n",
    "                decoder=decoder,\n",
    "                loss_fn=vae_losses.l2_loss_wrapper(),\n",
    "                beta=experiment_config[\"beta\"],\n",
    "            )\n",
    "            \n",
    "            current_accs = []\n",
    "            current_aucs = []\n",
    "            \n",
    "            for concept_idx in range(experiment_config[\"num_concepts\"]):\n",
    "                predictive_decoder = construct_decoder(\n",
    "                    units=experiment_config[\"latent_decoder_units\"],\n",
    "                    num_outputs=1,\n",
    "                )\n",
    "                predictive_decoder.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(experiment_config[\"learning_rate\"]),\n",
    "                    loss=(\n",
    "                        tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "                    ),\n",
    "                    metrics=[\n",
    "                        \"binary_accuracy\"\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "                print(\"\\tTraining model for concept\", concept_idx)\n",
    "                train_codes = wvae_model.sample_from_latent_distribution(\n",
    "                    *wvae_model.encoder(split_fn(x_train))\n",
    "                ).numpy()\n",
    "                test_codes = wvae_model.sample_from_latent_distribution(\n",
    "                    *wvae_model.encoder(split_fn(x_test))\n",
    "                ).numpy()\n",
    "                predictive_decoder.fit(\n",
    "                    x=train_codes,\n",
    "                    y=split_fn(c_train)[:, concept_idx],\n",
    "                    epochs=experiment_config[\"predictor_max_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "                print(\"\\t\\tEvaluating model\")\n",
    "                test_result = predictive_decoder.evaluate(\n",
    "                    test_codes,\n",
    "                    split_fn(c_test)[:, concept_idx],\n",
    "                    verbose=0,\n",
    "                    return_dict=True,\n",
    "                )\n",
    "                current_accs.append(\n",
    "                    test_result['binary_accuracy']\n",
    "                )\n",
    "\n",
    "                current_aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    split_fn(c_test)[:, concept_idx],\n",
    "                    predictive_decoder.predict(test_codes),\n",
    "                ))\n",
    "                print(\n",
    "                f\"\\t\\tTestconcept AUC = {current_aucs[-1]:.4f}, \"\n",
    "                f\"concept accuracy = {current_accs[-1]:.4f}\"\n",
    "            )\n",
    "            avg_concept_accs.append(np.mean(current_accs))\n",
    "            avg_concept_aucs.append(np.mean(current_aucs))\n",
    "            print(\n",
    "                f\"\\t\\tTest avg concept AUC = {avg_concept_aucs[-1]:.4f}, \"\n",
    "                f\"avg concept accuracy = {avg_concept_accs[-1]:.4f}\"\n",
    "            )\n",
    "            print(\"\\tDone with trial\", trial + 1)\n",
    "\n",
    "        avg_concept_acc_mean, avg_concept_acc_std = np.mean(avg_concept_accs), np.std(avg_concept_accs)\n",
    "        experiment_variables[\"latent_avg_concept_predictive_accuracies\"].append((avg_concept_acc_mean, avg_concept_acc_std))\n",
    "        print(f\"\\tTest average concept accuracy: {avg_concept_acc_mean:.4f}  {avg_concept_acc_std:.4f}\")\n",
    "\n",
    "        avg_concept_auc_mean, avg_concept_auc_std = np.mean(avg_concept_aucs), np.std(avg_concept_aucs)\n",
    "        experiment_variables[\"latent_avg_concept_predictive_aucs\"].append((avg_concept_auc_mean, avg_concept_auc_std))\n",
    "        print(f\"\\tTest average concept AUC: {avg_concept_auc_mean:.4f}  {avg_concept_auc_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(weak_vae)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "ada_mlvae_balanced_multilabel_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    latent_dim=(balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]),\n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[256, 512],\n",
    "    \n",
    "    latent_decoder_units=[64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    predictor_max_epochs=100,\n",
    "    \n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    beta=1,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"ada_ml_vae/balanced_multilabel_purity_latent_{balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}\"\n",
    "    ),\n",
    "    num_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    ")\n",
    "\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(ada_mlvae_balanced_multilabel_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "ada_mlvae_balanced_multilabel_figure_dir = os.path.join(ada_mlvae_balanced_multilabel_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(ada_mlvae_balanced_multilabel_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "ada_mlvae_balanced_multilabel_results = wvae_experiment_loop(\n",
    "    ada_mlvae_balanced_multilabel_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    include_all_losses=True,\n",
    "    include_encoder_purity=True,\n",
    "    vae_model=weak_vae.MLVaeArgmax,\n",
    "    datasets=[\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_0_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[0][0], balanced_multiclass_wvae_datasets[0][1], balanced_multiclass_wvae_datasets[0][2]),\n",
    "            (balanced_multiclass_wvae_datasets[0][3], balanced_multiclass_wvae_datasets[0][4], balanced_multiclass_wvae_datasets[0][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_1_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[1][0], balanced_multiclass_wvae_datasets[1][1], balanced_multiclass_wvae_datasets[1][2]),\n",
    "            (balanced_multiclass_wvae_datasets[1][3], balanced_multiclass_wvae_datasets[1][4], balanced_multiclass_wvae_datasets[1][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_2_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[2][0], balanced_multiclass_wvae_datasets[2][1], balanced_multiclass_wvae_datasets[2][2]),\n",
    "            (balanced_multiclass_wvae_datasets[2][3], balanced_multiclass_wvae_datasets[2][4], balanced_multiclass_wvae_datasets[2][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_3_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[3][0], balanced_multiclass_wvae_datasets[3][1], balanced_multiclass_wvae_datasets[3][2]),\n",
    "            (balanced_multiclass_wvae_datasets[3][3], balanced_multiclass_wvae_datasets[3][4], balanced_multiclass_wvae_datasets[3][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_4_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[4][0], balanced_multiclass_wvae_datasets[4][1], balanced_multiclass_wvae_datasets[4][2]),\n",
    "            (balanced_multiclass_wvae_datasets[4][3], balanced_multiclass_wvae_datasets[4][4], balanced_multiclass_wvae_datasets[4][5]),\n",
    "        ),\n",
    "    ],\n",
    "    oracle_matrix_cache=balanced_oracle_matrix_cache,\n",
    ")\n",
    "print(\"total_losses:\", ada_mlvae_balanced_multilabel_results[\"total_losses\"])\n",
    "print(\"purity_scores:\", ada_mlvae_balanced_multilabel_results[\"purity_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(weak_vae)\n",
    "\n",
    "############################################################################\n",
    "## Task accuracy experiment run\n",
    "############################################################################\n",
    "\n",
    "ada_mlvae_balanced_multilabel_results.update(wvae_bottleneck_predict_experiment_loop(\n",
    "    ada_mlvae_balanced_multilabel_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    vae_model=weak_vae.MLVaeArgmax,\n",
    "    datasets=[\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_0_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[0][0], balanced_multiclass_wvae_datasets[0][1], balanced_multiclass_wvae_datasets[0][2]),\n",
    "            (balanced_multiclass_wvae_datasets[0][3], balanced_multiclass_wvae_datasets[0][4], balanced_multiclass_wvae_datasets[0][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_1_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[1][0], balanced_multiclass_wvae_datasets[1][1], balanced_multiclass_wvae_datasets[1][2]),\n",
    "            (balanced_multiclass_wvae_datasets[1][3], balanced_multiclass_wvae_datasets[1][4], balanced_multiclass_wvae_datasets[1][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_2_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[2][0], balanced_multiclass_wvae_datasets[2][1], balanced_multiclass_wvae_datasets[2][2]),\n",
    "            (balanced_multiclass_wvae_datasets[2][3], balanced_multiclass_wvae_datasets[2][4], balanced_multiclass_wvae_datasets[2][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_3_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[3][0], balanced_multiclass_wvae_datasets[3][1], balanced_multiclass_wvae_datasets[3][2]),\n",
    "            (balanced_multiclass_wvae_datasets[3][3], balanced_multiclass_wvae_datasets[3][4], balanced_multiclass_wvae_datasets[3][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_4_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[4][0], balanced_multiclass_wvae_datasets[4][1], balanced_multiclass_wvae_datasets[4][2]),\n",
    "            (balanced_multiclass_wvae_datasets[4][3], balanced_multiclass_wvae_datasets[4][4], balanced_multiclass_wvae_datasets[4][5]),\n",
    "        ),\n",
    "    ],\n",
    "))\n",
    "print(\"latent_predictive_accuracies:\", ada_mlvae_balanced_multilabel_results[\"latent_predictive_accuracies\"])\n",
    "print(\"latent_predictive_aucs:\", ada_mlvae_balanced_multilabel_results[\"latent_predictive_aucs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(weak_vae)\n",
    "\n",
    "############################################################################\n",
    "## Task accuracy experiment run\n",
    "############################################################################\n",
    "\n",
    "ada_mlvae_balanced_multilabel_results.update(wvae_bottleneck_concept_predict_experiment_loop(\n",
    "    ada_mlvae_balanced_multilabel_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    vae_model=weak_vae.MLVaeArgmax,\n",
    "    datasets=[\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_0_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[0][0], balanced_multiclass_wvae_datasets[0][1], balanced_multiclass_wvae_datasets[0][2]),\n",
    "            (balanced_multiclass_wvae_datasets[0][3], balanced_multiclass_wvae_datasets[0][4], balanced_multiclass_wvae_datasets[0][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_1_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[1][0], balanced_multiclass_wvae_datasets[1][1], balanced_multiclass_wvae_datasets[1][2]),\n",
    "            (balanced_multiclass_wvae_datasets[1][3], balanced_multiclass_wvae_datasets[1][4], balanced_multiclass_wvae_datasets[1][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_2_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[2][0], balanced_multiclass_wvae_datasets[2][1], balanced_multiclass_wvae_datasets[2][2]),\n",
    "            (balanced_multiclass_wvae_datasets[2][3], balanced_multiclass_wvae_datasets[2][4], balanced_multiclass_wvae_datasets[2][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_3_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[3][0], balanced_multiclass_wvae_datasets[3][1], balanced_multiclass_wvae_datasets[3][2]),\n",
    "            (balanced_multiclass_wvae_datasets[3][3], balanced_multiclass_wvae_datasets[3][4], balanced_multiclass_wvae_datasets[3][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_4_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[4][0], balanced_multiclass_wvae_datasets[4][1], balanced_multiclass_wvae_datasets[4][2]),\n",
    "            (balanced_multiclass_wvae_datasets[4][3], balanced_multiclass_wvae_datasets[4][4], balanced_multiclass_wvae_datasets[4][5]),\n",
    "        ),\n",
    "    ],\n",
    "))\n",
    "print(\"latent_avg_concept_predictive_accuracies:\", ada_mlvae_balanced_multilabel_results[\"latent_avg_concept_predictive_accuracies\"])\n",
    "print(\"latent_avg_concept_predictive_aucs:\", ada_mlvae_balanced_multilabel_results[\"latent_avg_concept_predictive_aucs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(weak_vae)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "ada_mlvae_balanced_multilabel_extended_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    latent_dim=(2 * balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]),\n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    \n",
    "    decoder_units=[256, 512],\n",
    "\n",
    "    latent_decoder_units=[64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    predictor_max_epochs=100,\n",
    "    \n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    beta=1,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "       RESULTS_DIR,\n",
    "       f\"ada_ml_vae/balanced_multilabel_purity_latent_{2 * balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}\"\n",
    "    ),\n",
    "    num_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    ")\n",
    "\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(ada_mlvae_balanced_multilabel_extended_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "ada_mlvae_balanced_multilabel_extended_figure_dir = os.path.join(ada_mlvae_balanced_multilabel_extended_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(ada_mlvae_balanced_multilabel_extended_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "ada_mlvae_balanced_multilabel_extended_results = wvae_experiment_loop(\n",
    "    ada_mlvae_balanced_multilabel_extended_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    include_all_losses=True,\n",
    "    include_encoder_purity=True,\n",
    "    vae_model=weak_vae.MLVaeArgmax,\n",
    "    datasets=[\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_0_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[0][0], balanced_multiclass_wvae_datasets[0][1], balanced_multiclass_wvae_datasets[0][2]),\n",
    "            (balanced_multiclass_wvae_datasets[0][3], balanced_multiclass_wvae_datasets[0][4], balanced_multiclass_wvae_datasets[0][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_1_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[1][0], balanced_multiclass_wvae_datasets[1][1], balanced_multiclass_wvae_datasets[1][2]),\n",
    "            (balanced_multiclass_wvae_datasets[1][3], balanced_multiclass_wvae_datasets[1][4], balanced_multiclass_wvae_datasets[1][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_2_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[2][0], balanced_multiclass_wvae_datasets[2][1], balanced_multiclass_wvae_datasets[2][2]),\n",
    "            (balanced_multiclass_wvae_datasets[2][3], balanced_multiclass_wvae_datasets[2][4], balanced_multiclass_wvae_datasets[2][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_3_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[3][0], balanced_multiclass_wvae_datasets[3][1], balanced_multiclass_wvae_datasets[3][2]),\n",
    "            (balanced_multiclass_wvae_datasets[3][3], balanced_multiclass_wvae_datasets[3][4], balanced_multiclass_wvae_datasets[3][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_4_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[4][0], balanced_multiclass_wvae_datasets[4][1], balanced_multiclass_wvae_datasets[4][2]),\n",
    "            (balanced_multiclass_wvae_datasets[4][3], balanced_multiclass_wvae_datasets[4][4], balanced_multiclass_wvae_datasets[4][5]),\n",
    "        ),\n",
    "    ],\n",
    "    oracle_matrix_cache=balanced_oracle_matrix_cache,\n",
    ")\n",
    "print(\"total_losses:\", ada_mlvae_balanced_multilabel_extended_results[\"total_losses\"])\n",
    "print(\"purity_scores:\", ada_mlvae_balanced_multilabel_extended_results[\"purity_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(weak_vae)\n",
    "\n",
    "############################################################################\n",
    "## Task accuracy experiment run\n",
    "############################################################################\n",
    "\n",
    "ada_mlvae_balanced_multilabel_extended_results.update(wvae_bottleneck_predict_experiment_loop(\n",
    "    ada_mlvae_balanced_multilabel_extended_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    vae_model=weak_vae.MLVaeArgmax,\n",
    "    datasets=[\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_0_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[0][0], balanced_multiclass_wvae_datasets[0][1], balanced_multiclass_wvae_datasets[0][2]),\n",
    "            (balanced_multiclass_wvae_datasets[0][3], balanced_multiclass_wvae_datasets[0][4], balanced_multiclass_wvae_datasets[0][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_1_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[1][0], balanced_multiclass_wvae_datasets[1][1], balanced_multiclass_wvae_datasets[1][2]),\n",
    "            (balanced_multiclass_wvae_datasets[1][3], balanced_multiclass_wvae_datasets[1][4], balanced_multiclass_wvae_datasets[1][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_2_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[2][0], balanced_multiclass_wvae_datasets[2][1], balanced_multiclass_wvae_datasets[2][2]),\n",
    "            (balanced_multiclass_wvae_datasets[2][3], balanced_multiclass_wvae_datasets[2][4], balanced_multiclass_wvae_datasets[2][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_3_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[3][0], balanced_multiclass_wvae_datasets[3][1], balanced_multiclass_wvae_datasets[3][2]),\n",
    "            (balanced_multiclass_wvae_datasets[3][3], balanced_multiclass_wvae_datasets[3][4], balanced_multiclass_wvae_datasets[3][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_4_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[4][0], balanced_multiclass_wvae_datasets[4][1], balanced_multiclass_wvae_datasets[4][2]),\n",
    "            (balanced_multiclass_wvae_datasets[4][3], balanced_multiclass_wvae_datasets[4][4], balanced_multiclass_wvae_datasets[4][5]),\n",
    "        ),\n",
    "    ],\n",
    "))\n",
    "print(\"latent_predictive_accuracies:\", ada_mlvae_balanced_multilabel_extended_results[\"latent_predictive_accuracies\"])\n",
    "print(\"latent_predictive_aucs:\", ada_mlvae_balanced_multilabel_extended_results[\"latent_predictive_aucs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(weak_vae)\n",
    "\n",
    "############################################################################\n",
    "## Task accuracy experiment run\n",
    "############################################################################\n",
    "\n",
    "ada_mlvae_balanced_multilabel_extended_results.update(wvae_bottleneck_concept_predict_experiment_loop(\n",
    "    ada_mlvae_balanced_multilabel_extended_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    vae_model=weak_vae.MLVaeArgmax,\n",
    "    datasets=[\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_0_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[0][0], balanced_multiclass_wvae_datasets[0][1], balanced_multiclass_wvae_datasets[0][2]),\n",
    "            (balanced_multiclass_wvae_datasets[0][3], balanced_multiclass_wvae_datasets[0][4], balanced_multiclass_wvae_datasets[0][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_1_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[1][0], balanced_multiclass_wvae_datasets[1][1], balanced_multiclass_wvae_datasets[1][2]),\n",
    "            (balanced_multiclass_wvae_datasets[1][3], balanced_multiclass_wvae_datasets[1][4], balanced_multiclass_wvae_datasets[1][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_2_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[2][0], balanced_multiclass_wvae_datasets[2][1], balanced_multiclass_wvae_datasets[2][2]),\n",
    "            (balanced_multiclass_wvae_datasets[2][3], balanced_multiclass_wvae_datasets[2][4], balanced_multiclass_wvae_datasets[2][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_3_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[3][0], balanced_multiclass_wvae_datasets[3][1], balanced_multiclass_wvae_datasets[3][2]),\n",
    "            (balanced_multiclass_wvae_datasets[3][3], balanced_multiclass_wvae_datasets[3][4], balanced_multiclass_wvae_datasets[3][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_4_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[4][0], balanced_multiclass_wvae_datasets[4][1], balanced_multiclass_wvae_datasets[4][2]),\n",
    "            (balanced_multiclass_wvae_datasets[4][3], balanced_multiclass_wvae_datasets[4][4], balanced_multiclass_wvae_datasets[4][5]),\n",
    "        ),\n",
    "    ],\n",
    "))\n",
    "print(\"latent_avg_concept_predictive_accuracies:\", ada_mlvae_balanced_multilabel_extended_results[\"latent_avg_concept_predictive_accuracies\"])\n",
    "print(\"latent_avg_concept_predictive_aucs:\", ada_mlvae_balanced_multilabel_extended_results[\"latent_avg_concept_predictive_aucs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada-GVAE Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Task Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(weak_vae)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "ada_gvae_balanced_multilabel_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    latent_dim=(balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]),\n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[256, 512],\n",
    "    \n",
    "    latent_decoder_units=[64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    predictor_max_epochs=100,\n",
    "    \n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    beta=1,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "       RESULTS_DIR,\n",
    "       f\"ada_g_vae/balanced_multilabel_purity_latent_{balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}\"\n",
    "    ),\n",
    "    num_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    ")\n",
    "\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(ada_gvae_balanced_multilabel_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "ada_gvae_balanced_multilabel_figure_dir = os.path.join(ada_gvae_balanced_multilabel_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(ada_gvae_balanced_multilabel_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "ada_gvae_balanced_multilabel_results = wvae_experiment_loop(\n",
    "    ada_gvae_balanced_multilabel_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    include_all_losses=True,\n",
    "    include_encoder_purity=True,\n",
    "    vae_model=weak_vae.GroupVAEArgmax,\n",
    "    datasets=[\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_0_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[0][0], balanced_multiclass_wvae_datasets[0][1]),\n",
    "            (balanced_multiclass_wvae_datasets[0][2], balanced_multiclass_wvae_datasets[0][3]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_1_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[1][0], balanced_multiclass_wvae_datasets[1][1]),\n",
    "            (balanced_multiclass_wvae_datasets[1][2], balanced_multiclass_wvae_datasets[1][3]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_2_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[2][0], balanced_multiclass_wvae_datasets[2][1]),\n",
    "            (balanced_multiclass_wvae_datasets[2][2], balanced_multiclass_wvae_datasets[2][3]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_3_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[3][0], balanced_multiclass_wvae_datasets[3][1]),\n",
    "            (balanced_multiclass_wvae_datasets[3][2], balanced_multiclass_wvae_datasets[3][3]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_4_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[4][0], balanced_multiclass_wvae_datasets[4][1]),\n",
    "            (balanced_multiclass_wvae_datasets[4][2], balanced_multiclass_wvae_datasets[4][3]),\n",
    "        ),\n",
    "    ],\n",
    "    oracle_matrix_cache=balanced_oracle_matrix_cache,\n",
    ")\n",
    "print(\"total_losses:\", ada_gvae_balanced_multilabel_results[\"total_losses\"])\n",
    "print(\"purity_scores:\", ada_gvae_balanced_multilabel_results[\"purity_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(weak_vae)\n",
    "\n",
    "############################################################################\n",
    "## Task accuracy experiment run\n",
    "############################################################################\n",
    "\n",
    "ada_gvae_balanced_multilabel_results.update(wvae_bottleneck_predict_experiment_loop(\n",
    "    ada_gvae_balanced_multilabel_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    vae_model=weak_vae.GroupVAEArgmax,\n",
    "    datasets=[\n",
    "        (\n",
    "            # Change to \"balanced_multiclass_task_bin_concepts_dep_0_complete\" on rerun\n",
    "            \"multiclass_task_bin_concepts_dep_0\",\n",
    "            (balanced_multiclass_wvae_datasets[0][0], balanced_multiclass_wvae_datasets[0][1], balanced_multiclass_wvae_datasets[0][2]),\n",
    "            (balanced_multiclass_wvae_datasets[0][3], balanced_multiclass_wvae_datasets[0][4], balanced_multiclass_wvae_datasets[0][5]),\n",
    "        ),\n",
    "        (\n",
    "            # Change to \"balanced_multiclass_task_bin_concepts_dep_1_complete\" on rerun\n",
    "            \"multiclass_task_bin_concepts_dep_1\",\n",
    "            (balanced_multiclass_wvae_datasets[1][0], balanced_multiclass_wvae_datasets[1][1], balanced_multiclass_wvae_datasets[1][2]),\n",
    "            (balanced_multiclass_wvae_datasets[1][3], balanced_multiclass_wvae_datasets[1][4], balanced_multiclass_wvae_datasets[1][5]),\n",
    "        ),\n",
    "        (\n",
    "            # Change to \"balanced_multiclass_task_bin_concepts_dep_2_complete\" on rerun\n",
    "            \"multiclass_task_bin_concepts_dep_2\",\n",
    "            (balanced_multiclass_wvae_datasets[2][0], balanced_multiclass_wvae_datasets[2][1], balanced_multiclass_wvae_datasets[2][2]),\n",
    "            (balanced_multiclass_wvae_datasets[2][3], balanced_multiclass_wvae_datasets[2][4], balanced_multiclass_wvae_datasets[2][5]),\n",
    "        ),\n",
    "        (\n",
    "            # Change to \"balanced_multiclass_task_bin_concepts_dep_3_complete\" on rerun\n",
    "            \"multiclass_task_bin_concepts_dep_3\",\n",
    "            (balanced_multiclass_wvae_datasets[3][0], balanced_multiclass_wvae_datasets[3][1], balanced_multiclass_wvae_datasets[3][2]),\n",
    "            (balanced_multiclass_wvae_datasets[3][3], balanced_multiclass_wvae_datasets[3][4], balanced_multiclass_wvae_datasets[3][5]),\n",
    "        ),\n",
    "        (\n",
    "            # Change to \"balanced_multiclass_task_bin_concepts_dep_4_complete\" on rerun\n",
    "            \"multiclass_task_bin_concepts_dep_4\",\n",
    "            (balanced_multiclass_wvae_datasets[4][0], balanced_multiclass_wvae_datasets[4][1], balanced_multiclass_wvae_datasets[4][2]),\n",
    "            (balanced_multiclass_wvae_datasets[4][3], balanced_multiclass_wvae_datasets[4][4], balanced_multiclass_wvae_datasets[4][5]),\n",
    "        ),\n",
    "    ],\n",
    "))\n",
    "print(\"latent_predictive_accuracies:\", ada_gvae_balanced_multilabel_results[\"latent_predictive_accuracies\"])\n",
    "print(\"latent_predictive_aucs:\", ada_gvae_balanced_multilabel_results[\"latent_predictive_aucs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(weak_vae)\n",
    "\n",
    "############################################################################\n",
    "## Task accuracy experiment run\n",
    "############################################################################\n",
    "\n",
    "ada_gvae_balanced_multilabel_results.update(wvae_bottleneck_concept_predict_experiment_loop(\n",
    "    ada_gvae_balanced_multilabel_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    vae_model=weak_vae.GroupVAEArgmax,\n",
    "    datasets=[\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_0_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[0][0], balanced_multiclass_wvae_datasets[0][1], balanced_multiclass_wvae_datasets[0][2]),\n",
    "            (balanced_multiclass_wvae_datasets[0][3], balanced_multiclass_wvae_datasets[0][4], balanced_multiclass_wvae_datasets[0][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_1_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[1][0], balanced_multiclass_wvae_datasets[1][1], balanced_multiclass_wvae_datasets[1][2]),\n",
    "            (balanced_multiclass_wvae_datasets[1][3], balanced_multiclass_wvae_datasets[1][4], balanced_multiclass_wvae_datasets[1][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_2_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[2][0], balanced_multiclass_wvae_datasets[2][1], balanced_multiclass_wvae_datasets[2][2]),\n",
    "            (balanced_multiclass_wvae_datasets[2][3], balanced_multiclass_wvae_datasets[2][4], balanced_multiclass_wvae_datasets[2][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_3_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[3][0], balanced_multiclass_wvae_datasets[3][1], balanced_multiclass_wvae_datasets[3][2]),\n",
    "            (balanced_multiclass_wvae_datasets[3][3], balanced_multiclass_wvae_datasets[3][4], balanced_multiclass_wvae_datasets[3][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_4_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[4][0], balanced_multiclass_wvae_datasets[4][1], balanced_multiclass_wvae_datasets[4][2]),\n",
    "            (balanced_multiclass_wvae_datasets[4][3], balanced_multiclass_wvae_datasets[4][4], balanced_multiclass_wvae_datasets[4][5]),\n",
    "        ),\n",
    "    ],\n",
    "))\n",
    "print(\"latent_avg_concept_predictive_accuracies:\", ada_gvae_balanced_multilabel_results[\"latent_avg_concept_predictive_accuracies\"])\n",
    "print(\"latent_avg_concept_predictive_aucs:\", ada_gvae_balanced_multilabel_results[\"latent_avg_concept_predictive_aucs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(weak_vae)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "ada_gvae_balanced_multilabel_extended_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    latent_dim=(2 * balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]),\n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[256, 512],\n",
    "    \n",
    "    latent_decoder_units=[64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    predictor_max_epochs=100,\n",
    "    \n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    beta=1,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "       RESULTS_DIR,\n",
    "       f\"ada_g_vae/multilabel_purity_latent_{2 * balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}\"\n",
    "    ),\n",
    "    num_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    ")\n",
    "\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(ada_gvae_balanced_multilabel_extended_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "ada_gvae_balanced_multilabel_extended_figure_dir = os.path.join(ada_gvae_balanced_multilabel_extended_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(ada_gvae_balanced_multilabel_extended_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "ada_gvae_balanced_multilabel_extended_results = wvae_experiment_loop(\n",
    "    ada_gvae_balanced_multilabel_extended_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    include_all_losses=True,\n",
    "    include_encoder_purity=True,\n",
    "    vae_model=weak_vae.GroupVAEArgmax,\n",
    "    datasets=[\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_0_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[0][0], balanced_multiclass_wvae_datasets[0][1]),\n",
    "            (balanced_multiclass_wvae_datasets[0][2], balanced_multiclass_wvae_datasets[0][3]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_1_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[1][0], balanced_multiclass_wvae_datasets[1][1]),\n",
    "            (balanced_multiclass_wvae_datasets[1][2], balanced_multiclass_wvae_datasets[1][3]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_2_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[2][0], balanced_multiclass_wvae_datasets[2][1]),\n",
    "            (balanced_multiclass_wvae_datasets[2][2], balanced_multiclass_wvae_datasets[2][3]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_3_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[3][0], balanced_multiclass_wvae_datasets[3][1]),\n",
    "            (balanced_multiclass_wvae_datasets[3][2], balanced_multiclass_wvae_datasets[3][3]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_4_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[4][0], balanced_multiclass_wvae_datasets[4][1]),\n",
    "            (balanced_multiclass_wvae_datasets[4][2], balanced_multiclass_wvae_datasets[4][3]),\n",
    "        ),\n",
    "    ],\n",
    "    oracle_matrix_cache=balanced_oracle_matrix_cache,\n",
    ")\n",
    "print(\"total_losses:\", ada_gvae_balanced_multilabel_extended_results[\"total_losses\"])\n",
    "print(\"purity_scores:\", ada_gvae_balanced_multilabel_extended_results[\"purity_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(weak_vae)\n",
    "\n",
    "############################################################################\n",
    "## Task accuracy experiment run\n",
    "############################################################################\n",
    "\n",
    "ada_gvae_balanced_multilabel_extended_results.update(wvae_bottleneck_predict_experiment_loop(\n",
    "    ada_gvae_balanced_multilabel_extended_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    vae_model=weak_vae.GroupVAEArgmax,\n",
    "    datasets=[\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_0_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[0][0], balanced_multiclass_wvae_datasets[0][1], balanced_multiclass_wvae_datasets[0][2]),\n",
    "            (balanced_multiclass_wvae_datasets[0][3], balanced_multiclass_wvae_datasets[0][4], balanced_multiclass_wvae_datasets[0][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_1_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[1][0], balanced_multiclass_wvae_datasets[1][1], balanced_multiclass_wvae_datasets[1][2]),\n",
    "            (balanced_multiclass_wvae_datasets[1][3], balanced_multiclass_wvae_datasets[1][4], balanced_multiclass_wvae_datasets[1][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_2_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[2][0], balanced_multiclass_wvae_datasets[2][1], balanced_multiclass_wvae_datasets[2][2]),\n",
    "            (balanced_multiclass_wvae_datasets[2][3], balanced_multiclass_wvae_datasets[2][4], balanced_multiclass_wvae_datasets[2][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_3_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[3][0], balanced_multiclass_wvae_datasets[3][1], balanced_multiclass_wvae_datasets[3][2]),\n",
    "            (balanced_multiclass_wvae_datasets[3][3], balanced_multiclass_wvae_datasets[3][4], balanced_multiclass_wvae_datasets[3][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_4_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[4][0], balanced_multiclass_wvae_datasets[4][1], balanced_multiclass_wvae_datasets[4][2]),\n",
    "            (balanced_multiclass_wvae_datasets[4][3], balanced_multiclass_wvae_datasets[4][4], balanced_multiclass_wvae_datasets[4][5]),\n",
    "        ),\n",
    "    ],\n",
    "))\n",
    "print(\"latent_predictive_accuracies:\", ada_gvae_balanced_multilabel_extended_results[\"latent_predictive_accuracies\"])\n",
    "print(\"latent_predictive_aucs:\", ada_gvae_balanced_multilabel_extended_results[\"latent_predictive_aucs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(weak_vae)\n",
    "\n",
    "############################################################################\n",
    "## Task accuracy experiment run\n",
    "############################################################################\n",
    "\n",
    "ada_gvae_balanced_multilabel_extended_results.update(wvae_bottleneck_concept_predict_experiment_loop(\n",
    "    ada_gvae_balanced_multilabel_extended_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    vae_model=weak_vae.GroupVAEArgmax,\n",
    "    datasets=[\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_0_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[0][0], balanced_multiclass_wvae_datasets[0][1], balanced_multiclass_wvae_datasets[0][2]),\n",
    "            (balanced_multiclass_wvae_datasets[0][3], balanced_multiclass_wvae_datasets[0][4], balanced_multiclass_wvae_datasets[0][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_1_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[1][0], balanced_multiclass_wvae_datasets[1][1], balanced_multiclass_wvae_datasets[1][2]),\n",
    "            (balanced_multiclass_wvae_datasets[1][3], balanced_multiclass_wvae_datasets[1][4], balanced_multiclass_wvae_datasets[1][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_2_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[2][0], balanced_multiclass_wvae_datasets[2][1], balanced_multiclass_wvae_datasets[2][2]),\n",
    "            (balanced_multiclass_wvae_datasets[2][3], balanced_multiclass_wvae_datasets[2][4], balanced_multiclass_wvae_datasets[2][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_3_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[3][0], balanced_multiclass_wvae_datasets[3][1], balanced_multiclass_wvae_datasets[3][2]),\n",
    "            (balanced_multiclass_wvae_datasets[3][3], balanced_multiclass_wvae_datasets[3][4], balanced_multiclass_wvae_datasets[3][5]),\n",
    "        ),\n",
    "        (\n",
    "            \"balanced_multiclass_task_bin_concepts_dep_4_complete\",\n",
    "            (balanced_multiclass_wvae_datasets[4][0], balanced_multiclass_wvae_datasets[4][1], balanced_multiclass_wvae_datasets[4][2]),\n",
    "            (balanced_multiclass_wvae_datasets[4][3], balanced_multiclass_wvae_datasets[4][4], balanced_multiclass_wvae_datasets[4][5]),\n",
    "        ),\n",
    "    ],\n",
    "))\n",
    "print(\"latent_avg_concept_predictive_accuracies:\", ada_gvae_balanced_multilabel_extended_results[\"latent_avg_concept_predictive_accuracies\"])\n",
    "print(\"latent_avg_concept_predictive_aucs:\", ada_gvae_balanced_multilabel_extended_results[\"latent_avg_concept_predictive_aucs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beta-VAE Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.VAE.betaVAE as beta_vae\n",
    "\n",
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(beta_vae)\n",
    "\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "beta_vae_balanced_multilabel_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    latent_dim=(balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]),\n",
    "\n",
    "    beta=10,\n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    \n",
    "    decoder_units=[256, 512],\n",
    "    \n",
    "    latent_decoder_units=[64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    predictor_max_epochs=100,\n",
    "    \n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"beta_vae/balanced_multilabel_purity_latent_{balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}_beta_{10}\"\n",
    "    ),\n",
    "    num_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    ")\n",
    "\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(beta_vae_balanced_multilabel_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "beta_vae_balanced_multilabel_figure_dir = os.path.join(beta_vae_balanced_multilabel_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(beta_vae_balanced_multilabel_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "beta_vae_balanced_multilabel_results = wvae_experiment_loop(\n",
    "    beta_vae_balanced_multilabel_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    include_all_losses=True,\n",
    "    include_encoder_purity=True,\n",
    "    vae_model=beta_vae.BetaVAE,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "    oracle_matrix_cache=balanced_oracle_matrix_cache,\n",
    ")\n",
    "print(\"total_losses:\", beta_vae_balanced_multilabel_results[\"total_losses\"])\n",
    "print(\"purity_scores:\", beta_vae_balanced_multilabel_results[\"purity_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(beta_vae)\n",
    "\n",
    "############################################################################\n",
    "## Task accuracy experiment run\n",
    "############################################################################\n",
    "\n",
    "beta_vae_balanced_multilabel_results.update(wvae_bottleneck_predict_experiment_loop(\n",
    "    beta_vae_balanced_multilabel_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    vae_model=beta_vae.BetaVAE,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "))\n",
    "print(\"latent_predictive_accuracies:\", beta_vae_balanced_multilabel_results[\"latent_predictive_accuracies\"])\n",
    "print(\"latent_predictive_aucs:\", beta_vae_balanced_multilabel_results[\"latent_predictive_aucs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(beta_vae)\n",
    "\n",
    "############################################################################\n",
    "## Task accuracy experiment run\n",
    "############################################################################\n",
    "\n",
    "beta_vae_balanced_multilabel_results.update(wvae_bottleneck_concept_predict_experiment_loop(\n",
    "    beta_vae_balanced_multilabel_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    vae_model=beta_vae.BetaVAE,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "))\n",
    "print(\"latent_avg_concept_predictive_accuracies:\", beta_vae_balanced_multilabel_results[\"latent_avg_concept_predictive_accuracies\"])\n",
    "print(\"latent_avg_concept_predictive_aucs:\", beta_vae_balanced_multilabel_results[\"latent_avg_concept_predictive_aucs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.VAE.betaVAE as beta_vae\n",
    "\n",
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(beta_vae)\n",
    "\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "beta_vae_extended_balanced_multilabel_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    latent_dim=(2 * balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]),\n",
    "\n",
    "    beta=10,\n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    \n",
    "    decoder_units=[256, 512],\n",
    "    \n",
    "    latent_decoder_units=[64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    predictor_max_epochs=100,\n",
    "    \n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"beta_vae/balanced_multilabel_purity_latent_{2 * balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}_beta_{10}\"\n",
    "    ),\n",
    "    num_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    ")\n",
    "\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(beta_vae_extended_balanced_multilabel_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "beta_vae_extended_balanced_multilabel_figure_dir = os.path.join(beta_vae_extended_balanced_multilabel_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(beta_vae_extended_balanced_multilabel_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "beta_vae_extended_balanced_multilabel_results = wvae_experiment_loop(\n",
    "    beta_vae_extended_balanced_multilabel_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    include_all_losses=True,\n",
    "    vae_model=beta_vae.BetaVAE,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "    oracle_matrix_cache=balanced_oracle_matrix_cache,\n",
    ")\n",
    "print(\"total_losses:\", beta_vae_extended_balanced_multilabel_results[\"total_losses\"])\n",
    "print(\"purity_scores:\", beta_vae_extended_balanced_multilabel_results[\"purity_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(beta_vae)\n",
    "\n",
    "############################################################################\n",
    "## Task accuracy experiment run\n",
    "############################################################################\n",
    "\n",
    "beta_vae_extended_balanced_multilabel_results.update(wvae_bottleneck_predict_experiment_loop(\n",
    "    beta_vae_extended_balanced_multilabel_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    vae_model=beta_vae.BetaVAE,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "))\n",
    "print(\"latent_predictive_accuracies:\", beta_vae_extended_balanced_multilabel_results[\"latent_predictive_accuracies\"])\n",
    "print(\"latent_predictive_aucs:\", beta_vae_extended_balanced_multilabel_results[\"latent_predictive_aucs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(beta_vae)\n",
    "\n",
    "############################################################################\n",
    "## Task accuracy experiment run\n",
    "############################################################################\n",
    "\n",
    "beta_vae_extended_balanced_multilabel_results.update(wvae_bottleneck_concept_predict_experiment_loop(\n",
    "    beta_vae_extended_balanced_multilabel_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    vae_model=beta_vae.BetaVAE,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "))\n",
    "print(\"latent_avg_concept_predictive_accuracies:\", beta_vae_extended_balanced_multilabel_results[\"latent_avg_concept_predictive_accuracies\"])\n",
    "print(\"latent_avg_concept_predictive_aucs:\", beta_vae_extended_balanced_multilabel_results[\"latent_avg_concept_predictive_aucs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.VAE.betaVAE as beta_vae\n",
    "\n",
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(beta_vae)\n",
    "\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "vae_balanced_multilabel_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    latent_dim=(balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]),\n",
    "\n",
    "    beta=1,\n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    \n",
    "    decoder_units=[256, 512],\n",
    "    \n",
    "    latent_decoder_units=[64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    predictor_max_epochs=100,\n",
    "    \n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"beta_vae/balanced_multilabel_purity_latent_{balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}_beta_{1}\"\n",
    "    ),\n",
    "    num_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    ")\n",
    "\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(vae_balanced_multilabel_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "vae_balanced_multilabel_figure_dir = os.path.join(vae_balanced_multilabel_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(vae_balanced_multilabel_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "vae_balanced_multilabel_results = wvae_experiment_loop(\n",
    "    vae_balanced_multilabel_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    include_all_losses=True,\n",
    "    include_encoder_purity=True,\n",
    "    vae_model=beta_vae.BetaVAE,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "    oracle_matrix_cache=balanced_oracle_matrix_cache,\n",
    ")\n",
    "print(\"total_losses:\", vae_balanced_multilabel_results[\"total_losses\"])\n",
    "print(\"purity_scores:\", vae_balanced_multilabel_results[\"purity_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(beta_vae)\n",
    "############################################################################\n",
    "## Task accuracy experiment run\n",
    "############################################################################\n",
    "\n",
    "vae_balanced_multilabel_results.update(wvae_bottleneck_predict_experiment_loop(\n",
    "    vae_balanced_multilabel_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    vae_model=beta_vae.BetaVAE,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "))\n",
    "print(\"latent_predictive_accuracies:\", vae_balanced_multilabel_results[\"latent_predictive_accuracies\"])\n",
    "print(\"latent_predictive_aucs:\", vae_balanced_multilabel_results[\"latent_predictive_aucs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(beta_vae)\n",
    "\n",
    "############################################################################\n",
    "## Task accuracy experiment run\n",
    "############################################################################\n",
    "\n",
    "vae_balanced_multilabel_results.update(wvae_bottleneck_concept_predict_experiment_loop(\n",
    "    vae_balanced_multilabel_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    vae_model=beta_vae.BetaVAE,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "))\n",
    "print(\"latent_predictive_accuracies:\", vae_balanced_multilabel_results[\"latent_predictive_accuracies\"])\n",
    "print(\"latent_predictive_aucs:\", vae_balanced_multilabel_results[\"latent_predictive_aucs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.VAE.betaVAE as beta_vae\n",
    "\n",
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(beta_vae)\n",
    "\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "vae_extended_balanced_multilabel_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    latent_dim=(2 * balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]),\n",
    "\n",
    "    beta=1,\n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    include_pool=True,\n",
    "    encoder_units=[64, 64],\n",
    "    \n",
    "    decoder_units=[256, 512],\n",
    "    \n",
    "    latent_decoder_units=[64, 64],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    predictor_max_epochs=100,\n",
    "    \n",
    "    drop_prob=0.0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"beta_vae/balanced_multilabel_purity_latent_{2 * balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}_beta_{1}\"\n",
    "    ),\n",
    "    num_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    verbosity=0,\n",
    "    data_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    holdout_fraction=0.1,\n",
    "    visualization_frequency=25,\n",
    "    visualization_samples=6,\n",
    ")\n",
    "\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(vae_extended_balanced_multilabel_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "vae_extended_balanced_multilabel_figure_dir = os.path.join(vae_extended_balanced_multilabel_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(vae_extended_balanced_multilabel_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "vae_extended_balanced_multilabel_results = wvae_experiment_loop(\n",
    "    vae_extended_balanced_multilabel_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    include_all_losses=True,\n",
    "    include_encoder_purity=True,\n",
    "    vae_model=beta_vae.BetaVAE,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "    oracle_matrix_cache=balanced_oracle_matrix_cache,\n",
    ")\n",
    "print(\"total_losses:\", vae_extended_balanced_multilabel_results[\"total_losses\"])\n",
    "print(\"purity_scores:\", vae_extended_balanced_multilabel_results[\"purity_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(beta_vae)\n",
    "\n",
    "############################################################################\n",
    "## Task accuracy experiment run\n",
    "############################################################################\n",
    "\n",
    "vae_extended_balanced_multilabel_results.update(wvae_bottleneck_predict_experiment_loop(\n",
    "    vae_extended_balanced_multilabel_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    vae_model=beta_vae.BetaVAE,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "))\n",
    "print(\"latent_predictive_accuracies:\", vae_extended_balanced_multilabel_results[\"latent_predictive_accuracies\"])\n",
    "print(\"latent_predictive_aucs:\", vae_extended_balanced_multilabel_results[\"latent_predictive_aucs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vae_losses)\n",
    "reload(base_vae)\n",
    "reload(beta_vae)\n",
    "\n",
    "############################################################################\n",
    "## Task accuracy experiment run\n",
    "############################################################################\n",
    "\n",
    "vae_extended_balanced_multilabel_results.update(wvae_bottleneck_concept_predict_experiment_loop(\n",
    "    vae_extended_balanced_multilabel_experiment_config,\n",
    "    load_from_cache=True,\n",
    "    vae_model=beta_vae.BetaVAE,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "))\n",
    "print(\"latent_predictive_accuracies:\", vae_extended_balanced_multilabel_results[\"latent_predictive_accuracies\"])\n",
    "print(\"latent_predictive_aucs:\", vae_extended_balanced_multilabel_results[\"latent_predictive_aucs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCD Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_ccd_encoder(\n",
    "    input_shape,\n",
    "    filter_groups,\n",
    "    units,\n",
    "    latent_dims,\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    latent_act=None,  # Leaving sigmoid as used in original paper\n",
    "):\n",
    "    encoder_inputs = tf.keras.Input(shape=input_shape)\n",
    "    encoder_compute_graph = encoder_inputs\n",
    "    \n",
    "    # Start with our convolutions\n",
    "    num_convs = 0\n",
    "    for filter_group in filter_groups:\n",
    "        for (num_filters, kernel_size) in filter_group:\n",
    "            encoder_compute_graph = tf.keras.layers.Conv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=\"SAME\",\n",
    "                activation=None,\n",
    "                name=f'encoder_conv_{num_convs}',\n",
    "            )(encoder_compute_graph)\n",
    "            num_convs += 1\n",
    "            encoder_compute_graph = tf.keras.layers.BatchNormalization()(\n",
    "                encoder_compute_graph\n",
    "            )\n",
    "            encoder_compute_graph = tf.keras.activations.relu(encoder_compute_graph)\n",
    "        # Then do a max pool here to control the parameter count of the model\n",
    "        # at the end of each group\n",
    "        encoder_compute_graph = tf.keras.layers.MaxPooling2D(\n",
    "            pool_size=max_pool_window,\n",
    "            strides=max_pool_stride,\n",
    "        )(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Flatten this guy\n",
    "    encoder_compute_graph = tf.keras.layers.Flatten()(encoder_compute_graph)\n",
    "    \n",
    "    # Add a dropout if requested\n",
    "    if drop_prob:\n",
    "        encoder_compute_graph = tf.keras.layers.Dropout(drop_prob)(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Finally, include the fully connected bottleneck here\n",
    "    for i, units in enumerate(units):\n",
    "        encoder_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(encoder_compute_graph)\n",
    "    \n",
    "    # TIme to generate the latent code here\n",
    "    encoder_compute_graph = tf.keras.layers.Dense(\n",
    "        latent_dims,\n",
    "        activation=latent_act,\n",
    "        name=\"encoder_bypass_channel\",\n",
    "    )(encoder_compute_graph)\n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        encoder_compute_graph,\n",
    "        name=\"encoder\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_ccd_encoder(\n",
    "    input_shape,\n",
    "    filter_groups,\n",
    "    units,\n",
    "    latent_dims,\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    latent_act=None,  #\"sigmoid\",  # Leaving sigmoid as used in original paper\n",
    "):\n",
    "    encoder_inputs = tf.keras.Input(shape=input_shape)\n",
    "    encoder_compute_graph = encoder_inputs\n",
    "    \n",
    "    # Start with our convolutions\n",
    "    num_convs = 0\n",
    "    for filter_group in filter_groups:\n",
    "        for (num_filters, kernel_size) in filter_group:\n",
    "            encoder_compute_graph = tf.keras.layers.Conv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=\"SAME\",\n",
    "                activation=None,\n",
    "                name=f'encoder_conv_{num_convs}',\n",
    "            )(encoder_compute_graph)\n",
    "            num_convs += 1\n",
    "            encoder_compute_graph = tf.keras.layers.BatchNormalization()(\n",
    "                encoder_compute_graph\n",
    "            )\n",
    "            encoder_compute_graph = tf.keras.activations.relu(encoder_compute_graph)\n",
    "        # Then do a max pool here to control the parameter count of the model\n",
    "        # at the end of each group\n",
    "        encoder_compute_graph = tf.keras.layers.MaxPooling2D(\n",
    "            pool_size=max_pool_window,\n",
    "            strides=max_pool_stride,\n",
    "        )(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "\n",
    "    # Flatten this guy\n",
    "    encoder_compute_graph = tf.keras.layers.Flatten()(encoder_compute_graph)\n",
    "    \n",
    "    # Add a dropout if requested\n",
    "    if drop_prob:\n",
    "        encoder_compute_graph = tf.keras.layers.Dropout(drop_prob)(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Finally, include the fully connected bottleneck here\n",
    "    for i, units in enumerate(units):\n",
    "        encoder_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(encoder_compute_graph)\n",
    "    \n",
    "    # TIme to generate the latent code here\n",
    "    encoder_compute_graph = tf.keras.layers.Dense(\n",
    "        latent_dims,\n",
    "        activation=latent_act,\n",
    "        name=\"encoder_bypass_channel\",\n",
    "    )(encoder_compute_graph)\n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        encoder_compute_graph,\n",
    "        name=\"encoder\",\n",
    "    )\n",
    "\n",
    "def construct_ccd_decoder(units, num_outputs):\n",
    "    decoder_layers = [\n",
    "        tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation=tf.nn.relu,\n",
    "            name=f\"decoder_dense_{i+1}\",\n",
    "        ) for i, units in enumerate(units)\n",
    "    ]\n",
    "    return tf.keras.Sequential(\n",
    "        [tf.keras.layers.Flatten()] +\n",
    "        decoder_layers + [\n",
    "        tf.keras.layers.Dense(\n",
    "            num_outputs if num_outputs > 2 else 1,\n",
    "            activation=None,\n",
    "            name=\"decoder_model_output\",\n",
    "        )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.OCACE.topicModel as CCD\n",
    "import concepts_xai.evaluation.metrics.oracle as oracle\n",
    "import concepts_xai.evaluation.metrics.completeness as completeness\n",
    "\n",
    "############################################################################\n",
    "## Experiment loop\n",
    "############################################################################\n",
    "\n",
    "def ccd_experiment_loop(\n",
    "    experiment_config,\n",
    "    datasets,\n",
    "    load_from_cache=False,\n",
    "    oracle_matrix_cache=None,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    oracle_matrix_cache = oracle_matrix_cache or {}\n",
    "    experiment_variables = dict(\n",
    "        task_accuracies=[],\n",
    "        task_aucs=[],\n",
    "        reconstruction_accuracies=[],\n",
    "        reconstruction_aucs=[],\n",
    "        purity_scores=[],\n",
    "        non_oracle_purity_scores=[],\n",
    "        purity_matrices=[],\n",
    "        aligned_purity_matrices=[],\n",
    "        oracle_matrices=[],\n",
    "        completeness_scores=[],\n",
    "        direct_completeness_scores=[],\n",
    "        mean_similarities=[],\n",
    "    )\n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(datasets):\n",
    "                print(\"Found\", len(loaded_means), \"means for variable\", var_name, \"vs\", len(datasets), \"datasets\")\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)}.'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    utils.serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    for (\n",
    "        ds_name,\n",
    "        (x_train, y_train, c_train), \n",
    "        (x_test, y_test, c_test),\n",
    "    ) in datasets[start_ind:]:\n",
    "        num_concepts = experiment_config[\"num_concepts\"]\n",
    "        print(\"Training with concepts\", num_concepts, \"in dataset\", ds_name)\n",
    "        task_accs = []\n",
    "        recon_accs = []\n",
    "        aucs = []\n",
    "        recon_aucs = []\n",
    "        purity_mats = []\n",
    "        aligned_purity_mats = []\n",
    "        oracle_mats = []\n",
    "        purities = []\n",
    "        non_oracle_purities = []\n",
    "        compl_scores = []\n",
    "        dir_compl_scores = []\n",
    "        mean_sims = []\n",
    "        \n",
    "        channels_axis = (\n",
    "            -1 if experiment_config.get(\"data_format\", \"channels_last\") == \"channels_last\"\n",
    "            else 1\n",
    "        )\n",
    "        if experiment_config[\"num_outputs\"] == 1:\n",
    "            acc_fn = lambda y_true, y_pred: sklearn.metrics.roc_auc_score(\n",
    "                y_true,\n",
    "                y_pred\n",
    "            )\n",
    "        else:\n",
    "            acc_fn = lambda y_true, y_pred: sklearn.metrics.roc_auc_score(\n",
    "                tf.keras.utils.to_categorical(y_true),\n",
    "                scipy.special.softmax(y_pred, axis=-1),\n",
    "                multi_class='ovo',\n",
    "            )\n",
    "        \n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} for {num_concepts} concepts\")\n",
    "            \n",
    "            # Proceed to do and end-to-end model in case we want to\n",
    "            # do some task-specific pretraining\n",
    "            end_to_end_model, encoder, decoder = construct_end_to_end_model(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "                learning_rate=experiment_config[\"learning_rate\"],\n",
    "                encoder=construct_ccd_encoder(\n",
    "                    input_shape=experiment_config[\"input_shape\"],\n",
    "                    filter_groups=experiment_config[\"encoder_filter_groups\"],\n",
    "                    latent_dims=experiment_config[\"latent_dims\"],\n",
    "                    units=experiment_config[\"encoder_units\"],\n",
    "                    drop_prob=experiment_config.get(\"drop_prob\", 0.5),\n",
    "                    max_pool_window=experiment_config.get(\"max_pool_window\", (2, 2)),\n",
    "                    max_pool_stride=experiment_config.get(\"max_pool_stride\", (2, 2)),\n",
    "                    latent_act=experiment_config.get(\"latent_act\", None),\n",
    "                ),\n",
    "                decoder=construct_decoder(\n",
    "                    units=experiment_config[\"decoder_units\"],\n",
    "                    num_outputs=experiment_config[\"num_outputs\"],\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            print(\"\\tModel pre-training...\")\n",
    "            \n",
    "            early_stopping_monitor = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=experiment_config.get(\n",
    "                    \"early_stop_metric\",\n",
    "                    \"val_loss\",\n",
    "                ),\n",
    "                min_delta=experiment_config[\"min_delta\"],\n",
    "                patience=experiment_config[\"patience\"],\n",
    "                restore_best_weights=True,\n",
    "                verbose=2,\n",
    "                mode=experiment_config.get(\n",
    "                    \"early_stop_mode\",\n",
    "                    \"min\",\n",
    "                ),\n",
    "            )\n",
    "            end_to_end_model.fit(\n",
    "                x=x_train,\n",
    "                y=y_train,\n",
    "                epochs=experiment_config[\"max_epochs\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                callbacks=[\n",
    "                    early_stopping_monitor,\n",
    "                ],\n",
    "                validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                verbose=verbosity,\n",
    "            )\n",
    "            print(\"\\t\\tModel pre-training completed\")\n",
    "            print(\"\\tSerializing model\")\n",
    "            encoder.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/{ds_name}_encoder_num_concepts_{num_concepts}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            decoder.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/{ds_name}_decoder_num_concepts_{num_concepts}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            print(\"\\tEvaluating model\")\n",
    "            \n",
    "            test_result = end_to_end_model.evaluate(\n",
    "                x_test,\n",
    "                y_test,\n",
    "                verbose=0,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            if experiment_config[\"num_outputs\"] > 1:\n",
    "                task_accs.append(test_result['sparse_top_k_categorical_accuracy'])\n",
    "                \n",
    "                # Then lets apply a softmax activation over all the probability\n",
    "                # classes\n",
    "                preds = scipy.special.softmax(end_to_end_model.predict(x_test), axis=-1)\n",
    "\n",
    "                # And select just the labels that are in fact being used\n",
    "                one_hot_labels = tf.keras.utils.to_categorical(y_test)\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    one_hot_labels,\n",
    "                    preds,\n",
    "                    multi_class='ovo',\n",
    "                ))\n",
    "            else:\n",
    "                task_accs.append(test_result['binary_accuracy'])\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    y_test,\n",
    "                    end_to_end_model.predict(x_test),\n",
    "                ))\n",
    "            \n",
    "            # Now extract our concept vectors\n",
    "            topic_model = CCD.TopicModel(\n",
    "                concepts_to_labels_model=decoder,\n",
    "                n_channels=experiment_config[\"latent_dims\"],\n",
    "                n_concepts=num_concepts,\n",
    "                threshold=experiment_config.get(\"threshold\", 0.5),\n",
    "                loss_fn=end_to_end_model.loss,\n",
    "                top_k=experiment_config.get(\"top_k\", 32),\n",
    "                lambda1=experiment_config.get(\"lambda1\", 0.1),\n",
    "                lambda2=experiment_config.get(\"lambda2\", 0.1),\n",
    "                seed=experiment_config.get(\"seed\", None),\n",
    "                eps=experiment_config.get(\"eps\", 1e-5),\n",
    "                data_format=experiment_config.get(\n",
    "                    \"data_format\",\n",
    "                    \"channels_last\"\n",
    "                ),\n",
    "                allow_gradient_flow_to_c2l=experiment_config.get(\n",
    "                    'allow_gradient_flow_to_c2l',\n",
    "                    False,\n",
    "                ),\n",
    "                acc_metric=(\n",
    "                    tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "                    if experiment_config[\"num_outputs\"] > 1 else\n",
    "                    tf.keras.metrics.BinaryAccuracy()\n",
    "                ),\n",
    "            )\n",
    "            topic_model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(\n",
    "                    experiment_config.get(\"learning_rate\", 1e-3),\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Train it for a few epochs\n",
    "            print(\"\\tTopic model training...\")\n",
    "            topic_model.fit(\n",
    "                x=encoder(x_train),\n",
    "                y=y_train,\n",
    "                epochs=experiment_config[\"topic_model_train_epochs\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                verbose=verbosity,\n",
    "            )\n",
    "            print(\"\\t\\tTopic model training completed\")\n",
    "            \n",
    "            print(\"\\tSerializing model\")\n",
    "            topic_model.g_model.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/{ds_name}_topic_g_model_num_concepts_{num_concepts}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            np.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/{ds_name}_topic_vector_num_concepts_{num_concepts}_trial_{trial}.npy\"\n",
    "                ),\n",
    "                topic_model.topic_vector.numpy(),\n",
    "            )\n",
    "            print(\"\\tEvaluating model\")\n",
    "            \n",
    "            topic_result = topic_model.evaluate(\n",
    "                encoder(x_test),\n",
    "                y_test,\n",
    "                verbose=0,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            \n",
    "            if experiment_config[\"num_outputs\"] > 1:\n",
    "                recon_accs.append(topic_result['accuracy'])\n",
    "                # Then lets apply a softmax activation over all the probability\n",
    "                # classes\n",
    "                preds = scipy.special.softmax(\n",
    "                    topic_model(encoder(x_test))[0],\n",
    "                    axis=-1,\n",
    "                )\n",
    "\n",
    "                one_hot_labels = tf.keras.utils.to_categorical(y_test)\n",
    "                recon_aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    one_hot_labels,\n",
    "                    preds,\n",
    "                    multi_class='ovo',\n",
    "                ))\n",
    "            else:\n",
    "                recon_accs.append(topic_result['accuracy'])\n",
    "                recon_aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    y_test,\n",
    "                    topic_model(encoder(x_test))[0],\n",
    "                ))\n",
    "            mean_sims.append(topic_result['mean_sim'])\n",
    "            \n",
    "            print(\n",
    "                f\"\\t\\tTest auc = {aucs[-1]:.4f}, \"\n",
    "                f\"task accuracy = {task_accs[-1]:.4f}, \"\n",
    "                f\"task reconstruction accuracy = {recon_accs[-1]:.4f}, \"\n",
    "                f\"task reconstruction auc = {recon_aucs[-1]:.4f}, \"\n",
    "                f\"mean concept similarity = {mean_sims[-1]:.4f}\"\n",
    "            )\n",
    "            \n",
    "                        \n",
    "            # We start by extracting a completeness score for the extracted\n",
    "            # concept vectors\n",
    "            print(f\"\\t\\tComputing completeness scores...\")\n",
    "            compl_score, _ = completeness.completeness_score(\n",
    "                X=x_test,\n",
    "                y=y_test,\n",
    "                features_to_concepts_fn=encoder,\n",
    "                concepts_to_labels_model=decoder,\n",
    "                concept_vectors=np.transpose(topic_model.topic_vector.numpy()),\n",
    "                task_loss=end_to_end_model.loss,\n",
    "                channels_axis=channels_axis,\n",
    "                concept_score_fn=lambda f, c: completeness.dot_prod_concept_score(\n",
    "                    features=f,\n",
    "                    concept_vectors=c,\n",
    "                    channels_axis=channels_axis,\n",
    "                    beta=experiment_config.get(\"threshold\", 0.5),\n",
    "                ),\n",
    "                acc_fn=acc_fn,\n",
    "            )\n",
    "            compl_scores.append(compl_score)\n",
    "            \n",
    "            dir_compl_score, _ = completeness.direct_completeness_score(\n",
    "                X=x_test,\n",
    "                y=y_test,\n",
    "                features_to_concepts_fn=encoder,\n",
    "                concept_vectors=np.transpose(topic_model.topic_vector.numpy()),\n",
    "                task_loss=end_to_end_model.loss,\n",
    "                channels_axis=channels_axis,\n",
    "                concept_score_fn=lambda f, c: completeness.dot_prod_concept_score(\n",
    "                    features=f,\n",
    "                    concept_vectors=c,\n",
    "                    channels_axis=channels_axis,\n",
    "                    beta=experiment_config.get(\"threshold\", 0.5),\n",
    "                ),\n",
    "                acc_fn=acc_fn,\n",
    "            )\n",
    "            dir_compl_scores.append(dir_compl_score)\n",
    "            \n",
    "            print(\n",
    "                f\"\\t\\t\\tCompleteness Score: {compl_scores[-1]:.4f} \"\n",
    "                f\"and Direct Completeness Score: {dir_compl_scores[-1]:.4f}\"\n",
    "            )\n",
    "            \n",
    "            print(f\"\\t\\tComputing purity score...\")\n",
    "            concept_scores = topic_model.concept_scores(encoder(x_test)).numpy()\n",
    "            purity_score, (purity_mat, aligned_purity_mat), oracle_mat = oracle.oracle_impurity_score(\n",
    "                c_soft=concept_scores,\n",
    "                c_true=c_test,\n",
    "                output_matrices=True,\n",
    "                alignment_function=oracle.max_alignment_matrix,\n",
    "                oracle_matrix=oracle_matrix_cache.get(ds_name),\n",
    "            )\n",
    "            \n",
    "            purity_mats.append(purity_mat)\n",
    "            aligned_purity_mats.append(aligned_purity_mat)\n",
    "            oracle_mats.append(oracle_mat)\n",
    "            purities.append(purity_score)\n",
    "            print(f\"\\t\\t\\tDone {purity_score:.4f}\")\n",
    "\n",
    "            print(\"\\t\\tComputing non-oracle purity score...\")\n",
    "            non_oracle_purities.append(oracle.oracle_impurity_score(\n",
    "                c_soft=concept_scores,\n",
    "                c_true=c_test,\n",
    "                alignment_function=oracle.max_alignment_matrix,\n",
    "                oracle_matrix=construct_trivial_auc_mat(\n",
    "                    c_test.shape[-1]\n",
    "                ),\n",
    "                purity_matrix=aligned_purity_mat,\n",
    "            ))\n",
    "            print(f\"\\t\\t\\tDone {non_oracle_purities[-1]:.4f}\")\n",
    "            print(\"\\t\\tDone with trial\", trial + 1)\n",
    "\n",
    "        task_acc_mean, task_acc_std = np.mean(task_accs), np.std(task_accs)\n",
    "        experiment_variables[\"task_accuracies\"].append((task_acc_mean, task_acc_std))\n",
    "        print(f\"\\tTest task accuracy: {task_acc_mean:.4f}  {task_acc_std:.4f}\")\n",
    "\n",
    "        recon_acc_mean, recon_acc_std = np.mean(recon_accs), np.std(recon_accs)\n",
    "        experiment_variables[\"reconstruction_accuracies\"].append((recon_acc_mean, recon_acc_std))\n",
    "        print(f\"\\tTest reconstruction accuracy: {recon_acc_mean:.4f}  {recon_acc_std:.4f}\")\n",
    "\n",
    "        task_auc_mean, task_auc_std = np.mean(aucs), np.std(aucs)\n",
    "        experiment_variables[\"task_aucs\"].append((task_auc_mean, task_auc_std))\n",
    "        print(f\"\\tTest task AUC: {task_auc_mean:.4f}  {task_auc_std:.4f}\")\n",
    "        \n",
    "        recon_auc_mean, recon_auc_std = np.mean(recon_aucs), np.std(recon_aucs)\n",
    "        experiment_variables[\"reconstruction_aucs\"].append((recon_auc_mean, recon_auc_std))\n",
    "        print(f\"\\tTest reconstruction accuracy: {recon_auc_mean:.4f}  {recon_auc_std:.4f}\")\n",
    "        \n",
    "        mean_sim_mean, mean_sim_std = np.mean(mean_sims), np.std(mean_sims)\n",
    "        experiment_variables[\"mean_similarities\"].append((mean_sim_mean, mean_sim_std))\n",
    "        print(f\"\\tMean concept similarity: {mean_sim_mean:.4f}  {mean_sim_std:.4f}\")\n",
    "        \n",
    "        \n",
    "        compl_score_mean, compl_score_std = np.mean(compl_scores), np.std(compl_scores)\n",
    "        experiment_variables[\"completeness_scores\"].append((compl_score_mean, compl_score_std))\n",
    "        print(f\"\\tCompleteness Score: {compl_score_mean:.4f}  {compl_score_std:.4f}\")\n",
    "        \n",
    "        dir_compl_score_mean, dir_compl_score_std = np.mean(dir_compl_scores), np.std(dir_compl_scores)\n",
    "        experiment_variables[\"direct_completeness_scores\"].append((dir_compl_score_mean, dir_compl_score_std))\n",
    "        print(f\"\\tDirect completeness Score: {dir_compl_score_mean:.4f}  {dir_compl_score_std:.4f}\")\n",
    "\n",
    "        purity_mats = np.stack(purity_mats, axis=0)\n",
    "        purity_mat_mean = np.mean(purity_mats, axis=0)\n",
    "        purity_mat_std = np.std(purity_mats, axis=0)\n",
    "        print(\"\\tPurity matrix:\")\n",
    "        for i in range(purity_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(purity_mat_mean.shape[1]):\n",
    "                line += f'{purity_mat_mean[i, j]:.4f}  {purity_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"purity_matrices\"].append((purity_mat_mean, purity_mat_std))\n",
    "        \n",
    "        aligned_purity_mats = np.stack(aligned_purity_mats, axis=0)\n",
    "        aligned_purity_mat_mean = np.mean(aligned_purity_mats, axis=0)\n",
    "        aligned_purity_mat_std = np.std(aligned_purity_mats, axis=0)\n",
    "        print(\"\\tAligned purity matrix:\")\n",
    "        for i in range(aligned_purity_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(aligned_purity_mat_mean.shape[1]):\n",
    "                line += f'{aligned_purity_mat_mean[i, j]:.4f}  {aligned_purity_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"aligned_purity_matrices\"].append((aligned_purity_mat_mean, aligned_purity_mat_std))\n",
    "\n",
    "\n",
    "        oracle_mats = np.stack(oracle_mats, axis=0)\n",
    "        oracle_mat_mean = np.mean(oracle_mats, axis=0)\n",
    "        oracle_mat_std = np.std(oracle_mats, axis=0)\n",
    "        print(\"\\tOracle matrix:\")\n",
    "        for i in range(oracle_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(oracle_mat_mean.shape[1]):\n",
    "                line += f'{oracle_mat_mean[i, j]:.4f}  {oracle_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"oracle_matrices\"].append((oracle_mat_mean, oracle_mat_std))\n",
    "\n",
    "        purity_mean, purity_std = np.mean(purities), np.std(purities)\n",
    "        experiment_variables[\"purity_scores\"].append((purity_mean, purity_std))\n",
    "        print(f\"\\tPurity score: {purity_mean:.4f}  {purity_std:.4f}\")\n",
    "\n",
    "        non_oracle_purity_mean, non_oracle_purity_std = np.mean(non_oracle_purities), np.std(non_oracle_purities)\n",
    "        experiment_variables[\"non_oracle_purity_scores\"].append((non_oracle_purity_mean, non_oracle_purity_std))\n",
    "        print(f\"\\tNon-oracle purity score: {non_oracle_purity_mean:.4f}  {non_oracle_purity_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables\n",
    "\n",
    "def ccd_bottleneck_concept_predict_experiment_loop(\n",
    "    experiment_config,\n",
    "    datasets,\n",
    "    load_from_cache=False,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    experiment_variables = dict(\n",
    "        latent_avg_concept_predictive_accuracies=[],\n",
    "        latent_avg_concept_predictive_aucs=[],\n",
    "    )\n",
    "\n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                print(\"Could not find\", f'\"{file_name}_means.npz\" or \"{file_name}_stds.npz\" in cache.')\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(datasets):\n",
    "                print(\"Found\", len(loaded_means), \"means for variable\", var_name, \"vs\", len(datasets), \"datasets\")\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)} ({file_name}).'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            print(experiment_variables)\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    utils.serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    for (ds_name, (x_train, y_train, c_train), (x_test, y_test, c_test)) in datasets[start_ind:]:\n",
    "        num_concepts = experiment_config['num_concepts']\n",
    "        print(\"Training with num concepts\", num_concepts, \"in dataset\", ds_name)\n",
    "        avg_concept_accs = []\n",
    "        avg_concept_aucs = []\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']}\")\n",
    "            encoder = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/{ds_name}_encoder_num_concepts_{num_concepts}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            decoder = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/{ds_name}_decoder_num_concepts_{num_concepts}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            \n",
    "            g_model = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/{ds_name}_topic_g_model_num_concepts_{num_concepts}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            topic_vector = np.load(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/{ds_name}_topic_vector_num_concepts_{num_concepts}_trial_{trial}.npy\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            \n",
    "            # Now extract our concept vectors\n",
    "            topic_model = CCD.TopicModel(\n",
    "                concepts_to_labels_model=decoder,\n",
    "                n_channels=experiment_config[\"latent_dims\"],\n",
    "                n_concepts=num_concepts,\n",
    "                threshold=experiment_config.get(\"threshold\", 0.5),\n",
    "                loss_fn=(\n",
    "                    tf.keras.losses.BinaryCrossentropy(from_logits=True) if (experiment_config[\"num_outputs\"] <= 2)\n",
    "                    else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "                ),\n",
    "                top_k=experiment_config.get(\"top_k\", 32),\n",
    "                lambda1=experiment_config.get(\"lambda1\", 0.1),\n",
    "                lambda2=experiment_config.get(\"lambda2\", 0.1),\n",
    "                seed=experiment_config.get(\"seed\", None),\n",
    "                eps=experiment_config.get(\"eps\", 1e-5),\n",
    "                data_format=experiment_config.get(\n",
    "                    \"data_format\",\n",
    "                    \"channels_last\"\n",
    "                ),\n",
    "                allow_gradient_flow_to_c2l=experiment_config.get(\n",
    "                    'allow_gradient_flow_to_c2l',\n",
    "                    False,\n",
    "                ),\n",
    "                acc_metric=(\n",
    "                    tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "                    if experiment_config[\"num_outputs\"] > 1 else\n",
    "                    tf.keras.metrics.BinaryAccuracy()\n",
    "                ),\n",
    "                initial_topic_vector=topic_vector,\n",
    "            )\n",
    "            \n",
    "            \n",
    "            concept_scores = topic_model.concept_scores(encoder(x_test)).numpy()\n",
    "            \n",
    "            current_accs = []\n",
    "            current_aucs = []\n",
    "            \n",
    "            for concept_idx in range(experiment_config[\"data_concepts\"]):\n",
    "                predictive_decoder = construct_decoder(\n",
    "                    units=experiment_config[\"latent_decoder_units\"],\n",
    "                    num_outputs=1,\n",
    "                )\n",
    "                predictive_decoder.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(experiment_config[\"learning_rate\"]),\n",
    "                    loss=(\n",
    "                        tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "                    ),\n",
    "                    metrics=[\n",
    "                        \"binary_accuracy\"\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "                print(\"\\tTraining model for concept\", concept_idx)\n",
    "                train_codes = topic_model.concept_scores(encoder(x_train)).numpy()\n",
    "                test_codes = topic_model.concept_scores(encoder(x_test)).numpy()\n",
    "                predictive_decoder.fit(\n",
    "                    x=train_codes,\n",
    "                    y=c_train[:, concept_idx],\n",
    "                    epochs=experiment_config[\"predictor_max_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "                print(\"\\t\\tEvaluating model\")\n",
    "                test_result = predictive_decoder.evaluate(\n",
    "                    test_codes,\n",
    "                    c_test[:, concept_idx],\n",
    "                    verbose=0,\n",
    "                    return_dict=True,\n",
    "                )\n",
    "                current_accs.append(\n",
    "                    test_result['binary_accuracy']\n",
    "                )\n",
    "\n",
    "                current_aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    c_test[:, concept_idx],\n",
    "                    predictive_decoder.predict(test_codes),\n",
    "                ))\n",
    "            avg_concept_accs.append(np.mean(current_accs))\n",
    "            avg_concept_aucs.append(np.mean(current_aucs))\n",
    "            print(\n",
    "                f\"\\t\\tTest avg concept AUC = {avg_concept_aucs[-1]:.4f}, \"\n",
    "                f\"avg concept accuracy = {avg_concept_accs[-1]:.4f}\"\n",
    "            )\n",
    "            print(\"\\tDone with trial\", trial + 1)\n",
    "\n",
    "        avg_concept_acc_mean, avg_concept_acc_std = np.mean(avg_concept_accs), np.std(avg_concept_accs)\n",
    "        experiment_variables[\"latent_avg_concept_predictive_accuracies\"].append((avg_concept_acc_mean, avg_concept_acc_std))\n",
    "        print(f\"\\tTest average concept accuracy: {avg_concept_acc_mean:.4f}  {avg_concept_acc_std:.4f}\")\n",
    "\n",
    "        avg_concept_auc_mean, avg_concept_auc_std = np.mean(avg_concept_aucs), np.std(avg_concept_aucs)\n",
    "        experiment_variables[\"latent_avg_concept_predictive_aucs\"].append((avg_concept_auc_mean, avg_concept_auc_std))\n",
    "        print(f\"\\tTest average concept AUC: {avg_concept_auc_mean:.4f}  {avg_concept_auc_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables\n",
    "\n",
    "def ccd_compute_k(y, batch_size):\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    avg_class_ratio = np.mean(counts) / y.shape[0]\n",
    "    return int((avg_class_ratio * batch_size) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(completeness)\n",
    "reload(CBM)\n",
    "reload(CCD)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "ccd_balanced_multiclass_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    topic_model_train_epochs=50,\n",
    "    num_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    latent_dims=10,\n",
    "    threshold=0.0,\n",
    "    top_k=ccd_compute_k(y=balanced_multiclass_task_bin_concepts_dep_0_complete_train[1], batch_size=32),\n",
    "    lambda1=0.1,\n",
    "    lambda2=0.1,\n",
    "    seed=42,\n",
    "    eps=1e-5,\n",
    "    learning_rate=1e-3,\n",
    "    encoder_filter_groups=[\n",
    "        [(8, (7, 7))],\n",
    "        [(16, (5, 5))],\n",
    "        [(32, (3, 3))],\n",
    "        [(64, (3, 3))],\n",
    "    ],\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[64, 64],\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    pax_pool_stride=2,\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    \n",
    "    latent_decoder_units=[64, 64],\n",
    "    predictor_max_epochs=100,\n",
    "    data_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    \n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"ccd/balanced_multiclass_thresh_0_num_concepts_{balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}\"\n",
    "    ),\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    holdout_fraction=0.1,\n",
    "    trials=5,\n",
    "    verbosity=0,\n",
    "    early_stop_metric=\"val_loss\",\n",
    "    early_stop_mode=\"min\",\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(ccd_balanced_multiclass_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "ccd_balanced_multiclass_figure_dir = os.path.join(ccd_balanced_multiclass_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(ccd_balanced_multiclass_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "ccd_balanced_multiclass_results = ccd_experiment_loop(\n",
    "    experiment_config=ccd_balanced_multiclass_experiment_config,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "    load_from_cache=True,\n",
    "    oracle_matrix_cache=balanced_oracle_matrix_cache,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"task_accuracies:\", ccd_balanced_multiclass_results[\"task_accuracies\"])\n",
    "print(\"reconstruction_accuracies:\", ccd_balanced_multiclass_results[\"reconstruction_accuracies\"])\n",
    "print(\"task_aucs:\", ccd_balanced_multiclass_results[\"task_aucs\"])\n",
    "print(\"reconstruction_aucs:\", ccd_balanced_multiclass_results[\"reconstruction_aucs\"])\n",
    "print(\"purity_scores:\", ccd_balanced_multiclass_results[\"purity_scores\"])\n",
    "print(\"non_oracle_purity_scores:\", ccd_balanced_multiclass_results[\"non_oracle_purity_scores\"])\n",
    "print(\"completeness_scores:\", ccd_balanced_multiclass_results[\"completeness_scores\"])\n",
    "print(\"direct_completeness_scores:\", ccd_balanced_multiclass_results[\"direct_completeness_scores\"])\n",
    "print(\"mean_similarities:\", ccd_balanced_multiclass_results[\"mean_similarities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccd_balanced_multiclass_results.update(ccd_bottleneck_concept_predict_experiment_loop(\n",
    "    experiment_config=ccd_balanced_multiclass_experiment_config,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "    load_from_cache=True,\n",
    "))\n",
    "print(\"latent_avg_concept_predictive_accuracies:\", ccd_balanced_multiclass_results[\"latent_avg_concept_predictive_accuracies\"])\n",
    "print(\"latent_avg_concept_predictive_aucs:\", ccd_balanced_multiclass_results[\"latent_avg_concept_predictive_aucs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(completeness)\n",
    "reload(CBM)\n",
    "reload(CCD)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "ccd_balanced_multiclass_extended_experiment_config = dict(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    topic_model_train_epochs=50,\n",
    "    num_concepts=(2 * balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]),\n",
    "    latent_dims=10,\n",
    "    threshold=0.0,\n",
    "    top_k=ccd_compute_k(y=balanced_multiclass_task_bin_concepts_dep_0_complete_train[1], batch_size=32),\n",
    "    lambda1=0.1,\n",
    "    lambda2=0.1,\n",
    "    seed=42,\n",
    "    eps=1e-5,\n",
    "    learning_rate=1e-3,\n",
    "    encoder_filter_groups=[\n",
    "        [(8, (7, 7))],\n",
    "        [(16, (5, 5))],\n",
    "        [(32, (3, 3))],\n",
    "        [(64, (3, 3))],\n",
    "    ],\n",
    "    encoder_units=[64, 64],\n",
    "    decoder_units=[64, 64],\n",
    "    \n",
    "    latent_decoder_units=[64, 64],\n",
    "    predictor_max_epochs=100,\n",
    "    data_concepts=balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1],\n",
    "    \n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    pax_pool_stride=2,\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    patience=float(\"inf\"),\n",
    "    min_delta=1e-5,\n",
    "    results_dir=os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"ccd/balanced_multiclass_thresh_0_num_concepts_{2* balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]}\"\n",
    "    ),\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    holdout_fraction=0.1,\n",
    "    trials=5,\n",
    "    verbosity=0,\n",
    "    early_stop_metric=\"val_loss\",\n",
    "    early_stop_mode=\"min\",\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(ccd_balanced_multiclass_extended_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "ccd_balanced_multiclass_extended_figure_dir = os.path.join(ccd_balanced_multiclass_extended_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(ccd_balanced_multiclass_extended_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "ccd_balanced_multiclass_extended_results = ccd_experiment_loop(\n",
    "    experiment_config=ccd_balanced_multiclass_extended_experiment_config,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "    load_from_cache=True,\n",
    "    oracle_matrix_cache=balanced_oracle_matrix_cache,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"task_accuracies:\", ccd_balanced_multiclass_extended_results[\"task_accuracies\"])\n",
    "print(\"reconstruction_accuracies:\", ccd_balanced_multiclass_extended_results[\"reconstruction_accuracies\"])\n",
    "print(\"task_aucs:\", ccd_balanced_multiclass_extended_results[\"task_aucs\"])\n",
    "print(\"reconstruction_aucs:\", ccd_balanced_multiclass_extended_results[\"reconstruction_aucs\"])\n",
    "print(\"purity_scores:\", ccd_balanced_multiclass_extended_results[\"purity_scores\"])\n",
    "print(\"non_oracle_purity_scores:\", ccd_balanced_multiclass_extended_results[\"non_oracle_purity_scores\"])\n",
    "print(\"completeness_scores:\", ccd_balanced_multiclass_extended_results[\"completeness_scores\"])\n",
    "print(\"direct_completeness_scores:\", ccd_balanced_multiclass_extended_results[\"direct_completeness_scores\"])\n",
    "print(\"mean_similarities:\", ccd_balanced_multiclass_extended_results[\"mean_similarities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccd_balanced_multiclass_extended_results.update(ccd_bottleneck_concept_predict_experiment_loop(\n",
    "    experiment_config=ccd_balanced_multiclass_extended_experiment_config,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    "    load_from_cache=True,\n",
    "))\n",
    "print(\"latent_avg_concept_predictive_accuracies:\", ccd_balanced_multiclass_extended_results[\"latent_avg_concept_predictive_accuracies\"])\n",
    "print(\"latent_avg_concept_predictive_aucs:\", ccd_balanced_multiclass_extended_results[\"latent_avg_concept_predictive_aucs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENN Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.methods.SENN.base_senn as SENN\n",
    "import concepts_xai.methods.SENN.aggregators as aggregators\n",
    "reload(SENN)\n",
    "reload(aggregators)\n",
    "\n",
    "\n",
    "def construct_senn_coefficient_model(units, num_concepts, num_outputs):\n",
    "    decoder_layers = [tf.keras.layers.Flatten()] + [\n",
    "        tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation=tf.nn.relu,\n",
    "            name=f\"coefficient_model_dense_{i+1}\",\n",
    "        ) for i, units in enumerate(units)\n",
    "    ]\n",
    "    return tf.keras.Sequential(decoder_layers + [\n",
    "        tf.keras.layers.Dense(\n",
    "            num_concepts * num_outputs,\n",
    "            activation=None,\n",
    "            name=\"coefficient_model_output\",\n",
    "        ),\n",
    "        tf.keras.layers.Reshape([num_outputs, num_concepts])\n",
    "    ])\n",
    "\n",
    "def construct_senn_encoder(\n",
    "    input_shape,\n",
    "    filter_groups,\n",
    "    units,\n",
    "    latent_dims,\n",
    "    drop_prob=0.5,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    include_norm=False,\n",
    "    include_pool=False,\n",
    "):\n",
    "    encoder_inputs = tf.keras.Input(shape=input_shape)\n",
    "    encoder_compute_graph = encoder_inputs\n",
    "    \n",
    "    # Start with our convolutions\n",
    "    num_convs = 0\n",
    "    for filter_group in filter_groups:\n",
    "        for filter_args in filter_group:\n",
    "            if len(filter_args) == 2:\n",
    "                filter_args = (*filter_args, 1)\n",
    "            (num_filters, kernel_size, stride) = filter_args\n",
    "            encoder_compute_graph = tf.keras.layers.Conv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                strides=stride,\n",
    "                padding=\"SAME\",\n",
    "                activation=None if include_norm else \"relu\",\n",
    "                name=f'encoder_conv_{num_convs}',\n",
    "            )(encoder_compute_graph)\n",
    "            num_convs += 1\n",
    "            if include_norm:\n",
    "                encoder_compute_graph = tf.keras.layers.BatchNormalization()(\n",
    "                    encoder_compute_graph\n",
    "                )\n",
    "                encoder_compute_graph = tf.keras.activations.relu(encoder_compute_graph)\n",
    "        if include_pool:\n",
    "            # Then do a max pool here to control the parameter count of the model\n",
    "            # at the end of each group\n",
    "            encoder_compute_graph = tf.keras.layers.MaxPooling2D(\n",
    "                pool_size=max_pool_window,\n",
    "                strides=max_pool_stride,\n",
    "            )(\n",
    "                encoder_compute_graph\n",
    "            )\n",
    "    \n",
    "    # Flatten this guy\n",
    "    encoder_compute_graph = tf.keras.layers.Flatten()(encoder_compute_graph)\n",
    "    \n",
    "    # Add a dropout if requested\n",
    "    if drop_prob:\n",
    "        encoder_compute_graph = tf.keras.layers.Dropout(drop_prob)(\n",
    "            encoder_compute_graph\n",
    "        )\n",
    "    \n",
    "    # Finally, include the fully connected bottleneck here\n",
    "    for i, units in enumerate(units):\n",
    "        encoder_compute_graph = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu',\n",
    "            name=f\"encoder_dense_{i}\",\n",
    "        )(encoder_compute_graph)\n",
    "\n",
    "    mean = tf.keras.layers.Dense(latent_dims, activation=None, name=\"means\")(encoder_compute_graph)\n",
    "    log_var = tf.keras.layers.Dense(latent_dims, activation=None, name=\"log_var\")(encoder_compute_graph)\n",
    "    senn_encoder = tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        mean,\n",
    "        name=\"senn_encoder\",\n",
    "    )\n",
    "    vae_encoder = tf.keras.Model(\n",
    "        encoder_inputs,\n",
    "        [mean, log_var],\n",
    "        name=\"vae_encoder\",\n",
    "    )\n",
    "    return senn_encoder, vae_encoder\n",
    "\n",
    "def construct_senn_model(\n",
    "    concept_encoder,\n",
    "    concept_decoder,\n",
    "    coefficient_model,\n",
    "    num_outputs,\n",
    "    regularization_strength=0.1,\n",
    "    learning_rate=1e-3,\n",
    "    sparsity_strength=2e-5,\n",
    "):\n",
    "    def reconstruction_loss_fn(y_true, y_pred):\n",
    "        return vae_losses.bernoulli_fn_wrapper()(y_true, concept_decoder(y_pred))\n",
    "    \n",
    "    senn_model = SENN.SelfExplainingNN(\n",
    "        encoder_model=concept_encoder,\n",
    "        coefficient_model=coefficient_model,\n",
    "        aggregator_fn=(\n",
    "            aggregators.multiclass_additive_aggregator if (num_outputs > 2)\n",
    "            else aggregators.scalar_additive_aggregator\n",
    "        ),\n",
    "        task_loss_fn=(\n",
    "            tf.keras.losses.BinaryCrossentropy(from_logits=True) if (num_outputs <= 2)\n",
    "            else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        ),\n",
    "        reconstruction_loss_fn=reconstruction_loss_fn,\n",
    "        regularization_strength=regularization_strength,\n",
    "        sparsity_strength=sparsity_strength,\n",
    "        name=\"SENN\",\n",
    "        metrics=[\n",
    "            tf.keras.metrics.BinaryAccuracy() if (num_outputs <= 2)\n",
    "            else tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "        ],\n",
    "    )\n",
    "    senn_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    )\n",
    "    return senn_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concepts_xai.evaluation.metrics.oracle as oracle\n",
    "\n",
    "############################################################################\n",
    "## Experiment loop\n",
    "############################################################################\n",
    "\n",
    "def construct_trivial_auc_mat(num_concepts):\n",
    "    result = np.ones((num_concepts, num_concepts), dtype=np.float32) * 0.5\n",
    "    return result + np.eye(num_concepts, dtype=np.float32) * 0.5\n",
    "    \n",
    "def get_argmax_concept_explanations(preds, class_theta_scores):\n",
    "    inds = np.argmax(preds, axis=-1)\n",
    "    return np.take_along_axis(\n",
    "        class_theta_scores,\n",
    "        np.expand_dims(np.expand_dims(inds, axis=-1), axis=-1),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "def senn_experiment_loop(\n",
    "    experiment_config,\n",
    "    datasets,\n",
    "    load_from_cache=False,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    experiment_variables = dict(\n",
    "        task_accuracies=[],\n",
    "        task_aucs=[],\n",
    "        purity_scores=[],\n",
    "        non_oracle_purity_scores=[],\n",
    "        purity_matrices=[],\n",
    "        oracle_matrices=[],\n",
    "    )\n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(datasets):\n",
    "                print(\"Found\", len(loaded_means), \"means for variable\", var_name, \"vs\", len(datasets), \"datasets\")\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)}.'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    utils.serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "    \n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    for (ds_name, (x_train, y_train, c_train), (x_test, y_test, c_test)) in datasets[start_ind:]:\n",
    "        print(\"Training with dataset:\", ds_name)\n",
    "        task_accs = []\n",
    "        aucs = []\n",
    "        purity_mats = []\n",
    "        oracle_mats = []\n",
    "        purities = []\n",
    "        non_oracle_purities = []\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']} of dataset {ds_name}\")\n",
    "            \n",
    "            # Proceed to do and end-to-end model in case we want to\n",
    "            # do some task-specific pretraining\n",
    "            concept_encoder, vae_encoder = construct_senn_encoder(\n",
    "                input_shape=experiment_config[\"input_shape\"],\n",
    "                latent_dims=experiment_config[\"num_concepts\"],\n",
    "                filter_groups=experiment_config[\"filter_groups\"],\n",
    "                units=experiment_config[\"encoder_units\"],\n",
    "                drop_prob=experiment_config['drop_prob'],\n",
    "                max_pool_window=experiment_config['max_pool_window'],\n",
    "                max_pool_stride=experiment_config['max_pool_stride'],\n",
    "                include_pool=experiment_config['include_pool'],\n",
    "            )\n",
    "            concept_decoder = construct_vae_decoder(\n",
    "                output_shape=experiment_config[\"input_shape\"],\n",
    "                latent_dims=experiment_config[\"num_concepts\"],\n",
    "                units=experiment_config[\"decoder_units\"],\n",
    "            )\n",
    "            coefficient_model = construct_senn_coefficient_model(\n",
    "                units=experiment_config[\"coefficient_model_units\"],\n",
    "                num_concepts=experiment_config[\"num_concepts\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "            )\n",
    "            \n",
    "            if experiment_config.get(\"pretrain_autoencoder_epochs\"):\n",
    "                autoencoder = beta_vae.BetaVAE(\n",
    "                    encoder=vae_encoder,\n",
    "                    decoder=concept_decoder,\n",
    "                    loss_fn=vae_losses.bernoulli_fn_wrapper(),\n",
    "                    beta=experiment_config.get(\"beta\", 1),\n",
    "                )\n",
    "                \n",
    "                autoencoder.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(\n",
    "                        experiment_config.get(\"learning_rate\", 1e-3)\n",
    "                    ),\n",
    "                )\n",
    "                \n",
    "                print(\"\\tAutoencoder pre-training...\")\n",
    "                autoencoder.fit(\n",
    "                    x=x_train,\n",
    "                    epochs=experiment_config[\"pretrain_autoencoder_epochs\"],\n",
    "                    batch_size=experiment_config[\"batch_size\"],\n",
    "                    validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                    verbose=verbosity,\n",
    "                )\n",
    "                print(\"\\t\\tAutoencoder training completed\")\n",
    "\n",
    "            # Now time to actually construct and train the CBM\n",
    "            senn_model = construct_senn_model(\n",
    "                concept_encoder=concept_encoder,\n",
    "                concept_decoder=concept_decoder,\n",
    "                coefficient_model=coefficient_model,\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "                regularization_strength=experiment_config.get(\"regularization_strength\", 0.1),\n",
    "                learning_rate=experiment_config.get(\"learning_rate\", 1e-3),\n",
    "                sparsity_strength=experiment_config.get(\"sparsity_strength\", 2e-5),\n",
    "            )\n",
    "\n",
    "            early_stopping_monitor = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=experiment_config.get(\n",
    "                    \"early_stop_metric\",\n",
    "                    \"val_total_loss\",\n",
    "                ),\n",
    "                min_delta=experiment_config[\"min_delta\"],\n",
    "                patience=experiment_config[\"patience\"],\n",
    "                restore_best_weights=True,\n",
    "                verbose=2,\n",
    "                mode=experiment_config.get(\n",
    "                    \"early_stop_mode\",\n",
    "                    \"max\",\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            print(\"\\tSENN training...\")\n",
    "            senn_model.fit(\n",
    "                x=x_train,\n",
    "                y=y_train,\n",
    "                epochs=experiment_config[\"max_epochs\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                callbacks=[\n",
    "                    early_stopping_monitor,\n",
    "                ],\n",
    "                validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                verbose=verbosity,\n",
    "            )\n",
    "            print(\"\\t\\tSENN training completed\")\n",
    "            print(\"\\tSerializing model\")\n",
    "            concept_encoder.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/concept_encoder_{ds_name}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            concept_decoder.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/concept_decoder_{ds_name}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            coefficient_model.save(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/coefficient_model_{ds_name}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            print(\"\\tEvaluating model\")\n",
    "            test_result = senn_model.evaluate(\n",
    "                x_test,\n",
    "                y_test,\n",
    "                verbose=0,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            task_accs.append(\n",
    "                test_result['sparse_top_k_categorical_accuracy']\n",
    "                if experiment_config['num_outputs'] > 1 else\n",
    "                test_result['binary_accuracy']\n",
    "            )\n",
    "            \n",
    "            if experiment_config['num_outputs'] > 1:\n",
    "                # Then lets apply a softmax activation over all the probability\n",
    "                # classes\n",
    "                preds = scipy.special.softmax(\n",
    "                    senn_model.predict(x_test)[0],\n",
    "                    axis=-1\n",
    "                )\n",
    "\n",
    "                # And select just the labels that are in fact being used\n",
    "                one_hot_labels = tf.keras.utils.to_categorical(y_test)\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    one_hot_labels,\n",
    "                    preds,\n",
    "                    multi_class='ovo',\n",
    "                ))\n",
    "            else:\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    y_test,\n",
    "                    senn_model.predict(x_test)[0],\n",
    "                ))\n",
    "            \n",
    "            print(\n",
    "                f\"\\t\\tTest auc = {aucs[-1]:.4f}, \"\n",
    "                f\"task accuracy = {task_accs[-1]:.4f}\"\n",
    "            )\n",
    "\n",
    "            print(f\"\\t\\tComputing purity score...\")\n",
    "            x_test_preds, (_, x_test_theta_class_scores) = senn_model(x_test)\n",
    "            test_concept_scores = get_argmax_concept_explanations(\n",
    "                x_test_preds.numpy(),\n",
    "                x_test_theta_class_scores.numpy(),\n",
    "            )\n",
    "            purity_score, purity_mat, oracle_mat = oracle.oracle_impurity_score(\n",
    "                c_soft=test_concept_scores,\n",
    "                c_true=c_test,\n",
    "                output_matrices=True,\n",
    "            )\n",
    "            purity_mats.append(purity_mat)\n",
    "            oracle_mats.append(oracle_mat)\n",
    "            purities.append(purity_score)\n",
    "            print(f\"\\t\\t\\tDone {purity_score:.4f}\")\n",
    "\n",
    "            print(\"\\t\\tComputing non-oracle purity score...\")\n",
    "            non_oracle_purities.append(oracle.oracle_impurity_score(\n",
    "                c_soft=test_concept_scores,\n",
    "                c_true=c_test,\n",
    "                oracle_matrix=construct_trivial_auc_mat(\n",
    "                    experiment_config[\"num_concepts\"]\n",
    "                ),\n",
    "                purity_matrix=purity_mat,\n",
    "            ))\n",
    "            print(f\"\\t\\t\\tDone {non_oracle_purities[-1]:.4f}\")\n",
    "            print(\"\\t\\tDone with trial\", trial + 1)\n",
    "\n",
    "        task_acc_mean, task_acc_std = np.mean(task_accs), np.std(task_accs)\n",
    "        experiment_variables[\"task_accuracies\"].append((task_acc_mean, task_acc_std))\n",
    "        print(f\"\\tTest task accuracy: {task_acc_mean:.4f}  {task_acc_std:.4f}\")\n",
    "\n",
    "\n",
    "        task_auc_mean, task_auc_std = np.mean(aucs), np.std(aucs)\n",
    "        experiment_variables[\"task_aucs\"].append((task_auc_mean, task_auc_std))\n",
    "        print(f\"\\tTest task AUC: {task_auc_mean:.4f}  {task_auc_std:.4f}\")\n",
    "\n",
    "        purity_mats = np.stack(purity_mats, axis=0)\n",
    "        purity_mat_mean = np.mean(purity_mats, axis=0)\n",
    "        purity_mat_std = np.std(purity_mats, axis=0)\n",
    "        print(\"\\tPurity matrix:\")\n",
    "        for i in range(purity_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(purity_mat_mean.shape[1]):\n",
    "                line += f'{purity_mat_mean[i, j]:.4f}  {purity_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"purity_matrices\"].append((purity_mat_mean, purity_mat_std))\n",
    "\n",
    "\n",
    "        oracle_mats = np.stack(oracle_mats, axis=0)\n",
    "        oracle_mat_mean = np.mean(oracle_mats, axis=0)\n",
    "        oracle_mat_std = np.std(oracle_mats, axis=0)\n",
    "        print(\"\\tOracle matrix:\")\n",
    "        for i in range(oracle_mat_mean.shape[0]):\n",
    "            line = \"\\t\\t\"\n",
    "            for j in range(oracle_mat_mean.shape[1]):\n",
    "                line += f'{oracle_mat_mean[i, j]:.4f}  {oracle_mat_std[i, j]:.4f}    '\n",
    "            print(line)\n",
    "\n",
    "        experiment_variables[\"oracle_matrices\"].append((oracle_mat_mean, oracle_mat_std))\n",
    "\n",
    "        purity_mean, purity_std = np.mean(purities), np.std(purities)\n",
    "        experiment_variables[\"purity_scores\"].append((purity_mean, purity_std))\n",
    "        print(f\"\\tPurity score: {purity_mean:.4f}  {purity_std:.4f}\")\n",
    "\n",
    "        non_oracle_purity_mean, non_oracle_purity_std = np.mean(non_oracle_purities), np.std(non_oracle_purities)\n",
    "        experiment_variables[\"non_oracle_purity_scores\"].append((non_oracle_purity_mean, non_oracle_purity_std))\n",
    "        print(f\"\\tNon-oracle purity score: {non_oracle_purity_mean:.4f}  {non_oracle_purity_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables\n",
    "\n",
    "def senn_bottleneck_predict_experiment_loop(\n",
    "    experiment_config,\n",
    "    datasets,\n",
    "    load_from_cache=False,\n",
    "):\n",
    "    utils.reseed(87)\n",
    "    experiment_variables = dict(\n",
    "        latent_predictive_accuracies=[],\n",
    "        latent_predictive_aucs=[],\n",
    "    )\n",
    "\n",
    "    start_ind = 0\n",
    "    if load_from_cache:\n",
    "        cached = True\n",
    "        complete_cash = True\n",
    "        for var_name in experiment_variables:\n",
    "            file_name = os.path.join(\n",
    "                experiment_config[\"results_dir\"],\n",
    "                var_name\n",
    "            )\n",
    "            if (\n",
    "                (not os.path.exists(f'{file_name}_means.npz')) or\n",
    "                (not os.path.exists(f'{file_name}_stds.npz'))\n",
    "            ):\n",
    "                print(\"Could not find\", f'\"{file_name}_means.npz\" or \"{file_name}_stds.npz\" in cache.')\n",
    "                cached = False\n",
    "                break\n",
    "            loaded_means = np.load(f'{file_name}_means.npz')\n",
    "            if len(loaded_means) != len(datasets):\n",
    "                print(\"Found\", len(loaded_means), \"means for variable\", var_name, \"vs\", len(datasets), \"datasets\")\n",
    "                # Then we have a partial run here so let's just run the rest\n",
    "                if start_ind and start_ind != len(loaded_means):\n",
    "                    raise ValueError(\n",
    "                        f'Found inconsistent start indices in cached '\n",
    "                        f'data {start_ind} vs {len(loaded_means)} ({file_name}).'\n",
    "                    )\n",
    "                start_ind = len(loaded_means)\n",
    "                complete_cash = False\n",
    "        if cached:\n",
    "            # Then we have found all of the arrays of interest, so let's\n",
    "            # load them and use them\n",
    "            print(\"Experiment cache was hit\")\n",
    "            print(\"\\tLoading variables from\", experiment_config[\"results_dir\"])\n",
    "            for var_name in experiment_variables:\n",
    "                file_name = os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    var_name\n",
    "                )\n",
    "                experiment_variables[var_name] = list(zip(\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_means.npz')[x],\n",
    "                        np.load(f'{file_name}_means.npz')\n",
    "                    )),\n",
    "                    list(map(\n",
    "                        lambda x: np.load(f'{file_name}_stds.npz')[x],\n",
    "                        np.load(f'{file_name}_stds.npz')\n",
    "                    )),\n",
    "                ))\n",
    "            print(experiment_variables)\n",
    "            if complete_cash:\n",
    "                # Then we are good to go\n",
    "                if not os.path.exists(\n",
    "                    os.path.join(experiment_config[\"results_dir\"], \"config.yaml\")\n",
    "                ):\n",
    "                    # then serialize the config as this is a different version run\n",
    "                    utils.serialize_experiment_config(\n",
    "                        experiment_config,\n",
    "                        experiment_config[\"results_dir\"],\n",
    "                    )\n",
    "                return experiment_variables\n",
    "    \n",
    "    # Else, let's go ahead and run the whole thing\n",
    "    verbosity = experiment_config.get(\"verbosity\", 0)\n",
    "    Path(\n",
    "        os.path.join(\n",
    "            experiment_config[\"results_dir\"],\n",
    "            \"models\",\n",
    "        )\n",
    "    ).mkdir(parents=True, exist_ok=True)\n",
    "    for (ds_name, (x_train, y_train, c_train), (x_test, y_test, c_test)) in datasets[start_ind:]:\n",
    "        print(\"Training with dataset\", ds_name)\n",
    "        task_accs = []\n",
    "        aucs = []\n",
    "\n",
    "        for trial in range(experiment_config[\"trials\"]):\n",
    "            print(f\"\\tTrial {trial + 1}/{experiment_config['trials']}\")\n",
    "            concept_encoder_model = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/concept_encoder_{ds_name}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            concept_decoder_model = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/concept_decoder_{ds_name}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            coefficient_model = tf.keras.models.load_model(\n",
    "                os.path.join(\n",
    "                    experiment_config[\"results_dir\"],\n",
    "                    f\"models/coefficient_model_{ds_name}_trial_{trial}\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            \n",
    "            predictive_decoder = construct_decoder(\n",
    "                units=experiment_config[\"latent_decoder_units\"],\n",
    "                num_outputs=experiment_config[\"num_outputs\"],\n",
    "            )\n",
    "            predictive_decoder.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(experiment_config[\"learning_rate\"]),\n",
    "                loss=(\n",
    "                    tf.keras.losses.BinaryCrossentropy(from_logits=True) if (experiment_config[\"num_outputs\"] <= 2)\n",
    "                    else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "                ),\n",
    "                metrics=[\n",
    "                    \"binary_accuracy\" if (experiment_config[\"num_outputs\"] <= 2)\n",
    "                    else tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            print(\"\\tTraining model\")\n",
    "            train_codes = encoder(x_train)\n",
    "            if isinstance(train_codes, list):\n",
    "                train_codes = np.concatenate(list(map(lambda x: x.numpy(), train_codes)), axis=-1)\n",
    "            else:\n",
    "                train_codes = train_codes.numpy()\n",
    "            test_codes = encoder(x_test)\n",
    "            if isinstance(test_codes, list):\n",
    "                test_codes = np.concatenate(list(map(lambda x: x.numpy(), test_codes)), axis=-1)\n",
    "            else:\n",
    "                test_codes = test_codes.numpy()\n",
    "            predictive_decoder.fit(\n",
    "                x=train_codes,\n",
    "                y=y_train,\n",
    "                epochs=experiment_config[\"predictor_max_epochs\"],\n",
    "                batch_size=experiment_config[\"batch_size\"],\n",
    "                validation_split=experiment_config[\"holdout_fraction\"],\n",
    "                verbose=verbosity,\n",
    "            )\n",
    "            print(\"\\tEvaluating model\")\n",
    "            test_result = predictive_decoder.evaluate(\n",
    "                test_codes,\n",
    "                y_test,\n",
    "                verbose=0,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            task_accs.append(\n",
    "                test_result['sparse_top_k_categorical_accuracy']\n",
    "                if experiment_config['num_outputs'] > 1 else\n",
    "                test_result['binary_accuracy']\n",
    "            )\n",
    "\n",
    "            if experiment_config['num_outputs'] > 1:\n",
    "                # Then lets apply a softmax activation over all the probability\n",
    "                # classes\n",
    "                preds = scipy.special.softmax(\n",
    "                    predictive_decoder.predict(test_codes),\n",
    "                    axis=-1,\n",
    "                )\n",
    "\n",
    "                # And select just the labels that are in fact being used\n",
    "                print(np.sum(preds[:100, :], axis=-1))\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    y_test,\n",
    "                    preds,\n",
    "                    multi_class='ovo',\n",
    "                ))\n",
    "            else:\n",
    "                aucs.append(sklearn.metrics.roc_auc_score(\n",
    "                    y_test,\n",
    "                    predictive_decoder.predict(test_codes),\n",
    "                ))\n",
    "            \n",
    "            print(\n",
    "                f\"\\t\\tTest auc = {aucs[-1]:.4f}, \"\n",
    "                f\"task accuracy = {task_accs[-1]:.4f}\"\n",
    "            )\n",
    "            print(\"\\t\\tDone with trial\", trial + 1)\n",
    "\n",
    "        task_acc_mean, task_acc_std = np.mean(task_accs), np.std(task_accs)\n",
    "        experiment_variables[\"latent_predictive_accuracies\"].append((task_acc_mean, task_acc_std))\n",
    "        print(f\"\\tTest task accuracy: {task_acc_mean:.4f}  {task_acc_std:.4f}\")\n",
    "\n",
    "        task_auc_mean, task_auc_std = np.mean(aucs), np.std(aucs)\n",
    "        experiment_variables[\"latent_predictive_aucs\"].append((task_auc_mean, task_auc_std))\n",
    "        print(f\"\\tTest task AUC: {task_auc_mean:.4f}  {task_auc_std:.4f}\")\n",
    "\n",
    "        # And serialize the results\n",
    "        utils.serialize_results(\n",
    "            results_dict=experiment_variables,\n",
    "            results_dir=experiment_config[\"results_dir\"],\n",
    "        )\n",
    "    return experiment_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced Multiclass Task Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(SENN)\n",
    "# If convolution is used in encoder, then this may be needed :/\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment config\n",
    "############################################################################\n",
    "\n",
    "senn_dependency_multiclass_experiment_config = dict(\n",
    "    max_epochs=30, #100,\n",
    "    predictor_max_epochs=100,\n",
    "    batch_size=32,\n",
    "    trials=5,\n",
    "    learning_rate=1e-3,\n",
    "    \n",
    "    num_concepts=(balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1]),\n",
    "    input_shape=balanced_multiclass_task_bin_concepts_dep_0_complete_train[0].shape[1:],\n",
    "    num_outputs=(\n",
    "        len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) if len(set(balanced_multiclass_task_bin_concepts_dep_0_complete_train[1])) > 2\n",
    "        else 1\n",
    "    ),\n",
    "    \n",
    "    filter_groups=[\n",
    "       [(8, (7, 7), 1)],\n",
    "       [(16, (5, 5), 1)],\n",
    "       [(32, (3, 3), 1)],\n",
    "       [(64, (3, 3), 1)],\n",
    "    ],\n",
    "    encoder_units=[64, 64],\n",
    "#     encoder_units=[256, 128, 64, 64, 32],\n",
    "    include_pool=True,\n",
    "    decoder_units=[256, 512],\n",
    "    coefficient_model_units=[64, 64],\n",
    "    latent_decoder_units=[64, 64],\n",
    "    drop_prob=0,\n",
    "    max_pool_window=(2,2),\n",
    "    max_pool_stride=2,\n",
    "    \n",
    "    regularization_strength=0.1,\n",
    "    sparsity_strength=2e-5,\n",
    "    \n",
    "    concept_cardinality=[2 for _ in range(balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1])],\n",
    "    results_dir=os.path.join(RESULTS_DIR, \"senn/dependency_multiclass\"),\n",
    "    verbosity=1,\n",
    "\n",
    "    holdout_fraction=0.1,\n",
    "    patience=float(\"inf\"),\n",
    "    early_stop_metric=\"val_loss\",\n",
    "    early_stop_mode=\"min\",\n",
    "    min_delta=1e-5,\n",
    ")\n",
    "\n",
    "# Generate the experiment directory if it does not exist already\n",
    "Path(senn_dependency_multiclass_experiment_config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "senn_dependency_multiclass_figure_dir = os.path.join(senn_dependency_multiclass_experiment_config[\"results_dir\"], \"figures\")\n",
    "Path(senn_dependency_multiclass_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################################################\n",
    "## Experiment run\n",
    "############################################################################\n",
    "\n",
    "senn_dependency_multiclass_results = senn_experiment_loop(\n",
    "    senn_dependency_multiclass_experiment_config,\n",
    "    load_from_cache=False,\n",
    "    datasets=[\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_0_complete\", balanced_multiclass_task_bin_concepts_dep_0_complete_train, balanced_multiclass_task_bin_concepts_dep_0_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_1_complete\", balanced_multiclass_task_bin_concepts_dep_1_complete_train, balanced_multiclass_task_bin_concepts_dep_1_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_2_complete\", balanced_multiclass_task_bin_concepts_dep_2_complete_train, balanced_multiclass_task_bin_concepts_dep_2_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_3_complete\", balanced_multiclass_task_bin_concepts_dep_3_complete_train, balanced_multiclass_task_bin_concepts_dep_3_complete_test),\n",
    "        (\"balanced_multiclass_task_bin_concepts_dep_4_complete\", balanced_multiclass_task_bin_concepts_dep_4_complete_train, balanced_multiclass_task_bin_concepts_dep_4_complete_test),\n",
    "    ],\n",
    ")\n",
    "print(\"task_accuracies:\", senn_dependency_multiclass_results[\"task_accuracies\"])\n",
    "print(\"task_aucs:\", senn_dependency_multiclass_results[\"task_aucs\"])\n",
    "print(\"purity_scores:\", senn_dependency_multiclass_results[\"purity_scores\"])\n",
    "balanced_oracle_matrix_cache = {\n",
    "    \"balanced_multiclass_task_bin_concepts_dep_0\": senn_dependency_multiclass_results[\"oracle_matrices\"][0][0],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset-wide Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars = [0, 4]\n",
    "num_concepts = 5\n",
    "all_models = [\n",
    "    (\"Joint-CBM\", graph_dependency_balanced_multiclass_results, \"purity_scores\"),\n",
    "    (\"CW MaxPool-Mean\", cw_binary_balanced_multiclass_results, \"purity_scores\"),\n",
    "    (\"CW Feature Map\", cw_binary_balanced_multiclass_results, \"repr_purity_scores\"),\n",
    "    (f\"Ada-MLVAE (n\\_latent = {ada_mlvae_balanced_multilabel_experiment_config['latent_dim']})\", ada_mlvae_balanced_multilabel_results, \"purity_scores\"),\n",
    "    (f\"Ada-MLVAE (n\\_latent = {ada_mlvae_balanced_multilabel_extended_experiment_config['latent_dim']})\", ada_mlvae_balanced_multilabel_extended_results, \"purity_scores\"),\n",
    "    (f\"Ada-GVAE (n\\_latent = {ada_gvae_balanced_multilabel_experiment_config['latent_dim']})\", ada_gvae_balanced_multilabel_results, \"purity_scores\"),\n",
    "    (f\"Ada-GVAE (n\\_latent = {ada_gvae_balanced_multilabel_extended_experiment_config['latent_dim']})\", ada_gvae_balanced_multilabel_extended_results, \"purity_scores\"),\n",
    "    (f\"CCD (n\\_concepts = {ccd_balanced_multiclass_experiment_config['num_concepts']})\", ccd_balanced_multiclass_results, \"purity_scores\"),\n",
    "    (f\"CCD (n\\_concepts = {ccd_balanced_multiclass_extended_experiment_config['num_concepts']})\", ccd_balanced_multiclass_extended_results, \"purity_scores\"),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models * 1.25, 7))\n",
    "for i, (method_name, results, kword) in enumerate(all_models):\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        list(map(\n",
    "            lambda x: normalize_purity(x[0], num_concepts),\n",
    "            np.array(results[kword])[all_vars]\n",
    "        )),\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=list(map(\n",
    "            lambda x: 2*normalize_purity(x[1], num_concepts),\n",
    "            np.array(results[kword])[all_vars]\n",
    "        )),\n",
    "        capsize=10,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "lgd = ax.legend(prop={\"size\":20}, loc='upper center', bbox_to_anchor=(0.5,-0.15), ncol=(num_models - 1)//3)\n",
    "\n",
    "plt.ylabel(\"Oracle Impurity Score\", fontsize=20)\n",
    "if len(all_vars) > 1:\n",
    "    plt.xlabel(\"Number of Dependency Edges ($\\lambda$)\", fontsize=20)\n",
    "    plt.title(bold_text(\"Oracle Impurity (dSprites($\\lambda$))\"), fontsize=30)\n",
    "    plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "    ax.set_xticklabels(all_vars, fontsize=15)\n",
    "else:\n",
    "    plt.title(bold_text(\"Oracle Impurity (dSprites($\\lambda = \" + str(all_vars[0]) + \"$))\"), fontsize=30)\n",
    "    plt.xticks([], fontsize=15)\n",
    "# ax.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "ax.set_xticklabels(all_vars, fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "ax.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot concept accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = np.arange(0, balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1])\n",
    "num_concepts = 5\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        graph_dependency_balanced_multiclass_results,\n",
    "        \"concept_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_binary_balanced_multiclass_results,\n",
    "        \"concept_aucs\",\n",
    "        lambda x: 100 * np.mean(x),\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_binary_balanced_multiclass_results,\n",
    "        \"concept_aucs\",\n",
    "        lambda x: 100 * np.mean(x),\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-MLVAE (n\\_latent = {ada_mlvae_balanced_multilabel_experiment_config['latent_dim']})\",\n",
    "        ada_mlvae_balanced_multilabel_results,\n",
    "        \"aligned_purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-MLVAE (n\\_latent = {ada_mlvae_balanced_multilabel_extended_experiment_config['latent_dim']})\",\n",
    "        ada_mlvae_balanced_multilabel_extended_results,\n",
    "        \"aligned_purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-GVAE (n\\_latent = {ada_gvae_balanced_multilabel_experiment_config['latent_dim']})\",\n",
    "        ada_gvae_balanced_multilabel_results,\n",
    "        \"aligned_purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-GVAE (n\\_latent = {ada_gvae_balanced_multilabel_extended_experiment_config['latent_dim']})\",\n",
    "        ada_gvae_balanced_multilabel_extended_results,\n",
    "        \"aligned_purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_balanced_multiclass_experiment_config['num_concepts']})\",\n",
    "        ccd_balanced_multiclass_results,\n",
    "        \"purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_balanced_multiclass_extended_experiment_config['num_concepts']})\",\n",
    "        ccd_balanced_multiclass_extended_results,\n",
    "        \"purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    for i, (method_name, results, kword, transform_fn) in enumerate(all_models):\n",
    "        accs_means = list(map(\n",
    "            lambda x: x[0],\n",
    "            np.array(results[kword])[all_vars],\n",
    "        ))\n",
    "        accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "        accs_stds = list(map(\n",
    "            lambda x: x[1],\n",
    "            np.array(results[kword])[all_vars],\n",
    "        ))\n",
    "        accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "        ax.plot(\n",
    "            np.arange(0, len(all_vars)),\n",
    "            accs_means,\n",
    "            c=clrs[i*2],\n",
    "            zorder=1,\n",
    "        )\n",
    "        plt.scatter(\n",
    "            np.arange(0, len(all_vars)),\n",
    "            accs_means,\n",
    "            label=method_name,\n",
    "            s=150,\n",
    "            color=clrs[i*2],\n",
    "            zorder=2,\n",
    "        )\n",
    "        ax.fill_between(\n",
    "            np.arange(0, len(all_vars)),\n",
    "            accs_means - accs_stds,\n",
    "            accs_means + accs_stds,\n",
    "            alpha=0.3,\n",
    "            facecolor=clrs[i*2],\n",
    "        )\n",
    "    \n",
    "lgd = ax.legend(prop={\"size\":20}, loc='upper center', bbox_to_anchor=(0.5,-0.15), ncol=(num_models - 1)//3)\n",
    "\n",
    "\n",
    "plt.ylabel(\"AUC (\\%)\", fontsize=20)\n",
    "if len(all_vars) > 1:\n",
    "    plt.xlabel(\"Number of Dependency Edges ($\\lambda$)\", fontsize=20)\n",
    "    plt.title(bold_text(\"Mean Concept AUC (dSprites($\\lambda$))\"), fontsize=30)\n",
    "    plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "    ax.set_xticklabels(all_vars, fontsize=15)\n",
    "else:\n",
    "    plt.title(bold_text(\"Mean Concept AUC (dSprites($\\lambda = \" + str(all_vars[0]) + \"$))\"), fontsize=30)\n",
    "    plt.xticks([], fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot concept accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = np.arange(0, balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1])\n",
    "num_concepts = 5\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        graph_dependency_balanced_multiclass_results,\n",
    "        \"concept_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_binary_balanced_multiclass_results,\n",
    "        \"concept_aucs\",\n",
    "        lambda x: 100 * np.mean(x),\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_binary_balanced_multiclass_results,\n",
    "        \"repr_purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-MLVAE (n\\_latent = {ada_mlvae_balanced_multilabel_experiment_config['latent_dim']})\",\n",
    "        ada_mlvae_balanced_multilabel_results,\n",
    "        \"aligned_purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-MLVAE (n\\_latent = {ada_mlvae_balanced_multilabel_extended_experiment_config['latent_dim']})\",\n",
    "        ada_mlvae_balanced_multilabel_extended_results,\n",
    "        \"aligned_purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-GVAE (n\\_latent = {ada_gvae_balanced_multilabel_experiment_config['latent_dim']})\",\n",
    "        ada_gvae_balanced_multilabel_results,\n",
    "        \"aligned_purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-GVAE (n\\_latent = {ada_gvae_balanced_multilabel_extended_experiment_config['latent_dim']})\",\n",
    "        ada_gvae_balanced_multilabel_extended_results,\n",
    "        \"aligned_purity_matrices\",\n",
    "        lambda x:100 *  np.mean(np.diagonal(x)),\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_balanced_multiclass_experiment_config['num_concepts']})\",\n",
    "        ccd_balanced_multiclass_results,\n",
    "        \"aligned_purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_balanced_multiclass_extended_experiment_config['num_concepts']})\",\n",
    "        ccd_balanced_multiclass_extended_results,\n",
    "        \"aligned_purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models * 1.75, 5))\n",
    "for i, (method_name, results, kword, transform_fn) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=5,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=20, loc='upper center', bbox_to_anchor=(0.5,-0.15), ncol=(num_models - 1)//3)\n",
    "\n",
    "plt.ylabel(\"AUC (\\%)\", fontsize=20)\n",
    "if len(all_vars) > 1:\n",
    "    plt.xlabel(\"Number of Dependency Edges ($\\lambda$)\", fontsize=20)\n",
    "    plt.title(bold_text(\"Mean Concept AUC (dSprites($\\lambda$))\"), fontsize=30)\n",
    "    plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "    ax.set_xticklabels(all_vars, fontsize=15)\n",
    "else:\n",
    "    plt.title(bold_text(\"Mean Concept AUC (dSprites($\\lambda = \" + str(all_vars[0]) + \"$))\"), fontsize=30)\n",
    "    plt.xticks([], fontsize=15)\n",
    "plt.yticks(fontsize=20)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot concept accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = np.array([0, 4])\n",
    "num_concepts = 5\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        graph_dependency_balanced_multiclass_results,\n",
    "        \"concept_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_binary_balanced_multiclass_results,\n",
    "        \"concept_aucs\",\n",
    "        lambda x: 100 * np.mean(x),\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_binary_balanced_multiclass_results,\n",
    "        \"repr_purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-MLVAE (n\\_latent = {ada_mlvae_balanced_multilabel_experiment_config['latent_dim']})\",\n",
    "        ada_mlvae_balanced_multilabel_results,\n",
    "        \"aligned_purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-MLVAE (n\\_latent = {ada_mlvae_balanced_multilabel_extended_experiment_config['latent_dim']})\",\n",
    "        ada_mlvae_balanced_multilabel_extended_results,\n",
    "        \"aligned_purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-GVAE (n\\_latent = {ada_gvae_balanced_multilabel_experiment_config['latent_dim']})\",\n",
    "        ada_gvae_balanced_multilabel_results,\n",
    "        \"aligned_purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-GVAE (n\\_latent = {ada_gvae_balanced_multilabel_extended_experiment_config['latent_dim']})\",\n",
    "        ada_gvae_balanced_multilabel_extended_results,\n",
    "        \"aligned_purity_matrices\",\n",
    "        lambda x:100 *  np.mean(np.diagonal(x)),\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_balanced_multiclass_experiment_config['num_concepts']})\",\n",
    "        ccd_balanced_multiclass_results,\n",
    "        \"aligned_purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_balanced_multiclass_extended_experiment_config['num_concepts']})\",\n",
    "        ccd_balanced_multiclass_extended_results,\n",
    "        \"aligned_purity_matrices\",\n",
    "        lambda x: 100 * np.mean(np.diagonal(x)),\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models * 1, 5))\n",
    "for i, (method_name, results, kword, transform_fn) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=5,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=15, loc='upper center', bbox_to_anchor=(0.5,-0.15), ncol=(num_models - 1)//3)\n",
    "\n",
    "plt.ylabel(\"AUC (\\%)\", fontsize=20)\n",
    "if len(all_vars) > 1:\n",
    "    plt.xlabel(\"Number of Dependency Edges ($\\lambda$)\", fontsize=20)\n",
    "    plt.title(bold_text(\"Mean Concept AUC (dSprites($\\lambda$))\"), fontsize=30)\n",
    "    plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "    ax.set_xticklabels(all_vars, fontsize=15)\n",
    "else:\n",
    "    plt.title(bold_text(\"Mean Concept AUC (dSprites($\\lambda = \" + str(all_vars[0]) + \"$))\"), fontsize=30)\n",
    "    plt.xticks([], fontsize=15)\n",
    "plt.yticks(fontsize=20)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot task accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = np.arange(0, balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1])\n",
    "num_concepts = 5\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        graph_dependency_balanced_multiclass_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_binary_balanced_multiclass_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_binary_balanced_multiclass_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_balanced_multiclass_experiment_config['num_concepts']})\",\n",
    "        ccd_balanced_multiclass_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_balanced_multiclass_extended_experiment_config['num_concepts']})\",\n",
    "        ccd_balanced_multiclass_extended_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models * 2, 5))\n",
    "for i, (method_name, results, kword, transform_fn) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=10,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=20, loc='upper center', bbox_to_anchor=(0.5,-0.14), ncol=(num_models - 1)//2)\n",
    "\n",
    "plt.ylabel(\"AUC (\\%)\", fontsize=20)\n",
    "if len(all_vars) > 1:\n",
    "    plt.xlabel(\"Number of Dependency Edges ($\\lambda$)\", fontsize=20)\n",
    "    plt.title(bold_text(\"Downstream Task AUC (dSprites($\\lambda$))\"), fontsize=30)\n",
    "    plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "    ax.set_xticklabels(all_vars, fontsize=15)\n",
    "else:\n",
    "    plt.title(bold_text(\"Downstream Task AUC (dSprites($\\lambda = \" + str(all_vars[0]) + \"$))\"), fontsize=30)\n",
    "    plt.xticks([], fontsize=15)\n",
    "plt.yticks(fontsize=20)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot task accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = [0, 4]\n",
    "num_concepts = 5\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        graph_dependency_balanced_multiclass_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_binary_balanced_multiclass_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "#     (\n",
    "#         \"CW Mean\",\n",
    "#         cw_mean_binary_balanced_multiclass_results,\n",
    "#         \"task_aucs\",\n",
    "#         lambda x: 100 * x,\n",
    "#     ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_binary_balanced_multiclass_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_balanced_multiclass_experiment_config['num_concepts']})\",\n",
    "        ccd_balanced_multiclass_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_balanced_multiclass_extended_experiment_config['num_concepts']})\",\n",
    "        ccd_balanced_multiclass_extended_results,\n",
    "        \"task_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models * 1.5, 5))\n",
    "for i, (method_name, results, kword, transform_fn) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=10,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=20, loc='upper center', bbox_to_anchor=(0.5,-0.14), ncol=(num_models - 1)//2)\n",
    "\n",
    "plt.ylabel(\"AUC (\\%)\", fontsize=20)\n",
    "if len(all_vars) > 1:\n",
    "    plt.xlabel(\"Number of Dependency Edges ($\\lambda$)\", fontsize=20)\n",
    "    plt.title(bold_text(\"Downstream Task AUC (dSprites($\\lambda$))\"), fontsize=30)\n",
    "    plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "    ax.set_xticklabels(all_vars, fontsize=15)\n",
    "else:\n",
    "    plt.title(bold_text(\"Downstream Task AUC (dSprites($\\lambda = \" + str(all_vars[0]) + \"$))\"), fontsize=30)\n",
    "    plt.xticks([], fontsize=15)\n",
    "plt.yticks(fontsize=20)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot task accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = np.arange(0, balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1])\n",
    "num_concepts = 5\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        graph_dependency_balanced_multiclass_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_binary_balanced_multiclass_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_binary_balanced_multiclass_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_balanced_multiclass_experiment_config['num_concepts']})\",\n",
    "        ccd_balanced_multiclass_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_balanced_multiclass_extended_experiment_config['num_concepts']})\",\n",
    "        ccd_balanced_multiclass_extended_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models * 2, 5))\n",
    "for i, (method_name, results, kword, transform_fn) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=10,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=20, loc='upper center', bbox_to_anchor=(0.5,-0.14), ncol=(num_models - 1)//2)\n",
    "\n",
    "plt.ylabel(\"Accuracy (\\%)\", fontsize=20)\n",
    "if len(all_vars) > 1:\n",
    "    plt.xlabel(\"Number of Dependency Edges ($\\lambda$)\", fontsize=20)\n",
    "    plt.title(bold_text(\"Downstream Task Accuracy (dSprites($\\lambda$))\"), fontsize=30)\n",
    "    plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "    ax.set_xticklabels(all_vars, fontsize=15)\n",
    "else:\n",
    "    plt.title(bold_text(\"Downstream Task Accuracy (dSprites($\\lambda = \" + str(all_vars[0]) + \"$))\"), fontsize=30)\n",
    "    plt.xticks([], fontsize=15)\n",
    "plt.yticks(fontsize=20)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot task accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = [0, 4]\n",
    "num_concepts = 5\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        graph_dependency_balanced_multiclass_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_binary_balanced_multiclass_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_binary_balanced_multiclass_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_balanced_multiclass_experiment_config['num_concepts']})\",\n",
    "        ccd_balanced_multiclass_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_balanced_multiclass_extended_experiment_config['num_concepts']})\",\n",
    "        ccd_balanced_multiclass_extended_results,\n",
    "        \"task_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models * 1.5, 5))\n",
    "for i, (method_name, results, kword, transform_fn) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=10,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=20, loc='upper center', bbox_to_anchor=(0.5,-0.14), ncol=(num_models - 1)//2)\n",
    "\n",
    "plt.ylabel(\"Accuracy (\\%)\", fontsize=20)\n",
    "if len(all_vars) > 1:\n",
    "    plt.xlabel(\"Number of Dependency Edges ($\\lambda$)\", fontsize=20)\n",
    "    plt.title(bold_text(\"Downstream Task Accuracy (dSprites($\\lambda$))\"), fontsize=30)\n",
    "    plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "    ax.set_xticklabels(all_vars, fontsize=15)\n",
    "else:\n",
    "    plt.title(bold_text(\"Downstream Task Accuracy (dSprites($\\lambda = \" + str(all_vars[0]) + \"$))\"), fontsize=30)\n",
    "    plt.xticks([], fontsize=15)\n",
    "plt.yticks(fontsize=20)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot task accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = np.arange(0, balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1])\n",
    "num_concepts = 5\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        graph_dependency_balanced_multiclass_results,\n",
    "        \"latent_predictive_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_binary_balanced_multiclass_results,\n",
    "        \"latent_predictive_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_binary_balanced_multiclass_results,\n",
    "        \"latent_feature_predictive_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-MLVAE (n\\_latent = {ada_mlvae_balanced_multilabel_experiment_config['latent_dim']})\",\n",
    "        ada_mlvae_balanced_multilabel_results,\n",
    "        \"latent_predictive_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-MLVAE (n\\_latent = {ada_mlvae_balanced_multilabel_extended_experiment_config['latent_dim']})\",\n",
    "        ada_mlvae_balanced_multilabel_extended_results,\n",
    "        \"latent_predictive_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-GVAE (n\\_latent = {ada_gvae_balanced_multilabel_experiment_config['latent_dim']})\",\n",
    "        ada_gvae_balanced_multilabel_results,\n",
    "        \"latent_predictive_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-GVAE (n\\_latent = {ada_gvae_balanced_multilabel_extended_experiment_config['latent_dim']})\",\n",
    "        ada_gvae_balanced_multilabel_extended_results,\n",
    "        \"latent_predictive_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_balanced_multiclass_experiment_config['num_concepts']})\",\n",
    "        ccd_balanced_multiclass_results,\n",
    "        \"direct_completeness_scores\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_balanced_multiclass_extended_experiment_config['num_concepts']})\",\n",
    "        ccd_balanced_multiclass_extended_results,\n",
    "        \"direct_completeness_scores\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models * 2, 5))\n",
    "for i, (method_name, results, kword, transform_fn) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=5,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=20, loc='upper center', bbox_to_anchor=(0.5,-0.2), ncol=(num_models - 1)//2)\n",
    "\n",
    "plt.ylabel(\"AUC (\\%)\", fontsize=25)\n",
    "if len(all_vars) > 1:\n",
    "    plt.xlabel(\"Number of Dependency Edges ($\\lambda$)\", fontsize=25)\n",
    "    plt.title(bold_text(\"Downstream Task AUC from Concepts (dSprites($\\lambda$))\"), fontsize=30)\n",
    "    plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "    ax.set_xticklabels(all_vars, fontsize=15)\n",
    "else:\n",
    "    plt.title(bold_text(\"Downstream Task AUC from Concepts(dSprites($\\lambda = \" + str(all_vars[0]) + \"$))\"), fontsize=30)\n",
    "    plt.xticks([], fontsize=15)\n",
    "plt.yticks(fontsize=20)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot task accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = [0, 4]\n",
    "num_concepts = 5\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        graph_dependency_balanced_multiclass_results,\n",
    "        \"latent_predictive_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_binary_balanced_multiclass_results,\n",
    "        \"latent_predictive_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_binary_balanced_multiclass_results,\n",
    "        \"latent_feature_predictive_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-MLVAE (n\\_latent = {ada_mlvae_balanced_multilabel_experiment_config['latent_dim']})\",\n",
    "        ada_mlvae_balanced_multilabel_results,\n",
    "        \"latent_predictive_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-MLVAE (n\\_latent = {ada_mlvae_balanced_multilabel_extended_experiment_config['latent_dim']})\",\n",
    "        ada_mlvae_balanced_multilabel_extended_results,\n",
    "        \"latent_predictive_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-GVAE (n\\_latent = {ada_gvae_balanced_multilabel_experiment_config['latent_dim']})\",\n",
    "        ada_gvae_balanced_multilabel_results,\n",
    "        \"latent_predictive_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-GVAE (n\\_latent = {ada_gvae_balanced_multilabel_extended_experiment_config['latent_dim']})\",\n",
    "        ada_gvae_balanced_multilabel_extended_results,\n",
    "        \"latent_predictive_aucs\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_balanced_multiclass_experiment_config['num_concepts']})\",\n",
    "        ccd_balanced_multiclass_results,\n",
    "        \"direct_completeness_scores\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_balanced_multiclass_extended_experiment_config['num_concepts']})\",\n",
    "        ccd_balanced_multiclass_extended_results,\n",
    "        \"direct_completeness_scores\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models * 2, 5))\n",
    "for i, (method_name, results, kword, transform_fn) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=5,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=20, loc='upper center', bbox_to_anchor=(0.5,-0.2), ncol=(num_models - 1)//2)\n",
    "\n",
    "plt.ylabel(\"AUC (\\%)\", fontsize=25)\n",
    "if len(all_vars) > 1:\n",
    "    plt.xlabel(\"Number of Dependency Edges ($\\lambda$)\", fontsize=25)\n",
    "    plt.title(bold_text(\"Downstream Task AUC from Concepts (dSprites($\\lambda$))\"), fontsize=30)\n",
    "    plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "    ax.set_xticklabels(all_vars, fontsize=15)\n",
    "else:\n",
    "    plt.title(bold_text(\"Downstream Task AUC from Concepts(dSprites($\\lambda = \" + str(all_vars[0]) + \"$))\"), fontsize=30)\n",
    "    plt.xticks([], fontsize=15)\n",
    "plt.yticks(fontsize=20)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot task accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = np.arange(0, balanced_multiclass_task_bin_concepts_dep_0_complete_train[2].shape[-1])\n",
    "num_concepts = 5\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        graph_dependency_balanced_multiclass_results,\n",
    "        \"latent_predictive_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_binary_balanced_multiclass_results,\n",
    "        \"latent_predictive_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_binary_balanced_multiclass_results,\n",
    "        \"latent_feature_predictive_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-MLVAE (n\\_latent = {ada_mlvae_balanced_multilabel_experiment_config['latent_dim']})\",\n",
    "        ada_mlvae_balanced_multilabel_results,\n",
    "        \"latent_predictive_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-MLVAE (n\\_latent = {ada_mlvae_balanced_multilabel_extended_experiment_config['latent_dim']})\",\n",
    "        ada_mlvae_balanced_multilabel_extended_results,\n",
    "        \"latent_predictive_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-GVAE (n\\_latent = {ada_gvae_balanced_multilabel_experiment_config['latent_dim']})\",\n",
    "        ada_gvae_balanced_multilabel_results,\n",
    "        \"latent_predictive_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-GVAE (n\\_latent = {ada_gvae_balanced_multilabel_extended_experiment_config['latent_dim']})\",\n",
    "        ada_gvae_balanced_multilabel_extended_results,\n",
    "        \"latent_predictive_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_balanced_multiclass_experiment_config['num_concepts']})\",\n",
    "        ccd_balanced_multiclass_results,\n",
    "        \"direct_completeness_scores\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_balanced_multiclass_extended_experiment_config['num_concepts']})\",\n",
    "        ccd_balanced_multiclass_extended_results,\n",
    "        \"direct_completeness_scores\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models * 2, 5))\n",
    "for i, (method_name, results, kword, transform_fn) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=5,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=20, loc='upper center', bbox_to_anchor=(0.5,-0.2), ncol=(num_models - 1)//2)\n",
    "\n",
    "plt.ylabel(\"Accuracy (\\%)\", fontsize=25)\n",
    "if len(all_vars) > 1:\n",
    "    plt.xlabel(\"Number of Dependency Edges ($\\lambda$)\", fontsize=25)\n",
    "    plt.title(bold_text(\"Downstream Task Accuracy from Concepts (dSprites($\\lambda$))\"), fontsize=30)\n",
    "    plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "    ax.set_xticklabels(all_vars, fontsize=15)\n",
    "else:\n",
    "    plt.title(bold_text(\"Downstream Task Accuracy from Concepts(dSprites($\\lambda = \" + str(all_vars[0]) + \"$))\"), fontsize=30)\n",
    "    plt.xticks([], fontsize=15)\n",
    "plt.yticks(fontsize=20)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot task accuracies for all methods\n",
    "# Set up our figure\n",
    "all_vars = [0, 4]\n",
    "num_concepts = 5\n",
    "clrs = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "all_models = [\n",
    "    (\n",
    "        \"Joint-CBM\",\n",
    "        graph_dependency_balanced_multiclass_results,\n",
    "        \"latent_predictive_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        \"CW MaxPool-Mean\",\n",
    "        cw_binary_balanced_multiclass_results,\n",
    "        \"latent_predictive_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        \"CW Feature Map\",\n",
    "        cw_binary_balanced_multiclass_results,\n",
    "        \"latent_feature_predictive_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-MLVAE (n\\_latent = {ada_mlvae_balanced_multilabel_experiment_config['latent_dim']})\",\n",
    "        ada_mlvae_balanced_multilabel_results,\n",
    "        \"latent_predictive_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-MLVAE (n\\_latent = {ada_mlvae_balanced_multilabel_extended_experiment_config['latent_dim']})\",\n",
    "        ada_mlvae_balanced_multilabel_extended_results,\n",
    "        \"latent_predictive_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-GVAE (n\\_latent = {ada_gvae_balanced_multilabel_experiment_config['latent_dim']})\",\n",
    "        ada_gvae_balanced_multilabel_results,\n",
    "        \"latent_predictive_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"Ada-GVAE (n\\_latent = {ada_gvae_balanced_multilabel_extended_experiment_config['latent_dim']})\",\n",
    "        ada_gvae_balanced_multilabel_extended_results,\n",
    "        \"latent_predictive_accuracies\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_balanced_multiclass_experiment_config['num_concepts']})\",\n",
    "        ccd_balanced_multiclass_results,\n",
    "        \"direct_completeness_scores\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "    (\n",
    "        f\"CCD (n\\_concepts = {ccd_balanced_multiclass_extended_experiment_config['num_concepts']})\",\n",
    "        ccd_balanced_multiclass_extended_results,\n",
    "        \"direct_completeness_scores\",\n",
    "        lambda x: 100 * x,\n",
    "    ),\n",
    "]\n",
    "\n",
    "num_models = len(all_models) + 1\n",
    "\n",
    "clrs = sns.color_palette(\"husl\", num_models * 2)\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(num_models * 2, 5))\n",
    "for i, (method_name, results, kword, transform_fn) in enumerate(all_models):\n",
    "    accs_means = list(map(\n",
    "        lambda x: x[0],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_means = np.array(list(map(transform_fn, accs_means)))\n",
    "    accs_stds = list(map(\n",
    "        lambda x: x[1],\n",
    "        np.array(results[kword])[all_vars],\n",
    "    ))\n",
    "    accs_stds = np.array(list(map(transform_fn, accs_stds)))\n",
    "    ax.bar(\n",
    "        scale * (np.arange(0, len(all_vars)) - (1/2 - 1/(2 * num_models)) + i/num_models),\n",
    "        accs_means,\n",
    "        width=scale/num_models,\n",
    "        color=clrs[i*2],\n",
    "        align='center',\n",
    "        label=method_name,\n",
    "        yerr=2*accs_stds,\n",
    "        capsize=5,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "\n",
    "lgd = ax.legend(fontsize=20, loc='upper center', bbox_to_anchor=(0.5,-0.2), ncol=(num_models - 1)//2)\n",
    "\n",
    "plt.ylabel(\"Accuracy (\\%)\", fontsize=25)\n",
    "if len(all_vars) > 1:\n",
    "    plt.xlabel(\"Number of Dependency Edges ($\\lambda$)\", fontsize=25)\n",
    "    plt.title(bold_text(\"Downstream Task Accuracy from Concepts (dSprites($\\lambda$))\"), fontsize=30)\n",
    "    plt.xticks(np.arange(0, len(all_vars)), fontsize=15)\n",
    "    ax.set_xticklabels(all_vars, fontsize=15)\n",
    "else:\n",
    "    plt.title(bold_text(\"Downstream Task Accuracy from Concepts(dSprites($\\lambda = \" + str(all_vars[0]) + \"$))\"), fontsize=30)\n",
    "    plt.xticks([], fontsize=15)\n",
    "plt.yticks(fontsize=20)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "purity_dsprites.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
